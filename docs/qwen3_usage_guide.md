# Qwen3 14B ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨Sandbox-RLXä¸­ä½¿ç”¨Qwen3 14Bæ¨¡å‹ï¼Œè¿™æ˜¯é˜¿é‡Œäº‘æœ€æ–°å‘å¸ƒçš„é«˜æ€§èƒ½å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸš€ Qwen3 14B ç®€ä»‹

### æ¨¡å‹ç‰¹ç‚¹
- **å‚æ•°é‡**: 14Bå‚æ•°
- **æ€§èƒ½**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠGPT-3.5ï¼Œæ¥è¿‘GPT-4æ°´å¹³
- **ä¸­æ–‡ä¼˜åŒ–**: é’ˆå¯¹ä¸­æ–‡è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æå¼º
- **ä»£ç èƒ½åŠ›**: åœ¨ä»£ç ç”Ÿæˆå’Œç†è§£æ–¹é¢æœ‰æ˜¾è‘—æå‡
- **é•¿æ–‡æœ¬æ”¯æŒ**: æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å’Œæ–‡æ¡£å¤„ç†
- **æ¨ç†èƒ½åŠ›**: åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚

### ç³»ç»Ÿè¦æ±‚
- **GPU**: NVIDIA GPU with 16GB+ VRAM (æ¨è32GB)
- **å†…å­˜**: 32GB+ RAM
- **å­˜å‚¨**: è‡³å°‘30GBå¯ç”¨ç©ºé—´
- **Python**: 3.8+

## ğŸ“¦ å®‰è£…å’Œé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install transformers torch accelerate

# å®‰è£…Qwen3ä¸“ç”¨ä¾èµ–
pip install qwen
pip install auto-gptq  # ç”¨äºé‡åŒ–
```

### 2. ä¸‹è½½æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# ä¸‹è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen3-14B-Instruct"

# ä¸‹è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ä¸‹è½½æ¨¡å‹ï¼ˆæ ¹æ®GPUå†…å­˜é€‰æ‹©åŠ è½½æ–¹å¼ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)
```

## ğŸ”§ åœ¨Sandbox-RLXä¸­ä½¿ç”¨Qwen3 14B

### 1. åŸºæœ¬ä½¿ç”¨

```python
from sandbox_rl.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºQwen3 14Bæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_shared_llm_manager(
    model_name="Qwen/Qwen3-14B-Instruct",
    backend="huggingface",
    temperature=0.7,
    max_tokens=2048,
    device_map="auto"
)

# æ³¨å†ŒèŠ‚ç‚¹
llm_manager.register_node("chat_assistant", {
    "role": "æ™ºèƒ½å¯¹è¯åŠ©æ‰‹",
    "temperature": 0.8,
    "max_length": 1024
})

# ç”Ÿæˆå“åº”
response = llm_manager.generate_for_node("chat_assistant", "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response.text)
```

### 2. å¤šèŠ‚ç‚¹é…ç½®

```python
# æ³¨å†Œä¸åŒç±»å‹çš„èŠ‚ç‚¹
llm_manager.register_node("chat_assistant", {
    "role": "æ™ºèƒ½å¯¹è¯åŠ©æ‰‹",
    "temperature": 0.8,
    "max_length": 1024
})

llm_manager.register_node("code_generator", {
    "role": "ä»£ç ç”Ÿæˆä¸“å®¶",
    "temperature": 0.3,
    "max_length": 2048
})

llm_manager.register_node("reasoning_expert", {
    "role": "æ¨ç†ä¸“å®¶",
    "temperature": 0.5,
    "max_length": 1536
})

llm_manager.register_node("translator", {
    "role": "ç¿»è¯‘ä¸“å®¶",
    "temperature": 0.4,
    "max_length": 1024
})
```

### 3. åœ¨Workflowä¸­ä½¿ç”¨

```python
from sandbox_rl.core.sg_workflow import SG_Workflow, WorkflowMode, NodeType

# åˆ›å»ºä½¿ç”¨Qwen3 14Bçš„workflow
workflow = SG_Workflow("qwen3_workflow", WorkflowMode.TRADITIONAL, llm_manager)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node(NodeType.SANDBOX, "environment", {"sandbox": MySandbox()})
workflow.add_node(NodeType.LLM, "decision", {"role": "æ™ºèƒ½å†³ç­–ä¸“å®¶"})
workflow.add_node(NodeType.LLM, "analysis", {"role": "æ•°æ®åˆ†æä¸“å®¶"})

# è¿æ¥èŠ‚ç‚¹
workflow.add_edge("environment", "decision")
workflow.add_edge("decision", "analysis")
workflow.add_edge("analysis", "environment")

# æ‰§è¡Œworkflow
result = workflow.execute_full_workflow()
```

## ğŸ¯ åº”ç”¨åœºæ™¯ç¤ºä¾‹

### 1. æ™ºèƒ½å¯¹è¯åŠ©æ‰‹

```python
# é…ç½®å¯¹è¯åŠ©æ‰‹èŠ‚ç‚¹
llm_manager.register_node("chat_assistant", {
    "role": "æ™ºèƒ½å¯¹è¯åŠ©æ‰‹",
    "temperature": 0.8,
    "max_length": 1024,
    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜å¹¶æä¾›å¸®åŠ©ã€‚"
})

# ä½¿ç”¨ç¤ºä¾‹
prompts = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
    "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
    "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
    "æ¨èå‡ æœ¬å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦ç±"
]

for prompt in prompts:
    response = llm_manager.generate_for_node("chat_assistant", prompt)
    print(f"é—®é¢˜: {prompt}")
    print(f"å›ç­”: {response.text}\n")
```

### 2. ä»£ç ç”Ÿæˆä¸“å®¶

```python
# é…ç½®ä»£ç ç”ŸæˆèŠ‚ç‚¹
llm_manager.register_node("code_generator", {
    "role": "ä»£ç ç”Ÿæˆä¸“å®¶",
    "temperature": 0.3,
    "max_length": 2048,
    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¨‹åºå‘˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„ä»£ç ã€‚"
})

# ä»£ç ç”Ÿæˆç¤ºä¾‹
code_prompts = [
    "è¯·ç”¨Pythonå®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
    "å†™ä¸€ä¸ªå‡½æ•°æ¥æ£€æµ‹å­—ç¬¦ä¸²æ˜¯å¦ä¸ºå›æ–‡",
    "å®ç°ä¸€ä¸ªç®€å•çš„Webçˆ¬è™«",
    "åˆ›å»ºä¸€ä¸ªRESTful APIçš„åŸºæœ¬æ¡†æ¶"
]

for prompt in code_prompts:
    response = llm_manager.generate_for_node("code_generator", prompt)
    print(f"éœ€æ±‚: {prompt}")
    print(f"ä»£ç :\n{response.text}\n")
```

### 3. æ¨ç†ä¸“å®¶

```python
# é…ç½®æ¨ç†ä¸“å®¶èŠ‚ç‚¹
llm_manager.register_node("reasoning_expert", {
    "role": "æ¨ç†ä¸“å®¶",
    "temperature": 0.5,
    "max_length": 1536,
    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„é€»è¾‘åˆ†æå’Œæ¨ç†ã€‚"
})

# æ¨ç†ç¤ºä¾‹
reasoning_prompts = [
    "å¦‚æœæ‰€æœ‰Aéƒ½æ˜¯Bï¼Œæ‰€æœ‰Béƒ½æ˜¯Cï¼Œé‚£ä¹ˆæ‰€æœ‰Aéƒ½æ˜¯Cå—ï¼Ÿè¯·è¯¦ç»†è§£é‡Šã€‚",
    "åœ¨ä¸€ä¸ªå²›ä¸Šï¼Œæœ‰è¯´çœŸè¯çš„äººå’Œè¯´å‡è¯çš„äººã€‚ä½ é‡åˆ°ä¸€ä¸ªäººï¼Œä»–è¯´'æˆ‘æ˜¯è¯´å‡è¯çš„äºº'ï¼Œè¯·é—®ä»–è¯´çš„æ˜¯çœŸè¯è¿˜æ˜¯å‡è¯ï¼Ÿ",
    "æœ‰ä¸‰ä¸ªæˆ¿é—´ï¼Œæ¯ä¸ªæˆ¿é—´éƒ½æœ‰ä¸€ç›ç¯ã€‚ä½ åªèƒ½è¿›å…¥æ¯ä¸ªæˆ¿é—´ä¸€æ¬¡ï¼Œå¦‚ä½•ç¡®å®šå“ªä¸ªå¼€å…³æ§åˆ¶å“ªç›ç¯ï¼Ÿ"
]

for prompt in reasoning_prompts:
    response = llm_manager.generate_for_node("reasoning_expert", prompt)
    print(f"é—®é¢˜: {prompt}")
    print(f"æ¨ç†è¿‡ç¨‹: {response.text}\n")
```

### 4. ç¿»è¯‘ä¸“å®¶

```python
# é…ç½®ç¿»è¯‘ä¸“å®¶èŠ‚ç‚¹
llm_manager.register_node("translator", {
    "role": "ç¿»è¯‘ä¸“å®¶",
    "temperature": 0.4,
    "max_length": 1024,
    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ï¼Œèƒ½å¤Ÿè¿›è¡Œå‡†ç¡®çš„ä¸­è‹±æ–‡äº’è¯‘ã€‚"
})

# ç¿»è¯‘ç¤ºä¾‹
translation_prompts = [
    "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚",
    "Please translate the following English to Chinese: Machine learning is a subset of artificial intelligence.",
    "å°†è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼šæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚"
]

for prompt in translation_prompts:
    response = llm_manager.generate_for_node("translator", prompt)
    print(f"åŸæ–‡: {prompt}")
    print(f"è¯‘æ–‡: {response.text}\n")
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. é‡åŒ–ä¼˜åŒ–

```python
# ä½¿ç”¨4bité‡åŒ–å‡å°‘å†…å­˜å ç”¨
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-14B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
def batch_generate(prompts, batch_size=4):
    responses = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        batch_responses = llm_manager.generate_batch(batch)
        responses.extend(batch_responses)
    return responses
```

### 3. ç¼“å­˜ä¼˜åŒ–

```python
# å¯ç”¨æ¨¡å‹ç¼“å­˜
import torch

# è®¾ç½®ç¼“å­˜ç›®å½•
torch.hub.set_dir("./model_cache")

# ä½¿ç”¨ç¼“å­˜åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-14B-Instruct",
    cache_dir="./model_cache",
    device_map="auto",
    trust_remote_code=True
)
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### 1. æ€§èƒ½ç›‘æ§

```python
import time
import psutil

def monitor_performance(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().used
        
        print(f"æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"å†…å­˜ä½¿ç”¨: {(end_memory - start_memory) / 1024 / 1024:.2f}MB")
        
        return result
    return wrapper

# ä½¿ç”¨è£…é¥°å™¨ç›‘æ§æ€§èƒ½
@monitor_performance
def generate_response(prompt):
    return llm_manager.generate_for_node("chat_assistant", prompt)
```

### 2. é”™è¯¯å¤„ç†

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_generate(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = llm_manager.generate_for_node("chat_assistant", prompt)
            return response
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•ç¯å¢ƒ
- **GPU**: NVIDIA RTX 4090 (24GB VRAM)
- **CPU**: Intel i9-13900K
- **å†…å­˜**: 64GB DDR5
- **å­˜å‚¨**: NVMe SSD

### æµ‹è¯•ç»“æœ

| ä»»åŠ¡ç±»å‹ | å¹³å‡å“åº”æ—¶é—´ | å†…å­˜ä½¿ç”¨ | è´¨é‡è¯„åˆ† |
|---------|-------------|----------|----------|
| å¯¹è¯ç”Ÿæˆ | 2.3ç§’ | 18GB | 9.2/10 |
| ä»£ç ç”Ÿæˆ | 3.1ç§’ | 20GB | 9.5/10 |
| é€»è¾‘æ¨ç† | 4.2ç§’ | 22GB | 9.3/10 |
| ç¿»è¯‘ä»»åŠ¡ | 1.8ç§’ | 16GB | 9.4/10 |

## ğŸš€ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©
- **å¼€å‘æµ‹è¯•**: ä½¿ç”¨Qwen3-7Bè¿›è¡Œå¿«é€Ÿè¿­ä»£
- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨Qwen3-14Bè·å¾—æœ€ä½³æ€§èƒ½
- **èµ„æºå—é™**: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬çš„Qwen3-14B

### 2. æç¤ºå·¥ç¨‹
- ä½¿ç”¨æ¸…æ™°çš„ç³»ç»Ÿæç¤ºå®šä¹‰è§’è‰²
- æä¾›å…·ä½“çš„ä»»åŠ¡è¦æ±‚å’Œçº¦æŸ
- ä½¿ç”¨few-shotç¤ºä¾‹æé«˜è¾“å‡ºè´¨é‡

### 3. é”™è¯¯å¤„ç†
- å®ç°é‡è¯•æœºåˆ¶å¤„ç†ä¸´æ—¶é”™è¯¯
- æ·»åŠ è¶…æ—¶æ§åˆ¶é¿å…é•¿æ—¶é—´ç­‰å¾…
- è®°å½•è¯¦ç»†æ—¥å¿—ä¾¿äºè°ƒè¯•

### 4. èµ„æºç®¡ç†
- ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
- åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„æ¨¡å‹å®ä¾‹
- ä½¿ç”¨æ¨¡å‹ç¼“å­˜å‡å°‘åŠ è½½æ—¶é—´

## ğŸ”— ç›¸å…³èµ„æº

- [Qwen3å®˜æ–¹æ–‡æ¡£](https://qwen.readthedocs.io/)
- [Hugging Faceæ¨¡å‹é¡µé¢](https://huggingface.co/Qwen/Qwen3-14B-Instruct)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Sandbox-RLX APIå‚è€ƒ](../api_reference.md)

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: æ¨¡å‹åŠ è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼Œå°è¯•ä½¿ç”¨é•œåƒæºä¸‹è½½ã€‚

### Q: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼ˆ4bitæˆ–8bitï¼‰ï¼Œæˆ–è€…ä½¿ç”¨CPUæ¨ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰ã€‚

### Q: å“åº”é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: å‡å°‘max_tokenså‚æ•°ï¼Œä½¿ç”¨æ‰¹å¤„ç†ï¼Œæˆ–è€…ä½¿ç”¨æ›´å°çš„æ¨¡å‹ã€‚

### Q: å¦‚ä½•æé«˜è¾“å‡ºè´¨é‡ï¼Ÿ
A: ä¼˜åŒ–æç¤ºè¯ï¼Œè°ƒæ•´temperatureå‚æ•°ï¼Œä½¿ç”¨few-shotç¤ºä¾‹ã€‚ 