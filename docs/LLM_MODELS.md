# Sandbox-RL LLMæ¨¡å‹æ”¯æŒ

Sandbox-RLæ¡†æ¶æ”¯æŒå¤šç§ç«çƒ­çš„å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—çš„å¼€æºæ›¿ä»£å“ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æ”¯æŒçš„æ¨¡å‹ç±»å‹ã€ç‰¹ç‚¹å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸš€ æ”¯æŒçš„æ¨¡å‹ç±»å‹

### 1. GPTç³»åˆ—æ¨¡å‹

#### GPT-2 (å¼€æº)
- **æ¨¡å‹**: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **ç‰¹ç‚¹**: OpenAIçš„å¼€æºæ¨¡å‹ï¼Œå‚æ•°é‡ä»124Måˆ°1.5B
- **ä¼˜åŠ¿**: è½»é‡çº§ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²
- **é€‚ç”¨åœºæ™¯**: æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€åˆ›æ„å†™ä½œ

```python
from sandbox_rl.core.llm_interface import create_gpt2_manager

# åˆ›å»ºGPT-2æ¨¡å‹ç®¡ç†å™¨
llm_manager = create_gpt2_manager("gpt2-medium", device="auto")
```

### 2. LLaMAç³»åˆ—æ¨¡å‹

#### LLaMA2 (Metaå¼€æº)
- **æ¨¡å‹**: `meta-llama/Llama-2-7b-chat-hf`, `meta-llama/Llama-2-13b-chat-hf`, `meta-llama/Llama-2-70b-chat-hf`
- **ç‰¹ç‚¹**: Metaå¼€æºçš„å¼ºå¤§å¯¹è¯æ¨¡å‹ï¼Œæ”¯æŒå¤šè½®å¯¹è¯
- **ä¼˜åŠ¿**: æ€§èƒ½ä¼˜ç§€ï¼Œç¤¾åŒºæ”¯æŒå¥½ï¼Œæœ‰ä¸°å¯Œçš„å¾®è°ƒç‰ˆæœ¬
- **é€‚ç”¨åœºæ™¯**: é€šç”¨å¯¹è¯ã€æ¨ç†ã€åˆ›æ„å†™ä½œ

```python
from sandbox_rl.core.llm_interface import create_llama2_manager

# åˆ›å»ºLLaMA2æ¨¡å‹ç®¡ç†å™¨
llm_manager = create_llama2_manager("meta-llama/Llama-2-7b-chat-hf")
```

#### CodeLLaMA (ä»£ç ä¸“ç”¨)
- **æ¨¡å‹**: `codellama/CodeLlama-7b-Instruct-hf`, `codellama/CodeLlama-13b-Instruct-hf`
- **ç‰¹ç‚¹**: ä¸“é—¨é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–çš„LLaMAå˜ä½“
- **ä¼˜åŠ¿**: ä»£ç ç”Ÿæˆèƒ½åŠ›å¼ºï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€
- **é€‚ç”¨åœºæ™¯**: ä»£ç ç”Ÿæˆã€ä»£ç è¡¥å…¨ã€ç¼–ç¨‹åŠ©æ‰‹

```python
from sandbox_rl.core.llm_interface import create_codellama_manager

# åˆ›å»ºCodeLLaMAæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_codellama_manager("codellama/CodeLlama-7b-Instruct-hf")
```

### 3. Qwenç³»åˆ—æ¨¡å‹ (é˜¿é‡Œäº‘)

#### Qwenæ¨¡å‹
- **æ¨¡å‹**: `Qwen/Qwen-1_8B-Chat`, `Qwen/Qwen-7B-Chat`, `Qwen/Qwen-14B-Chat`, `Qwen/Qwen-72B-Chat`
- **ç‰¹ç‚¹**: é˜¿é‡Œäº‘å¼€æºçš„ä¸­è‹±æ–‡åŒè¯­æ¨¡å‹
- **ä¼˜åŠ¿**: ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œæ”¯æŒé•¿æ–‡æœ¬
- **é€‚ç”¨åœºæ™¯**: ä¸­æ–‡å¯¹è¯ã€å¤šè¯­è¨€åº”ç”¨ã€é•¿æ–‡æœ¬å¤„ç†

```python
from sandbox_rl.core.llm_interface import create_qwen_manager

# åˆ›å»ºQwenæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_qwen_manager("Qwen/Qwen-7B-Chat")
```

#### Qwen3æ¨¡å‹ (æœ€æ–°ç‰ˆæœ¬)
- **æ¨¡å‹**: `Qwen/Qwen3-1.5B-Instruct`, `Qwen/Qwen3-7B-Instruct`, `Qwen/Qwen3-14B-Instruct`, `Qwen/Qwen3-32B-Instruct`
- **ç‰¹ç‚¹**: é˜¿é‡Œäº‘æœ€æ–°å‘å¸ƒçš„Qwen3ç³»åˆ—æ¨¡å‹ï¼Œæ€§èƒ½å¤§å¹…æå‡
- **ä¼˜åŠ¿**: 
  - **æ›´å¼ºçš„æ¨ç†èƒ½åŠ›**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚
  - **æ›´å¥½çš„ä¸­æ–‡ç†è§£**: é’ˆå¯¹ä¸­æ–‡è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–
  - **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: æ”¯æŒæ›´é•¿çš„å¯¹è¯å’Œæ–‡æ¡£å¤„ç†
  - **æ›´å¿«çš„æ¨ç†é€Ÿåº¦**: ä¼˜åŒ–äº†æ¨¡å‹æ¶æ„ï¼Œæ¨ç†æ•ˆç‡æ›´é«˜
  - **æ›´å¥½çš„ä»£ç èƒ½åŠ›**: åœ¨ä»£ç ç”Ÿæˆå’Œç†è§£æ–¹é¢æœ‰æ˜¾è‘—æå‡
- **é€‚ç”¨åœºæ™¯**: é«˜çº§å¯¹è¯ã€å¤æ‚æ¨ç†ã€ä»£ç ç”Ÿæˆã€é•¿æ–‡æ¡£å¤„ç†ã€å¤šè¯­è¨€åº”ç”¨

```python
from sandbox_rl.core.llm_interface import create_qwen3_manager

# åˆ›å»ºQwen3æ¨¡å‹ç®¡ç†å™¨
llm_manager = create_qwen3_manager("Qwen/Qwen3-14B-Instruct")

# æˆ–è€…ä½¿ç”¨é€šç”¨æ¥å£
llm_manager = create_shared_llm_manager("Qwen/Qwen3-14B-Instruct")
```

**Qwen3 14Bç‰¹åˆ«æ¨è**:
- **å‚æ•°é‡**: 14Bå‚æ•°ï¼Œåœ¨æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡
- **æ€§èƒ½**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠGPT-3.5ï¼Œæ¥è¿‘GPT-4æ°´å¹³
- **å†…å­˜éœ€æ±‚**: çº¦28-32GBæ˜¾å­˜ï¼ˆä½¿ç”¨4bité‡åŒ–å¯é™è‡³8-12GBï¼‰
- **æ¨ç†é€Ÿåº¦**: ç›¸æ¯”Qwen2æœ‰æ˜¾è‘—æå‡
- **é€‚ç”¨åœºæ™¯**: ä¼ä¸šçº§åº”ç”¨ã€é«˜çº§AIåŠ©æ‰‹ã€å¤æ‚ä»»åŠ¡å¤„ç†

### 4. Mistralç³»åˆ—æ¨¡å‹

#### Mistral-7B
- **æ¨¡å‹**: `mistralai/Mistral-7B-Instruct-v0.2`, `mistralai/Mistral-7B-v0.1`
- **ç‰¹ç‚¹**: é«˜æ€§èƒ½çš„7Bå‚æ•°æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚
- **ä¼˜åŠ¿**: æ¨ç†èƒ½åŠ›å¼ºï¼Œå“åº”é€Ÿåº¦å¿«ï¼Œèµ„æºå ç”¨ç›¸å¯¹è¾ƒä½
- **é€‚ç”¨åœºæ™¯**: é€šç”¨å¯¹è¯ã€æ¨ç†ä»»åŠ¡ã€åˆ›æ„å†™ä½œ

```python
from sandbox_rl.core.llm_interface import create_mistral_manager

# åˆ›å»ºMistralæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_mistral_manager("mistralai/Mistral-7B-Instruct-v0.2")
```

#### Mixtral-8x7B
- **æ¨¡å‹**: `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **ç‰¹ç‚¹**: æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘70Bæ¨¡å‹ä½†èµ„æºå ç”¨æ›´å°‘
- **ä¼˜åŠ¿**: æ€§èƒ½ä¼˜ç§€ï¼Œæ¨ç†èƒ½åŠ›å¼ºï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡
- **é€‚ç”¨åœºæ™¯**: å¤æ‚æ¨ç†ã€å¤šæ­¥éª¤ä»»åŠ¡ã€é«˜è´¨é‡å¯¹è¯

### 5. Gemmaç³»åˆ—æ¨¡å‹ (Google)

#### Gemmaæ¨¡å‹
- **æ¨¡å‹**: `google/gemma-2b-it`, `google/gemma-7b-it`
- **ç‰¹ç‚¹**: Googleå¼€æºçš„è½»é‡çº§æ¨¡å‹ï¼ŒåŸºäºGeminiæŠ€æœ¯
- **ä¼˜åŠ¿**: è½»é‡çº§ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¾å¤‡
- **é€‚ç”¨åœºæ™¯**: ç§»åŠ¨åº”ç”¨ã€å®æ—¶å¯¹è¯ã€èµ„æºå—é™ç¯å¢ƒ

```python
from sandbox_rl.core.llm_interface import create_gemma_manager

# åˆ›å»ºGemmaæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_gemma_manager("google/gemma-2b-it")
```

### 6. Phiç³»åˆ—æ¨¡å‹ (Microsoft)

#### Phiæ¨¡å‹
- **æ¨¡å‹**: `microsoft/Phi-2`, `microsoft/Phi-1_5`
- **ç‰¹ç‚¹**: Microsoftçš„å°å‹é«˜æ•ˆæ¨¡å‹ï¼Œå‚æ•°é‡å°ä½†æ€§èƒ½ä¼˜ç§€
- **ä¼˜åŠ¿**: èµ„æºå ç”¨æä½ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²
- **é€‚ç”¨åœºæ™¯**: æœ¬åœ°åº”ç”¨ã€å®æ—¶å¤„ç†ã€èµ„æºå—é™ç¯å¢ƒ

```python
from sandbox_rl.core.llm_interface import create_phi_manager

# åˆ›å»ºPhiæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_phi_manager("microsoft/Phi-2")
```

### 7. ä¸­æ–‡æ¨¡å‹ç³»åˆ—

#### Yiæ¨¡å‹ (01.AI)
- **æ¨¡å‹**: `01-ai/Yi-6B-Chat`, `01-ai/Yi-34B-Chat`
- **ç‰¹ç‚¹**: é«˜è´¨é‡çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡åŒè¯­
- **ä¼˜åŠ¿**: ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œå¯¹è¯è‡ªç„¶ï¼Œæ€§èƒ½ä¼˜ç§€
- **é€‚ç”¨åœºæ™¯**: ä¸­æ–‡å¯¹è¯ã€å¤šè¯­è¨€åº”ç”¨ã€å®¢æœç³»ç»Ÿ

```python
from sandbox_rl.core.llm_interface import create_yi_manager

# åˆ›å»ºYiæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_yi_manager("01-ai/Yi-6B-Chat")
```

#### ChatGLMæ¨¡å‹ (æ¸…åå¤§å­¦)
- **æ¨¡å‹**: `THUDM/chatglm3-6b`, `THUDM/chatglm2-6b`
- **ç‰¹ç‚¹**: æ¸…åå¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼Œæ”¯æŒå¤šè½®å¯¹è¯
- **ä¼˜åŠ¿**: ä¸­æ–‡å¯¹è¯èƒ½åŠ›å¼ºï¼Œæ”¯æŒé•¿æ–‡æœ¬ï¼Œå¼€æºå‹å¥½
- **é€‚ç”¨åœºæ™¯**: ä¸­æ–‡å¯¹è¯ã€æ–‡æ¡£é—®ç­”ã€çŸ¥è¯†é—®ç­”

```python
from sandbox_rl.core.llm_interface import create_chatglm_manager

# åˆ›å»ºChatGLMæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_chatglm_manager("THUDM/chatglm3-6b")
```

#### Baichuanæ¨¡å‹ (ç™¾å·æ™ºèƒ½)
- **æ¨¡å‹**: `baichuan-inc/Baichuan2-7B-Chat`, `baichuan-inc/Baichuan2-13B-Chat`
- **ç‰¹ç‚¹**: ç™¾å·æ™ºèƒ½å¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹
- **ä¼˜åŠ¿**: ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œå¯¹è¯è‡ªç„¶ï¼Œæ€§èƒ½ç¨³å®š
- **é€‚ç”¨åœºæ™¯**: ä¸­æ–‡å¯¹è¯ã€çŸ¥è¯†é—®ç­”ã€åˆ›æ„å†™ä½œ

```python
from sandbox_rl.core.llm_interface import create_baichuan_manager

# åˆ›å»ºBaichuanæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_baichuan_manager("baichuan-inc/Baichuan2-7B-Chat")
```

#### InternLMæ¨¡å‹ (ä¸Šæµ·AIå®éªŒå®¤)
- **æ¨¡å‹**: `internlm/internlm-chat-7b`, `internlm/internlm-chat-20b`
- **ç‰¹ç‚¹**: ä¸Šæµ·AIå®éªŒå®¤å¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹
- **ä¼˜åŠ¿**: ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œæ”¯æŒé•¿æ–‡æœ¬ï¼Œæ€§èƒ½ä¼˜ç§€
- **é€‚ç”¨åœºæ™¯**: ä¸­æ–‡å¯¹è¯ã€æ–‡æ¡£å¤„ç†ã€çŸ¥è¯†é—®ç­”

```python
from sandbox_rl.core.llm_interface import create_internlm_manager

# åˆ›å»ºInternLMæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_internlm_manager("internlm/internlm-chat-7b")
```

### 8. ä»£ç ç”Ÿæˆæ¨¡å‹

#### StarCoderæ¨¡å‹ (BigCode)
- **æ¨¡å‹**: `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`
- **ç‰¹ç‚¹**: ä¸“é—¨é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–çš„æ¨¡å‹
- **ä¼˜åŠ¿**: ä»£ç ç”Ÿæˆèƒ½åŠ›å¼ºï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œç†è§£ä»£ç ç»“æ„
- **é€‚ç”¨åœºæ™¯**: ä»£ç ç”Ÿæˆã€ä»£ç è¡¥å…¨ã€ç¼–ç¨‹æ•™è‚²

```python
from sandbox_rl.core.llm_interface import create_starcoder_manager

# åˆ›å»ºStarCoderæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_starcoder_manager("bigcode/starcoder2-7b")
```

### 9. å…¶ä»–é«˜æ€§èƒ½æ¨¡å‹

#### Falconæ¨¡å‹ (TII)
- **æ¨¡å‹**: `tiiuae/falcon-7b-instruct`, `tiiuae/falcon-40b-instruct`
- **ç‰¹ç‚¹**: TIIå¼€æºçš„é«˜æ€§èƒ½æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚
- **ä¼˜åŠ¿**: æ€§èƒ½ä¼˜ç§€ï¼Œæ¨ç†èƒ½åŠ›å¼ºï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡
- **é€‚ç”¨åœºæ™¯**: å¤æ‚æ¨ç†ã€å¤šæ­¥éª¤ä»»åŠ¡ã€é«˜è´¨é‡å¯¹è¯

```python
from sandbox_rl.core.llm_interface import create_falcon_manager

# åˆ›å»ºFalconæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_falcon_manager("tiiuae/falcon-7b-instruct")
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from sandbox_rl.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_shared_llm_manager(
    model_name="Qwen/Qwen3-14B-Instruct",
    backend="huggingface",
    device="auto"
)

# æ³¨å†ŒèŠ‚ç‚¹
llm_manager.register_node("my_node", {
    "role": "å¯¹è¯åŠ©æ‰‹",
    "temperature": 0.7,
    "max_length": 512
})

# ç”Ÿæˆå“åº”
response = llm_manager.generate_for_node("my_node", "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response.text)
```

### 2. é€šè¿‡ç±»å‹åˆ›å»ºæ¨¡å‹

```python
from sandbox_rl.core.llm_interface import create_model_by_type

# æ ¹æ®ç±»å‹åˆ›å»ºæ¨¡å‹
llm_manager = create_model_by_type("qwen3", device="auto")

# æ³¨å†ŒèŠ‚ç‚¹
llm_manager.register_node("test_node", {
    "role": "æµ‹è¯•èŠ‚ç‚¹",
    "temperature": 0.7,
    "max_length": 256
})

# ç”Ÿæˆå“åº”
response = llm_manager.generate_for_node("test_node", "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½")
print(response.text)
```

### 3. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

```python
from sandbox_rl.core.llm_interface import get_available_models

# è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
models = get_available_models()
for model_type, model_list in models.items():
    print(f"{model_type}: {model_list}")
```

## ğŸ“Š æ¨¡å‹é€‰æ‹©æŒ‡å—

### æŒ‰åº”ç”¨åœºæ™¯é€‰æ‹©

| åº”ç”¨åœºæ™¯ | æ¨èæ¨¡å‹ | ç†ç”± |
|---------|---------|------|
| ä¸­æ–‡å¯¹è¯ | **Qwen3-14B**, Qwen-7B, Yi-6B, ChatGLM3 | ä¸­æ–‡ç†è§£èƒ½åŠ›å¼ºï¼Œæ€§èƒ½ä¼˜ç§€ |
| ä»£ç ç”Ÿæˆ | **Qwen3-14B**, CodeLLaMA, StarCoder | ä»£ç èƒ½åŠ›æ˜¾è‘—æå‡ |
| è½»é‡çº§åº”ç”¨ | Phi-2, Gemma-2B | èµ„æºå ç”¨ä½ï¼Œé€Ÿåº¦å¿« |
| é«˜æ€§èƒ½æ¨ç† | **Qwen3-14B**, Mistral-7B, LLaMA2-13B | æ¨ç†èƒ½åŠ›å¼º |
| é•¿æ–‡æœ¬å¤„ç† | **Qwen3-14B**, Qwen-14B, InternLM-20B | æ”¯æŒé•¿æ–‡æœ¬ |
| ç§»åŠ¨ç«¯åº”ç”¨ | Phi-2, Gemma-2B | è½»é‡çº§ï¼Œé€‚åˆç§»åŠ¨è®¾å¤‡ |
| ä¼ä¸šçº§åº”ç”¨ | **Qwen3-14B**, Qwen3-32B | æ€§èƒ½ç¨³å®šï¼ŒåŠŸèƒ½å…¨é¢ |

### æŒ‰èµ„æºéœ€æ±‚é€‰æ‹©

| èµ„æºçº§åˆ« | æ¨èæ¨¡å‹ | å†…å­˜éœ€æ±‚ | GPUéœ€æ±‚ |
|---------|---------|---------|---------|
| æä½èµ„æº | Phi-2, Gemma-2B | <4GB | å¯é€‰ |
| ä½èµ„æº | Qwen-1.8B, Yi-6B | 4-8GB | æ¨è |
| ä¸­ç­‰èµ„æº | Mistral-7B, LLaMA2-7B | 8-16GB | å¿…éœ€ |
| é«˜èµ„æº | **Qwen3-14B**, Qwen-14B, LLaMA2-13B | 16-32GB | å¿…éœ€ |
| æé«˜èµ„æº | **Qwen3-32B**, Qwen-72B, LLaMA2-70B | >32GB | å¤šGPU |

### Qwen3ç³»åˆ—è¯¦ç»†å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | æ€§èƒ½ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|----------|----------|
| Qwen3-1.5B | 1.5B | 3-6GB | è½»é‡å¿«é€Ÿ | ç§»åŠ¨ç«¯ã€å®æ—¶åº”ç”¨ |
| Qwen3-7B | 7B | 8-16GB | å¹³è¡¡æ€§èƒ½ | é€šç”¨åº”ç”¨ã€å¼€å‘æµ‹è¯• |
| **Qwen3-14B** | **14B** | **16-32GB** | **é«˜æ€§èƒ½** | **ä¼ä¸šçº§ã€å¤æ‚ä»»åŠ¡** |
| Qwen3-32B | 32B | 32-64GB | é¡¶çº§æ€§èƒ½ | ç ”ç©¶ã€é«˜ç«¯åº”ç”¨ |

## ğŸš€ GPTçš„å¼€æºæ›¿ä»£å“

### ä¸ºä»€ä¹ˆé€‰æ‹©å¼€æºæ¨¡å‹ï¼Ÿ

1. **æˆæœ¬æ•ˆç›Š**: æ— éœ€æ”¯ä»˜APIè´¹ç”¨ï¼Œä¸€æ¬¡æ€§éƒ¨ç½²æˆæœ¬
2. **æ•°æ®éšç§**: æœ¬åœ°éƒ¨ç½²ï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°
3. **å®šåˆ¶åŒ–**: å¯ä»¥å¾®è°ƒå’Œå®šåˆ¶æ¨¡å‹
4. **å¯æ§æ€§**: å®Œå…¨æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºå’Œè¾“å‡º
5. **ç¦»çº¿ä½¿ç”¨**: ä¸ä¾èµ–ç½‘ç»œè¿æ¥

### ä¸»è¦æ›¿ä»£æ–¹æ¡ˆ

| GPTç‰ˆæœ¬ | å¼€æºæ›¿ä»£å“ | ä¼˜åŠ¿ |
|---------|-----------|------|
| GPT-3.5 | **Qwen3-14B**, LLaMA2-7B, Mistral-7B | æ€§èƒ½æ¥è¿‘æˆ–è¶…è¶Šï¼Œå¼€æºå…è´¹ |
| GPT-4 | **Qwen3-32B**, LLaMA2-70B, Qwen-72B | å‚æ•°é‡å¤§ï¼Œæ€§èƒ½ä¼˜ç§€ |
| GPT-4 Code | **Qwen3-14B**, CodeLLaMA, StarCoder | ä¸“é—¨é’ˆå¯¹ä»£ç ä¼˜åŒ– |

## ğŸ†• Qwen3 14B ç‰¹åˆ«æ¨è

### ä¸ºä»€ä¹ˆé€‰æ‹©Qwen3 14Bï¼Ÿ

1. **æ€§èƒ½å“è¶Š**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠGPT-3.5ï¼Œæ¥è¿‘GPT-4æ°´å¹³
2. **ä¸­æ–‡ä¼˜åŒ–**: é’ˆå¯¹ä¸­æ–‡è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æå¼º
3. **ä»£ç èƒ½åŠ›**: åœ¨ä»£ç ç”Ÿæˆå’Œç†è§£æ–¹é¢æœ‰æ˜¾è‘—æå‡
4. **é•¿æ–‡æœ¬æ”¯æŒ**: æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å’Œæ–‡æ¡£å¤„ç†
5. **æ¨ç†èƒ½åŠ›**: åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚
6. **èµ„æºå¹³è¡¡**: 14Bå‚æ•°åœ¨æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡

### ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»ºQwen3 14Bæ¨¡å‹ç®¡ç†å™¨
llm_manager = create_shared_llm_manager(
    model_name="Qwen/Qwen3-14B-Instruct",
    backend="huggingface",
    temperature=0.7,
    max_tokens=2048
)

# æ³¨å†Œä¸åŒç±»å‹çš„èŠ‚ç‚¹
llm_manager.register_node("chat_assistant", {
    "role": "æ™ºèƒ½å¯¹è¯åŠ©æ‰‹",
    "temperature": 0.8,
    "max_length": 1024
})

llm_manager.register_node("code_generator", {
    "role": "ä»£ç ç”Ÿæˆä¸“å®¶",
    "temperature": 0.3,
    "max_length": 2048
})

llm_manager.register_node("reasoning_expert", {
    "role": "æ¨ç†ä¸“å®¶",
    "temperature": 0.5,
    "max_length": 1536
})

# ä½¿ç”¨ç¤ºä¾‹
chat_response = llm_manager.generate_for_node("chat_assistant", "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹")
code_response = llm_manager.generate_for_node("code_generator", "è¯·ç”¨Pythonå®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•")
reasoning_response = llm_manager.generate_for_node("reasoning_expert", "å¦‚æœæ‰€æœ‰Aéƒ½æ˜¯Bï¼Œæ‰€æœ‰Béƒ½æ˜¯Cï¼Œé‚£ä¹ˆæ‰€æœ‰Aéƒ½æ˜¯Cå—ï¼Ÿ")
```

## ğŸ”— ç›¸å…³èµ„æº

- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)
- [Qwen3å®˜æ–¹æ–‡æ¡£](https://qwen.readthedocs.io/)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [æ¨¡å‹è®¸å¯è¯è¯´æ˜](https://huggingface.co/docs/hub/repositories-licenses)
- [æ¨¡å‹æ€§èƒ½åŸºå‡†](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)