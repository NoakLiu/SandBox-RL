# Core On-Policy RL å®ç°æ€»ç»“

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£æ€»ç»“äº†åœ¨Sandbox-RL coreæ¨¡å—ä¸­å®ç°çš„on-policy RLåŠŸèƒ½ï¼ŒåŒ…æ‹¬åˆä½œå› å­ï¼ˆCooperation Factorï¼‰å’Œèƒ½åŠ›å› å­ï¼ˆCompetence Factorï¼‰çš„æ”¯æŒã€‚

## ğŸ”§ å®ç°ä½ç½®

### ä¸»è¦æ–‡ä»¶
- `sandgraph/core/rl_algorithms.py` - æ ¸å¿ƒRLç®—æ³•å®ç°
- `sandgraph/core/__init__.py` - æ¨¡å—å¯¼å‡ºé…ç½®

### æµ‹è¯•æ–‡ä»¶
- `test_core_rl.py` - åŠŸèƒ½æµ‹è¯•è„šæœ¬
- `demo/core_on_policy_rl_demo.py` - æ¼”ç¤ºè„šæœ¬

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

### 1. åˆä½œå› å­ (Cooperation Factor)

```python
@dataclass
class CooperationFactor:
    cooperation_type: CooperationType = CooperationType.NONE
    cooperation_strength: float = 0.0  # [0.0, 1.0]
    team_size: int = 1
    shared_reward_ratio: float = 0.5  # [0.0, 1.0]
    knowledge_transfer_rate: float = 0.1  # [0.0, 1.0]
    resource_sharing_enabled: bool = False
    communication_cost: float = 0.01  # åˆä½œæˆæœ¬
```

**åˆä½œç±»å‹ (CooperationType):**
- `NONE` - æ— åˆä½œ
- `TEAM_BASED` - å›¢é˜Ÿåˆä½œ
- `SHARED_REWARDS` - å…±äº«å¥–åŠ±
- `KNOWLEDGE_TRANSFER` - çŸ¥è¯†è½¬ç§»
- `RESOURCE_SHARING` - èµ„æºå…±äº«

### 2. èƒ½åŠ›å› å­ (Competence Factor)

```python
@dataclass
class CompetenceFactor:
    competence_type: CompetenceType = CompetenceType.GENERAL
    base_capability: float = 0.5  # [0.0, 1.0]
    learning_rate: float = 0.01  # [0.0, 1.0]
    adaptation_speed: float = 0.1  # [0.0, 1.0]
    specialization_level: float = 0.0  # [0.0, 1.0]
    experience_decay: float = 0.95  # [0.0, 1.0]
    max_capability: float = 1.0  # [0.0, 1.0]
```

**èƒ½åŠ›ç±»å‹ (CompetenceType):**
- `GENERAL` - é€šç”¨æ™ºèƒ½ä½“
- `SPECIALIZED` - ä¸“ä¸šæ™ºèƒ½ä½“
- `ADAPTIVE` - è‡ªé€‚åº”æ™ºèƒ½ä½“
- `EXPERT` - ä¸“å®¶æ™ºèƒ½ä½“
- `NOVICE` - æ–°æ‰‹æ™ºèƒ½ä½“

### 3. On-Policy RL æ™ºèƒ½ä½“

```python
class OnPolicyRLAgent:
    def __init__(self, agent_id: str, config: RLConfig, state_dim: int = 64, action_dim: int = 10):
        # æ”¯æŒåˆä½œå’Œèƒ½åŠ›å› å­çš„æ™ºèƒ½ä½“
    
    def get_action(self, state: Dict[str, Any], cooperation_context: Optional[Dict[str, Any]] = None):
        # è·å–åŠ¨ä½œï¼Œè€ƒè™‘åˆä½œä¸Šä¸‹æ–‡
    
    def update_capability(self, reward: float, team_performance: Optional[float] = None):
        # æ›´æ–°æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œæ”¯æŒå›¢é˜Ÿå­¦ä¹ 
```

### 4. å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

```python
class MultiAgentOnPolicyRL:
    def __init__(self, num_agents: int = 8, state_dim: int = 64, action_dim: int = 10,
                 cooperation_configs: Optional[List[CooperationFactor]] = None,
                 competence_configs: Optional[List[CompetenceFactor]] = None):
        # å¤šæ™ºèƒ½ä½“on-policy RLç³»ç»Ÿ
    
    def step(self, agent_id: str, state: Dict[str, Any]) -> Tuple[str, float, float]:
        # æ™ºèƒ½ä½“æ‰§è¡Œä¸€æ­¥
    
    def update_agent(self, agent_id: str, step: TrajectoryStep):
        # æ›´æ–°æ™ºèƒ½ä½“ç»éªŒ
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from sandbox_rl.core.rl_algorithms import (
    CooperationType, CompetenceType,
    CooperationFactor, CompetenceFactor,
    MultiAgentOnPolicyRL
)

# åˆ›å»ºåˆä½œé…ç½®
cooperation_config = CooperationFactor(
    cooperation_type=CooperationType.TEAM_BASED,
    cooperation_strength=0.3,
    team_size=4,
    shared_reward_ratio=0.6
)

# åˆ›å»ºèƒ½åŠ›é…ç½®
competence_config = CompetenceFactor(
    competence_type=CompetenceType.ADAPTIVE,
    base_capability=0.5,
    learning_rate=0.02,
    adaptation_speed=0.15
)

# åˆ›å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
multi_agent_system = MultiAgentOnPolicyRL(
    num_agents=8,
    cooperation_configs=[cooperation_config] * 8,
    competence_configs=[competence_config] * 8
)
```

### 2. æ™ºèƒ½ä½“äº¤äº’

```python
# æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œ
state = {"position": [0, 0, 0], "energy": 1.0}
action, log_prob, value = multi_agent_system.step("agent_0", state)

# æ›´æ–°æ™ºèƒ½ä½“
trajectory_step = TrajectoryStep(
    state=state,
    action=action,
    reward=0.5,
    value=value,
    log_prob=log_prob,
    done=False
)
multi_agent_system.update_agent("agent_0", trajectory_step)
```

### 3. è·å–ç»Ÿè®¡ä¿¡æ¯

```python
# è·å–æ™ºèƒ½ä½“ç»Ÿè®¡
agent_stats = multi_agent_system.get_agent_stats()
for agent_id, stats in agent_stats.items():
    print(f"{agent_id}: capability={stats['capability']:.3f}")

# è·å–å›¢é˜Ÿä¿¡æ¯
team_stats = multi_agent_system.get_team_stats()
```

## ğŸ“Š åŠŸèƒ½ç‰¹æ€§

### 1. åˆä½œæœºåˆ¶
- **å›¢é˜Ÿåˆä½œ**: æ™ºèƒ½ä½“ç»„æˆå›¢é˜Ÿï¼Œå…±äº«å¥–åŠ±å’ŒçŸ¥è¯†
- **å…±äº«å¥–åŠ±**: æ™ºèƒ½ä½“é—´æŒ‰æ¯”ä¾‹åˆ†é…å¥–åŠ±
- **çŸ¥è¯†è½¬ç§»**: æ™ºèƒ½ä½“é—´ä¼ é€’å­¦ä¹ ç»éªŒ
- **èµ„æºå…±äº«**: æ™ºèƒ½ä½“å…±äº«è®¡ç®—å’Œå­˜å‚¨èµ„æº

### 2. èƒ½åŠ›è¿›åŒ–
- **åŠ¨æ€èƒ½åŠ›**: æ™ºèƒ½ä½“èƒ½åŠ›éšç»éªŒåŠ¨æ€è°ƒæ•´
- **å›¢é˜Ÿå­¦ä¹ **: é€šè¿‡åˆä½œæå‡ä¸ªä½“èƒ½åŠ›
- **ä¸“ä¸šåŒ–**: æ”¯æŒæ™ºèƒ½ä½“å‘ç‰¹å®šé¢†åŸŸä¸“ä¸šåŒ–
- **ç»éªŒè¡°å‡**: é˜²æ­¢èƒ½åŠ›è¿‡åº¦è†¨èƒ€

### 3. On-Policy å­¦ä¹ 
- **å®æ—¶æ›´æ–°**: åŸºäºå½“å‰ç­–ç•¥è¿›è¡Œå­¦ä¹ 
- **ç»éªŒå›æ”¾**: æ”¯æŒç»éªŒç¼“å†²åŒºç®¡ç†
- **ç­–ç•¥ä¼˜åŒ–**: ä½¿ç”¨PPOç­‰on-policyç®—æ³•
- **å¤šæ™ºèƒ½ä½“åè°ƒ**: æ”¯æŒå¤šæ™ºèƒ½ä½“é—´çš„åè°ƒå­¦ä¹ 

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•
```bash
python test_core_rl.py
```

### é¢„æœŸè¾“å‡º
```
âœ… Successfully imported core RL modules

ğŸ”— Testing Cooperation Factors:
  - Cooperation Type: team_based
  - Cooperation Strength: 0.3
  - Team Size: 4

ğŸ¯ Testing Competence Factors:
  - Competence Type: adaptive
  - Base Capability: 0.5
  - Learning Rate: 0.02

âœ… All tests passed successfully!
```

## ğŸ”® æ‰©å±•æ–¹å‘

### 1. ç¥ç»ç½‘ç»œé›†æˆ
- é›†æˆPyTorch/TensorFlowç¥ç»ç½‘ç»œ
- å®ç°çœŸæ­£çš„ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ
- æ”¯æŒGPUåŠ é€Ÿè®­ç»ƒ

### 2. é«˜çº§åˆä½œç­–ç•¥
- åŠ¨æ€å›¢é˜Ÿé‡ç»„
- ç«äº‰ä¸åˆä½œå¹³è¡¡
- å¤šå±‚æ¬¡åˆä½œç»“æ„

### 3. ç¯å¢ƒé›†æˆ
- ä¸Sandbox-RLç¯å¢ƒç³»ç»Ÿé›†æˆ
- æ”¯æŒçœŸå®ä»»åŠ¡åœºæ™¯
- å¤šä»»åŠ¡å­¦ä¹ æ”¯æŒ

### 4. ç›‘æ§å’Œå¯è§†åŒ–
- è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
- åˆä½œæ•ˆæœåˆ†æ
- èƒ½åŠ›è¿›åŒ–è¿½è¸ª

## ğŸ“ æ€»ç»“

æœ¬æ¬¡å®ç°æˆåŠŸåœ¨Sandbox-RL coreæ¨¡å—ä¸­é›†æˆäº†ï¼š

1. **åˆä½œå› å­ç³»ç»Ÿ** - æ”¯æŒå¤šç§åˆä½œæ¨¡å¼
2. **èƒ½åŠ›å› å­ç³»ç»Ÿ** - æ”¯æŒæ™ºèƒ½ä½“èƒ½åŠ›è¿›åŒ–
3. **On-Policy RLæ¡†æ¶** - æ”¯æŒå®æ—¶ç­–ç•¥å­¦ä¹ 
4. **å¤šæ™ºèƒ½ä½“åè°ƒ** - æ”¯æŒå›¢é˜Ÿåä½œå­¦ä¹ 

è¿™äº›åŠŸèƒ½ä¸ºSandbox-RLæä¾›äº†å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ èƒ½åŠ›ï¼Œæ”¯æŒå¤æ‚çš„åä½œå’Œç«äº‰åœºæ™¯ï¼Œä¸ºåç»­çš„é«˜çº§AIç³»ç»Ÿå¼€å‘å¥ å®šäº†åšå®åŸºç¡€ã€‚
