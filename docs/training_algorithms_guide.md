# è®­ç»ƒç®—æ³•æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»SandGraphXä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•ï¼ŒåŒ…æ‹¬PPOã€GRPOç­‰ç®—æ³•çš„åŸç†ã€é…ç½®å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸš€ ç®—æ³•æ¦‚è¿°

SandGraphXæä¾›äº†å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¼˜åŒ–LLMçš„å†³ç­–ç­–ç•¥ï¼š

- **PPO (Proximal Policy Optimization)** - è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•
- **GRPO (Group Robust Policy Optimization)** - ç»„é²æ£’ç­–ç•¥ä¼˜åŒ–ç®—æ³•
- **SAC (Soft Actor-Critic)** - è½¯Actor-Criticç®—æ³•
- **TD3 (Twin Delayed Deep Deterministic Policy Gradient)** - åŒå»¶è¿Ÿæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•
- **å¢å¼ºç‰ˆç®—æ³•** - é›†æˆAReaLæ¡†æ¶çš„ä¼˜åŒ–ç‰ˆæœ¬

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. è½¨è¿¹ (Trajectory)
è½¨è¿¹æ˜¯æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’çš„å®Œæ•´åºåˆ—ï¼ŒåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯ã€‚

```python
@dataclass
class TrajectoryStep:
    state: Dict[str, Any]      # ç¯å¢ƒçŠ¶æ€
    action: str                # æ™ºèƒ½ä½“åŠ¨ä½œ
    reward: float              # å³æ—¶å¥–åŠ±
    value: float               # çŠ¶æ€ä»·å€¼ä¼°è®¡
    log_prob: float            # åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    done: bool                 # æ˜¯å¦ç»“æŸ
    advantage: float = 0.0     # ä¼˜åŠ¿å‡½æ•°
    return_: float = 0.0       # ç´¯ç§¯å›æŠ¥
```

### 2. ä¼˜åŠ¿å‡½æ•° (Advantage Function)
è¡¡é‡æŸä¸ªåŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿ï¼Œç”¨äºæŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚

### 3. ç­–ç•¥æ¢¯åº¦ (Policy Gradient)
åŸºäºç­–ç•¥æ¢¯åº¦çš„ç®—æ³•ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ã€‚

## ğŸ”§ åŸºç¡€ç®—æ³•

### 1. PPO (Proximal Policy Optimization)

PPOæ˜¯ä¸€ç§ç¨³å®šã€é«˜æ•ˆçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡è£å‰ªç›®æ ‡å‡½æ•°æ¥é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **ç¨³å®šæ€§**: é€šè¿‡è£å‰ªé¿å…è¿‡å¤§çš„ç­–ç•¥æ›´æ–°
- **æ ·æœ¬æ•ˆç‡**: æ”¯æŒå¤šæ¬¡ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®
- **æ˜“äºè°ƒå‚**: ç›¸å¯¹è¾ƒå°‘çš„è¶…å‚æ•°

#### ç®—æ³•åŸç†

```python
# PPOæ ¸å¿ƒæ›´æ–°å…¬å¼
def compute_policy_loss(self, batch, new_log_probs):
    policy_losses = []
    
    for i, step in enumerate(batch):
        # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = math.exp(new_log_probs[i] - step.log_prob)
        
        # PPOè£å‰ª
        clipped_ratio = max(min(ratio, 1 + self.config.clip_ratio), 
                           1 - self.config.clip_ratio)
        
        # ç­–ç•¥æŸå¤±
        policy_loss = -min(ratio * step.advantage, 
                          clipped_ratio * step.advantage)
        policy_losses.append(policy_loss)
    
    return sum(policy_losses) / len(policy_losses)
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    algorithm: RLAlgorithm = RLAlgorithm.PPO
    learning_rate: float = 3e-4      # å­¦ä¹ ç‡
    gamma: float = 0.99              # æŠ˜æ‰£å› å­
    gae_lambda: float = 0.95         # GAEå‚æ•°
    clip_ratio: float = 0.2          # PPOè£å‰ªæ¯”ç‡
    value_loss_coef: float = 0.5     # ä»·å€¼æŸå¤±ç³»æ•°
    entropy_coef: float = 0.01       # ç†µæŸå¤±ç³»æ•°
    max_grad_norm: float = 0.5       # æ¢¯åº¦è£å‰ª
    batch_size: int = 32             # æ‰¹æ¬¡å¤§å°
    ppo_epochs: int = 4              # PPOæ›´æ–°è½®æ•°
    target_kl: float = 0.01          # ç›®æ ‡KLæ•£åº¦
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_ppo_trainer, RLAlgorithm
from sandgraph.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºLLMç®¡ç†å™¨
llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")

# åˆ›å»ºPPOè®­ç»ƒå™¨
ppo_trainer = create_ppo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# æ·»åŠ ç»éªŒ
ppo_trainer.add_experience(
    state={"user_count": 100, "engagement": 0.5},
    action="CREATE_POST",
    reward=2.5,
    done=False
)

# æ›´æ–°ç­–ç•¥
result = ppo_trainer.update_policy()
print(f"Policy updated: {result}")
```

### 2. GRPO (Group Robust Policy Optimization)

GRPOæ˜¯ä¸€ç§é’ˆå¯¹å¤šç»„æ•°æ®çš„é²æ£’ä¼˜åŒ–ç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†ä¸åŒç”¨æˆ·ç¾¤ä½“æˆ–ç¯å¢ƒæ¡ä»¶çš„æƒ…å†µã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **é²æ£’æ€§**: å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–å…·æœ‰é²æ£’æ€§
- **ç»„ä¼˜åŒ–**: åŒæ—¶ä¼˜åŒ–å¤šä¸ªç»„çš„è¡¨ç°
- **å…¬å¹³æ€§**: é¿å…æŸäº›ç»„è¢«å¿½ç•¥

#### ç®—æ³•åŸç†

```python
def compute_robust_loss(self, group_losses: Dict[str, float]) -> float:
    """è®¡ç®—é²æ£’æŸå¤±"""
    # è®¡ç®—æ¯ç»„æŸå¤±
    group_losses_list = list(group_losses.values())
    
    # é²æ£’æŸå¤±ï¼šè€ƒè™‘æœ€å·®ç»„çš„è¡¨ç°
    worst_group_loss = max(group_losses_list)
    avg_group_loss = sum(group_losses_list) / len(group_losses_list)
    
    # é²æ£’æ€§é¡¹
    robustness_term = self.config.robustness_coef * worst_group_loss
    
    return avg_group_loss + robustness_term
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    # GRPOç‰¹æœ‰å‚æ•°
    group_size: int = 4              # ç»„å¤§å°
    robustness_coef: float = 0.1     # é²æ£’æ€§ç³»æ•°
    
    # å…¶ä»–å‚æ•°ä¸PPOç›¸åŒ
    algorithm: RLAlgorithm = RLAlgorithm.GRPO
    learning_rate: float = 3e-4
    gamma: float = 0.99
    # ...
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_grpo_trainer

# åˆ›å»ºGRPOè®­ç»ƒå™¨
grpo_trainer = create_grpo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    robustness_coef=0.1
)

# ä¸ºä¸åŒç»„æ·»åŠ ç»éªŒ
grpo_trainer.add_experience(
    state={"user_count": 100, "engagement": 0.5},
    action="CREATE_POST",
    reward=2.5,
    done=False,
    group_id="young_users"  # æŒ‡å®šç»„
)

grpo_trainer.add_experience(
    state={"user_count": 50, "engagement": 0.3},
    action="LIKE_POST",
    reward=1.0,
    done=False,
    group_id="senior_users"  # æŒ‡å®šç»„
)

# æ›´æ–°ç­–ç•¥
result = grpo_trainer.update_policy()
print(f"GRPO update result: {result}")
```

### 3. SAC (Soft Actor-Critic)

SACæ˜¯ä¸€ç§åŸºäºæœ€å¤§ç†µçš„Actor-Criticç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œé€šè¿‡æœ€å¤§åŒ–æœŸæœ›å›æŠ¥å’Œç­–ç•¥ç†µæ¥å®ç°æ›´å¥½çš„æ¢ç´¢ã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **æœ€å¤§ç†µ**: åœ¨æœ€å¤§åŒ–å›æŠ¥çš„åŒæ—¶æœ€å¤§åŒ–ç­–ç•¥ç†µ
- **è‡ªåŠ¨è°ƒæ•´**: è‡ªåŠ¨è°ƒæ•´ç†µæ­£åˆ™åŒ–ç³»æ•°
- **æ ·æœ¬æ•ˆç‡**: ä½¿ç”¨ç»éªŒå›æ”¾ç¼“å†²åŒºæé«˜æ ·æœ¬æ•ˆç‡
- **ç¨³å®šæ€§**: è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œæé«˜è®­ç»ƒç¨³å®šæ€§

#### ç®—æ³•åŸç†

```python
def compute_actor_loss(self, batch, new_log_probs):
    """è®¡ç®—ActoræŸå¤±ï¼ˆç­–ç•¥æŸå¤±ï¼‰"""
    actor_losses = []
    
    for i, step in enumerate(batch):
        # SACçš„ActoræŸå¤±ï¼šæœ€å¤§åŒ–Qå€¼å‡å»ç†µæ­£åˆ™åŒ–
        q_value = step.value  # ä»Criticç½‘ç»œè·å–
        entropy = -new_log_probs[i]  # åŠ¨ä½œç†µ
        
        # ActoræŸå¤± = -(Q(s,a) - Î± * log Ï€(a|s))
        actor_loss = -(q_value - self.alpha * new_log_probs[i])
        actor_losses.append(actor_loss)
    
    return sum(actor_losses) / len(actor_losses)

def compute_alpha_loss(self, new_log_probs):
    """è®¡ç®—alphaæŸå¤±ï¼ˆç”¨äºè‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°ï¼‰"""
    # è®¡ç®—å½“å‰ç­–ç•¥çš„ç†µ
    current_entropy = -sum(new_log_probs) / len(new_log_probs)
    
    # AlphaæŸå¤±ï¼šä½¿ç†µæ¥è¿‘ç›®æ ‡ç†µ
    alpha_loss = -self.log_alpha * (current_entropy + self.target_entropy)
    
    return alpha_loss
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    # SACç‰¹æœ‰å‚æ•°
    alpha: float = 0.2                    # ç†µæ­£åˆ™åŒ–ç³»æ•°
    tau: float = 0.005                    # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
    target_update_freq: int = 1           # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
    auto_alpha_tuning: bool = True        # è‡ªåŠ¨è°ƒæ•´alpha
    
    # å…¶ä»–å‚æ•°
    algorithm: RLAlgorithm = RLAlgorithm.SAC
    learning_rate: float = 3e-4
    gamma: float = 0.99
    # ...
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_sac_trainer

# åˆ›å»ºSACè®­ç»ƒå™¨
sac_trainer = create_sac_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# SACè®­ç»ƒå¾ªç¯
for episode in range(1000):
    for step in range(100):
        state = get_continuous_state()
        action = select_continuous_action(state)
        reward = execute_continuous_action(action)
        done = check_continuous_done()
        
        sac_trainer.add_experience(state, action, reward, done)
    
    if episode % 10 == 0:
        result = sac_trainer.update_policy()
        print(f"Episode {episode}: Alpha = {result.get('alpha', 0):.4f}")
```

### 4. TD3 (Twin Delayed Deep Deterministic Policy Gradient)

TD3æ˜¯DDPGçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡åŒQç½‘ç»œã€å»¶è¿Ÿç­–ç•¥æ›´æ–°å’Œç›®æ ‡ç­–ç•¥å¹³æ»‘æ¥è§£å†³è¿‡ä¼°è®¡é—®é¢˜ã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **åŒQç½‘ç»œ**: ä½¿ç”¨ä¸¤ä¸ªCriticç½‘ç»œå‡å°‘è¿‡ä¼°è®¡
- **å»¶è¿Ÿæ›´æ–°**: å»¶è¿Ÿç­–ç•¥æ›´æ–°æé«˜ç¨³å®šæ€§
- **ç›®æ ‡å¹³æ»‘**: ä¸ºç›®æ ‡åŠ¨ä½œæ·»åŠ å™ªå£°æé«˜é²æ£’æ€§
- **å™ªå£°è£å‰ª**: é™åˆ¶ç›®æ ‡ç­–ç•¥å™ªå£°èŒƒå›´

#### ç®—æ³•åŸç†

```python
def compute_critic_loss(self, batch, new_values):
    """è®¡ç®—åŒCriticæŸå¤±"""
    targets = self.compute_td3_q_target(batch)
    
    # åŒQç½‘ç»œæŸå¤±
    critic1_losses = []
    critic2_losses = []
    
    for i, step in enumerate(batch):
        # æ·»åŠ å™ªå£°åˆ°ç›®æ ‡åŠ¨ä½œï¼ˆç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼‰
        noisy_target = targets[i] + random.uniform(-self.config.noise_clip, self.config.noise_clip)
        noisy_target = max(min(noisy_target, targets[i] + self.config.noise_clip), 
                          targets[i] - self.config.noise_clip)
        
        # ä¸¤ä¸ªCriticç½‘ç»œçš„æŸå¤±
        critic1_loss = (new_values[i] - noisy_target) ** 2
        critic2_loss = (new_values[i] + 0.01 - noisy_target) ** 2
        
        critic1_losses.append(critic1_loss)
        critic2_losses.append(critic2_loss)
    
    return (sum(critic1_losses) / len(critic1_losses), 
            sum(critic2_losses) / len(critic2_losses))
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    # TD3ç‰¹æœ‰å‚æ•°
    policy_noise: float = 0.2             # ç­–ç•¥å™ªå£°
    noise_clip: float = 0.5               # å™ªå£°è£å‰ª
    policy_freq: int = 2                  # ç­–ç•¥æ›´æ–°é¢‘ç‡
    delay_freq: int = 2                   # å»¶è¿Ÿæ›´æ–°é¢‘ç‡
    
    # å…¶ä»–å‚æ•°
    algorithm: RLAlgorithm = RLAlgorithm.TD3
    learning_rate: float = 3e-4
    gamma: float = 0.99
    # ...
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_td3_trainer

# åˆ›å»ºTD3è®­ç»ƒå™¨
td3_trainer = create_td3_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# TD3è®­ç»ƒå¾ªç¯
for episode in range(1000):
    for step in range(100):
        state = get_continuous_state()
        action = select_deterministic_action(state)
        reward = execute_deterministic_action(action)
        done = check_deterministic_done()
        
        td3_trainer.add_experience(state, action, reward, done)
    
    if episode % 10 == 0:
        result = td3_trainer.update_policy()
        print(f"Episode {episode}: Policy Updated = {result.get('policy_updated', False)}")
```

## ğŸš€ å¢å¼ºç‰ˆç®—æ³•

### 1. å¢å¼ºç‰ˆRLè®­ç»ƒå™¨

å¢å¼ºç‰ˆç®—æ³•é›†æˆäº†AReaLæ¡†æ¶ï¼Œæä¾›æ›´å¥½çš„ç¼“å­˜ã€æ€§èƒ½å’Œç›‘æ§åŠŸèƒ½ã€‚

#### æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½ç¼“å­˜**: åŸºäºAReaLæ¡†æ¶çš„é«˜æ•ˆç¼“å­˜
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šçº¿ç¨‹æ‰¹å¤„ç†
- **å®æ—¶ç›‘æ§**: è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- **è‡ªé€‚åº”ä¼˜åŒ–**: åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°

#### é…ç½®å‚æ•°

```python
@dataclass
class EnhancedRLConfig(RLConfig):
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_size: int = 10000
    cache_policy: CachePolicy = CachePolicy.LRU
    cache_ttl: int = 3600
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    enable_batching: bool = True
    batch_prefetch: bool = True
    parallel_processing: bool = True
    max_workers: int = 4
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_interval: float = 1.0
    
    # æŒä¹…åŒ–é…ç½®
    enable_persistence: bool = True
    persistence_interval: int = 100
    persistence_path: str = "./cache/rl_cache"
    
    # å†…å­˜ç®¡ç†é…ç½®
    max_memory_usage: float = 0.8
    gc_threshold: int = 1000
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.enhanced_rl_algorithms import (
    create_enhanced_ppo_trainer,
    create_enhanced_grpo_trainer,
    create_optimized_rl_trainer,
    IntegrationLevel
)

# åˆ›å»ºå¢å¼ºç‰ˆPPOè®­ç»ƒå™¨
enhanced_ppo = create_enhanced_ppo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    enable_caching=True
)

# åˆ›å»ºå¢å¼ºç‰ˆGRPOè®­ç»ƒå™¨
enhanced_grpo = create_enhanced_grpo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    robustness_coef=0.1,
    enable_caching=True
)

# åˆ›å»ºä¼˜åŒ–çš„RLè®­ç»ƒå™¨
optimized_trainer = create_optimized_rl_trainer(
    llm_manager=llm_manager,
    algorithm=RLAlgorithm.GRPO,
    cache_size=10000,
    enable_parallel=True
)
```

### 2. ç¼“å­˜ç­–ç•¥

å¢å¼ºç‰ˆç®—æ³•æ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥ï¼š

```python
class CachePolicy(Enum):
    LRU = "lru"        # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"        # æœ€å°‘ä½¿ç”¨
    FIFO = "fifo"      # å…ˆè¿›å…ˆå‡º
    RANDOM = "random"  # éšæœºæ›¿æ¢
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”æ›¿æ¢
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
class EnhancedTrajectoryProcessor:
    """å¢å¼ºç‰ˆè½¨è¿¹å¤„ç†å™¨ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå¹¶è¡Œå¤„ç†"""
    
    def __init__(self, config: EnhancedRLConfig):
        self.config = config
        self.batch_queue = queue.Queue()
        self.processed_batches = deque(maxlen=100)
        
        if config.parallel_processing:
            self._start_worker_threads()
    
    def _process_batch(self, batch: List[TrajectoryStep]) -> Dict[str, Any]:
        """å¤„ç†æ‰¹æ¬¡æ•°æ®"""
        # æå–ç‰¹å¾
        states = [step.state for step in batch]
        actions = [step.action for step in batch]
        rewards = np.array([step.reward for step in batch])
        values = np.array([step.value for step in batch])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "mean_reward": float(np.mean(rewards)),
            "std_reward": float(np.std(rewards)),
            "batch_size": len(batch)
        }
        
        return {
            "states": states,
            "actions": actions,
            "rewards": rewards,
            "values": values,
            "stats": stats
        }
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### 1. è®­ç»ƒç»Ÿè®¡

```python
# è·å–è®­ç»ƒç»Ÿè®¡
stats = trainer.get_training_stats()
print(f"Algorithm: {stats['algorithm']}")
print(f"Training Step: {stats['training_step']}")
print(f"Recent Updates: {stats['recent_updates']}")

# è·å–å¢å¼ºç‰ˆç»Ÿè®¡
enhanced_stats = enhanced_trainer.get_enhanced_stats()
print(f"Cache Stats: {enhanced_stats['cache_stats']}")
print(f"Performance Stats: {enhanced_stats['performance_stats']}")
```

### 2. æ€§èƒ½æŒ‡æ ‡

```python
# ç¼“å­˜æ€§èƒ½
cache_stats = enhanced_stats['cache_stats']
print(f"Cache Hit Rate: {cache_stats['hit_rate']:.3f}")
print(f"Cache Size: {cache_stats['size']}")
print(f"Memory Usage: {cache_stats['memory_usage']:.2f} GB")

# è®­ç»ƒæ€§èƒ½
perf_stats = enhanced_stats['performance_stats']
print(f"Training Steps: {perf_stats['training_steps']}")
print(f"Total Training Time: {perf_stats['total_training_time']:.2f}s")
print(f"Average Update Time: {perf_stats['total_training_time'] / perf_stats['training_steps']:.3f}s")
```

## ğŸ¯ ç®—æ³•é€‰æ‹©æŒ‡å—

### 1. ä½•æ—¶ä½¿ç”¨PPO
- **ç®€å•ç¯å¢ƒ**: çŠ¶æ€ç©ºé—´ç›¸å¯¹ç®€å•
- **ç¨³å®šè®­ç»ƒ**: éœ€è¦ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
- **å¿«é€ŸåŸå‹**: å¿«é€ŸéªŒè¯æƒ³æ³•
- **èµ„æºæœ‰é™**: è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µ

### 2. ä½•æ—¶ä½¿ç”¨GRPO
- **å¤šç»„æ•°æ®**: å¤„ç†ä¸åŒç”¨æˆ·ç¾¤ä½“
- **é²æ£’æ€§è¦æ±‚**: éœ€è¦å¯¹æŠ—æ•°æ®åˆ†å¸ƒå˜åŒ–
- **å…¬å¹³æ€§**: ç¡®ä¿æ‰€æœ‰ç»„éƒ½å¾—åˆ°ä¼˜åŒ–
- **å¤æ‚ç¯å¢ƒ**: ç¯å¢ƒæ¡ä»¶å¤šå˜çš„æƒ…å†µ

### 3. ä½•æ—¶ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³•
- **å¤§è§„æ¨¡è®­ç»ƒ**: å¤„ç†å¤§é‡æ•°æ®
- **æ€§èƒ½è¦æ±‚**: éœ€è¦æœ€ä½³æ€§èƒ½
- **ç›‘æ§éœ€æ±‚**: éœ€è¦è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
- **ç”Ÿäº§ç¯å¢ƒ**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 4. ä½•æ—¶ä½¿ç”¨SAC
- **è¿ç»­åŠ¨ä½œç©ºé—´**: åŠ¨ä½œæ˜¯è¿ç»­å€¼çš„æƒ…å†µ
- **æ¢ç´¢éœ€æ±‚**: éœ€è¦æ›´å¥½çš„æ¢ç´¢ç­–ç•¥
- **æ ·æœ¬æ•ˆç‡**: é‡è§†æ ·æœ¬æ•ˆç‡çš„åœºæ™¯
- **ç¨³å®šæ€§è¦æ±‚**: éœ€è¦ç¨³å®šè®­ç»ƒè¿‡ç¨‹

### 5. ä½•æ—¶ä½¿ç”¨TD3
- **ç¡®å®šæ€§ç­–ç•¥**: éœ€è¦ç¡®å®šæ€§åŠ¨ä½œè¾“å‡º
- **è¿‡ä¼°è®¡é—®é¢˜**: å­˜åœ¨Qå€¼è¿‡ä¼°è®¡çš„æƒ…å†µ
- **ç¨³å®šæ€§ä¼˜å…ˆ**: ä¼˜å…ˆè€ƒè™‘è®­ç»ƒç¨³å®šæ€§
- **è¿ç»­æ§åˆ¶**: è¿ç»­æ§åˆ¶ä»»åŠ¡

## ğŸ”§ æœ€ä½³å®è·µ

### 1. å‚æ•°è°ƒä¼˜

```python
# PPOå‚æ•°è°ƒä¼˜
ppo_config = RLConfig(
    algorithm=RLAlgorithm.PPO,
    learning_rate=3e-4,      # é€‚ä¸­å­¦ä¹ ç‡
    clip_ratio=0.2,          # æ ‡å‡†è£å‰ªæ¯”ç‡
    batch_size=64,           # è¾ƒå¤§æ‰¹æ¬¡
    ppo_epochs=4             # å¤šæ¬¡æ›´æ–°
)

# GRPOå‚æ•°è°ƒä¼˜
grpo_config = RLConfig(
    algorithm=RLAlgorithm.GRPO,
    learning_rate=2e-4,      # ç¨ä½å­¦ä¹ ç‡
    robustness_coef=0.1,     # é€‚ä¸­é²æ£’æ€§
    group_size=4,            # åˆç†ç»„å¤§å°
    batch_size=32            # é€‚ä¸­æ‰¹æ¬¡
)

# SACå‚æ•°è°ƒä¼˜
sac_config = RLConfig(
    algorithm=RLAlgorithm.SAC,
    learning_rate=3e-4,      # æ ‡å‡†å­¦ä¹ ç‡
    alpha=0.2,               # é€‚ä¸­ç†µç³»æ•°
    tau=0.005,               # è½¯æ›´æ–°ç³»æ•°
    auto_alpha_tuning=True,  # å¯ç”¨è‡ªåŠ¨è°ƒæ•´
    batch_size=64            # è¾ƒå¤§æ‰¹æ¬¡
)

# TD3å‚æ•°è°ƒä¼˜
td3_config = RLConfig(
    algorithm=RLAlgorithm.TD3,
    learning_rate=3e-4,      # æ ‡å‡†å­¦ä¹ ç‡
    policy_noise=0.2,        # é€‚ä¸­ç­–ç•¥å™ªå£°
    noise_clip=0.5,          # å™ªå£°è£å‰ª
    policy_freq=2,           # ç­–ç•¥æ›´æ–°é¢‘ç‡
    batch_size=64            # è¾ƒå¤§æ‰¹æ¬¡
)
```

### 2. ç¼“å­˜é…ç½®

```python
# é«˜æ€§èƒ½ç¼“å­˜é…ç½®
enhanced_config = EnhancedRLConfig(
    enable_caching=True,
    cache_size=50000,        # å¤§ç¼“å­˜
    cache_policy=CachePolicy.LRU,
    parallel_processing=True,
    max_workers=8
)

# å†…å­˜ä¼˜åŒ–é…ç½®
memory_optimized_config = EnhancedRLConfig(
    enable_caching=True,
    cache_size=5000,         # å°ç¼“å­˜
    cache_policy=CachePolicy.LFU,
    max_memory_usage=0.6,    # é™åˆ¶å†…å­˜ä½¿ç”¨
    enable_persistence=True  # å¯ç”¨æŒä¹…åŒ–
)
```

### 3. ç›‘æ§è®¾ç½®

```python
# è¯¦ç»†ç›‘æ§é…ç½®
monitoring_config = EnhancedRLConfig(
    enable_metrics=True,
    metrics_interval=0.5,    # é«˜é¢‘ç›‘æ§
    enable_persistence=True,
    persistence_interval=50  # é¢‘ç¹ä¿å­˜
)

# è½»é‡ç›‘æ§é…ç½®
lightweight_config = EnhancedRLConfig(
    enable_metrics=True,
    metrics_interval=5.0,    # ä½é¢‘ç›‘æ§
    enable_persistence=False # ä¸æŒä¹…åŒ–
)
```

## ğŸš€ è¿è¡Œæ¼”ç¤º

### 1. åŸºç¡€ç®—æ³•æ¼”ç¤º

```bash
# è¿è¡ŒPPOæ¼”ç¤º
python demo/enhanced_rl_cache_demo.py --algorithm ppo

# è¿è¡ŒGRPOæ¼”ç¤º
python demo/enhanced_rl_cache_demo.py --algorithm grpo

# è¿è¡Œæ€§èƒ½å¯¹æ¯”
python demo/enhanced_rl_cache_demo.py --compare
```

### 2. å¢å¼ºç‰ˆç®—æ³•æ¼”ç¤º

```bash
# è¿è¡Œå¢å¼ºç‰ˆæ¼”ç¤º
python demo/enhanced_areal_integration_demo.py --demo advanced

# è¿è¡Œæ€§èƒ½æµ‹è¯•
python demo/enhanced_areal_integration_demo.py --demo performance
```

### 3. æ–°ç®—æ³•æ¼”ç¤º

```bash
# è¿è¡ŒSACç®—æ³•æ¼”ç¤º
python demo/new_rl_algorithms_demo.py --demo sac

# è¿è¡ŒTD3ç®—æ³•æ¼”ç¤º
python demo/new_rl_algorithms_demo.py --demo td3

# è¿è¡Œå¢å¼ºç‰ˆæ–°ç®—æ³•æ¼”ç¤º
python demo/new_rl_algorithms_demo.py --demo enhanced

# è¿è¡Œç®—æ³•æ€§èƒ½å¯¹æ¯”
python demo/new_rl_algorithms_demo.py --demo compare
```

## ğŸ“š ç¤ºä¾‹ä»£ç 

### 1. å®Œæ•´çš„è®­ç»ƒå¾ªç¯

```python
from sandgraph.core.enhanced_rl_algorithms import create_optimized_rl_trainer
from sandgraph.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºè®­ç»ƒå™¨
llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
trainer = create_optimized_rl_trainer(
    llm_manager=llm_manager,
    algorithm=RLAlgorithm.GRPO,
    cache_size=10000,
    enable_parallel=True
)

# è®­ç»ƒå¾ªç¯
for episode in range(1000):
    # æ”¶é›†ç»éªŒ
    for step in range(100):
        state = get_current_state()
        action = select_action(state)
        reward = execute_action(action)
        done = check_done()
        
        trainer.add_experience(state, action, reward, done)
    
    # æ›´æ–°ç­–ç•¥
    if episode % 10 == 0:
        result = trainer.update_policy()
        print(f"Episode {episode}: {result}")
    
    # ç›‘æ§æ€§èƒ½
    if episode % 100 == 0:
        stats = trainer.get_enhanced_stats()
        print(f"Performance: {stats['performance_stats']}")
```

### 2. å¤šç»„è®­ç»ƒ

```python
# ä¸ºä¸åŒç”¨æˆ·ç»„è®­ç»ƒ
user_groups = ["young_users", "senior_users", "power_users"]

for group in user_groups:
    for episode in range(500):
        # æ”¶é›†è¯¥ç»„çš„ç»éªŒ
        state = get_group_state(group)
        action = select_action_for_group(state, group)
        reward = execute_group_action(action, group)
        done = check_group_done(group)
        
        trainer.add_experience(state, action, reward, done, group_id=group)
    
    # æ›´æ–°ç­–ç•¥
    result = trainer.update_policy()
    print(f"Group {group} update: {result}")
```

### 3. SACè¿ç»­åŠ¨ä½œè®­ç»ƒ

```python
from sandgraph.core.rl_algorithms import create_sac_trainer

# åˆ›å»ºSACè®­ç»ƒå™¨
sac_trainer = create_sac_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# SACè®­ç»ƒå¾ªç¯
for episode in range(1000):
    for step in range(100):
        state = get_continuous_state()
        action = select_continuous_action(state)
        reward = execute_continuous_action(action)
        done = check_continuous_done()
        
        sac_trainer.add_experience(state, action, reward, done)
    
    if episode % 10 == 0:
        result = sac_trainer.update_policy()
        print(f"Episode {episode}: Alpha = {result.get('alpha', 0):.4f}")
```

### 4. TD3ç¡®å®šæ€§ç­–ç•¥è®­ç»ƒ

```python
from sandgraph.core.rl_algorithms import create_td3_trainer

# åˆ›å»ºTD3è®­ç»ƒå™¨
td3_trainer = create_td3_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# TD3è®­ç»ƒå¾ªç¯
for episode in range(1000):
    for step in range(100):
        state = get_continuous_state()
        action = select_deterministic_action(state)
        reward = execute_deterministic_action(action)
        done = check_deterministic_done()
        
        td3_trainer.add_experience(state, action, reward, done)
    
    if episode % 10 == 0:
        result = td3_trainer.update_policy()
        print(f"Episode {episode}: Policy Updated = {result.get('policy_updated', False)}")
```

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç®—æ³•ï¼Ÿ
A: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©ï¼šç®€å•ä»»åŠ¡ç”¨PPOï¼Œå¤æ‚å¤šç»„ä»»åŠ¡ç”¨GRPOï¼Œé«˜æ€§èƒ½éœ€æ±‚ç”¨å¢å¼ºç‰ˆç®—æ³•ã€‚

### Q: å¦‚ä½•è°ƒä¼˜å­¦ä¹ ç‡ï¼Ÿ
A: ä»3e-4å¼€å§‹ï¼Œæ ¹æ®è®­ç»ƒç¨³å®šæ€§è°ƒæ•´ã€‚ä¸ç¨³å®šæ—¶é™ä½ï¼Œæ”¶æ•›æ…¢æ—¶æé«˜ã€‚

### Q: å¦‚ä½•è®¾ç½®ç¼“å­˜å¤§å°ï¼Ÿ
A: æ ¹æ®å¯ç”¨å†…å­˜å’Œè®¿é—®æ¨¡å¼è®¾ç½®ã€‚å†…å­˜å……è¶³æ—¶è®¾ç½®å¤§ç¼“å­˜ï¼Œè®¿é—®é¢‘ç¹æ—¶ä½¿ç”¨LRUç­–ç•¥ã€‚

### Q: å¦‚ä½•å¤„ç†è®­ç»ƒä¸ç¨³å®šï¼Ÿ
A: é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œè°ƒæ•´è£å‰ªæ¯”ç‡ï¼Œä½¿ç”¨æ¢¯åº¦è£å‰ªã€‚

### Q: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Ÿ
A: ä½¿ç”¨`get_enhanced_stats()`è·å–è¯¦ç»†ç»Ÿè®¡ï¼Œå®šæœŸæ£€æŸ¥æŸå¤±å’Œæ€§èƒ½æŒ‡æ ‡ã€‚

### Q: ä½•æ—¶ä½¿ç”¨SACè€Œä¸æ˜¯PPOï¼Ÿ
A: å½“éœ€è¦è¿ç»­åŠ¨ä½œç©ºé—´ã€æ›´å¥½çš„æ¢ç´¢ç­–ç•¥æˆ–è‡ªåŠ¨ç†µè°ƒæ•´æ—¶ï¼Œé€‰æ‹©SACã€‚

### Q: ä½•æ—¶ä½¿ç”¨TD3è€Œä¸æ˜¯DDPGï¼Ÿ
A: å½“å­˜åœ¨Qå€¼è¿‡ä¼°è®¡é—®é¢˜ã€éœ€è¦æ›´ç¨³å®šçš„è®­ç»ƒæˆ–ç¡®å®šæ€§ç­–ç•¥æ—¶ï¼Œé€‰æ‹©TD3ã€‚

### Q: å¦‚ä½•è°ƒæ•´SACçš„alphaå‚æ•°ï¼Ÿ
A: å¯ç”¨`auto_alpha_tuning=True`è‡ªåŠ¨è°ƒæ•´ï¼Œæˆ–æ‰‹åŠ¨è®¾ç½®åˆé€‚çš„alphaå€¼ï¼ˆ0.1-0.3ï¼‰ã€‚

### Q: TD3çš„ç­–ç•¥æ›´æ–°é¢‘ç‡å¦‚ä½•è®¾ç½®ï¼Ÿ
A: é€šå¸¸è®¾ç½®ä¸º2-4ï¼Œé¢‘ç‡è¶Šé«˜è®­ç»ƒè¶Šç¨³å®šä½†è®¡ç®—å¼€é”€è¶Šå¤§ã€‚

## ğŸ”— ç›¸å…³èµ„æº

- **[AReaLé›†æˆæŒ‡å—](areal_integration_guide.md)** - AReaLæ¡†æ¶é›†æˆ
- **[APIå‚è€ƒ](api_reference.md)** - å®Œæ•´APIæ–‡æ¡£
- **[ç›‘æ§æŒ‡å—](monitoring_guide.md)** - è®­ç»ƒç›‘æ§
- **[ç¤ºä¾‹æŒ‡å—](examples_guide.md)** - æ›´å¤šä½¿ç”¨ç¤ºä¾‹ 