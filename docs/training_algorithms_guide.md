# è®­ç»ƒç®—æ³•æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»SandGraphXä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•ï¼ŒåŒ…æ‹¬PPOã€GRPOç­‰ç®—æ³•çš„åŸç†ã€é…ç½®å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸš€ ç®—æ³•æ¦‚è¿°

SandGraphXæä¾›äº†å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¼˜åŒ–LLMçš„å†³ç­–ç­–ç•¥ï¼š

- **PPO (Proximal Policy Optimization)** - è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•
- **GRPO (Group Robust Policy Optimization)** - ç»„é²æ£’ç­–ç•¥ä¼˜åŒ–ç®—æ³•
- **å¢å¼ºç‰ˆç®—æ³•** - é›†æˆAReaLæ¡†æ¶çš„ä¼˜åŒ–ç‰ˆæœ¬

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. è½¨è¿¹ (Trajectory)
è½¨è¿¹æ˜¯æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’çš„å®Œæ•´åºåˆ—ï¼ŒåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯ã€‚

```python
@dataclass
class TrajectoryStep:
    state: Dict[str, Any]      # ç¯å¢ƒçŠ¶æ€
    action: str                # æ™ºèƒ½ä½“åŠ¨ä½œ
    reward: float              # å³æ—¶å¥–åŠ±
    value: float               # çŠ¶æ€ä»·å€¼ä¼°è®¡
    log_prob: float            # åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    done: bool                 # æ˜¯å¦ç»“æŸ
    advantage: float = 0.0     # ä¼˜åŠ¿å‡½æ•°
    return_: float = 0.0       # ç´¯ç§¯å›æŠ¥
```

### 2. ä¼˜åŠ¿å‡½æ•° (Advantage Function)
è¡¡é‡æŸä¸ªåŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿ï¼Œç”¨äºæŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚

### 3. ç­–ç•¥æ¢¯åº¦ (Policy Gradient)
åŸºäºç­–ç•¥æ¢¯åº¦çš„ç®—æ³•ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ã€‚

## ğŸ”§ åŸºç¡€ç®—æ³•

### 1. PPO (Proximal Policy Optimization)

PPOæ˜¯ä¸€ç§ç¨³å®šã€é«˜æ•ˆçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡è£å‰ªç›®æ ‡å‡½æ•°æ¥é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **ç¨³å®šæ€§**: é€šè¿‡è£å‰ªé¿å…è¿‡å¤§çš„ç­–ç•¥æ›´æ–°
- **æ ·æœ¬æ•ˆç‡**: æ”¯æŒå¤šæ¬¡ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®
- **æ˜“äºè°ƒå‚**: ç›¸å¯¹è¾ƒå°‘çš„è¶…å‚æ•°

#### ç®—æ³•åŸç†

```python
# PPOæ ¸å¿ƒæ›´æ–°å…¬å¼
def compute_policy_loss(self, batch, new_log_probs):
    policy_losses = []
    
    for i, step in enumerate(batch):
        # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = math.exp(new_log_probs[i] - step.log_prob)
        
        # PPOè£å‰ª
        clipped_ratio = max(min(ratio, 1 + self.config.clip_ratio), 
                           1 - self.config.clip_ratio)
        
        # ç­–ç•¥æŸå¤±
        policy_loss = -min(ratio * step.advantage, 
                          clipped_ratio * step.advantage)
        policy_losses.append(policy_loss)
    
    return sum(policy_losses) / len(policy_losses)
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    algorithm: RLAlgorithm = RLAlgorithm.PPO
    learning_rate: float = 3e-4      # å­¦ä¹ ç‡
    gamma: float = 0.99              # æŠ˜æ‰£å› å­
    gae_lambda: float = 0.95         # GAEå‚æ•°
    clip_ratio: float = 0.2          # PPOè£å‰ªæ¯”ç‡
    value_loss_coef: float = 0.5     # ä»·å€¼æŸå¤±ç³»æ•°
    entropy_coef: float = 0.01       # ç†µæŸå¤±ç³»æ•°
    max_grad_norm: float = 0.5       # æ¢¯åº¦è£å‰ª
    batch_size: int = 32             # æ‰¹æ¬¡å¤§å°
    ppo_epochs: int = 4              # PPOæ›´æ–°è½®æ•°
    target_kl: float = 0.01          # ç›®æ ‡KLæ•£åº¦
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_ppo_trainer, RLAlgorithm
from sandgraph.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºLLMç®¡ç†å™¨
llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")

# åˆ›å»ºPPOè®­ç»ƒå™¨
ppo_trainer = create_ppo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4
)

# æ·»åŠ ç»éªŒ
ppo_trainer.add_experience(
    state={"user_count": 100, "engagement": 0.5},
    action="CREATE_POST",
    reward=2.5,
    done=False
)

# æ›´æ–°ç­–ç•¥
result = ppo_trainer.update_policy()
print(f"Policy updated: {result}")
```

### 2. GRPO (Group Robust Policy Optimization)

GRPOæ˜¯ä¸€ç§é’ˆå¯¹å¤šç»„æ•°æ®çš„é²æ£’ä¼˜åŒ–ç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†ä¸åŒç”¨æˆ·ç¾¤ä½“æˆ–ç¯å¢ƒæ¡ä»¶çš„æƒ…å†µã€‚

#### æ ¸å¿ƒç‰¹ç‚¹
- **é²æ£’æ€§**: å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–å…·æœ‰é²æ£’æ€§
- **ç»„ä¼˜åŒ–**: åŒæ—¶ä¼˜åŒ–å¤šä¸ªç»„çš„è¡¨ç°
- **å…¬å¹³æ€§**: é¿å…æŸäº›ç»„è¢«å¿½ç•¥

#### ç®—æ³•åŸç†

```python
def compute_robust_loss(self, group_losses: Dict[str, float]) -> float:
    """è®¡ç®—é²æ£’æŸå¤±"""
    # è®¡ç®—æ¯ç»„æŸå¤±
    group_losses_list = list(group_losses.values())
    
    # é²æ£’æŸå¤±ï¼šè€ƒè™‘æœ€å·®ç»„çš„è¡¨ç°
    worst_group_loss = max(group_losses_list)
    avg_group_loss = sum(group_losses_list) / len(group_losses_list)
    
    # é²æ£’æ€§é¡¹
    robustness_term = self.config.robustness_coef * worst_group_loss
    
    return avg_group_loss + robustness_term
```

#### é…ç½®å‚æ•°

```python
@dataclass
class RLConfig:
    # GRPOç‰¹æœ‰å‚æ•°
    group_size: int = 4              # ç»„å¤§å°
    robustness_coef: float = 0.1     # é²æ£’æ€§ç³»æ•°
    
    # å…¶ä»–å‚æ•°ä¸PPOç›¸åŒ
    algorithm: RLAlgorithm = RLAlgorithm.GRPO
    learning_rate: float = 3e-4
    gamma: float = 0.99
    # ...
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.rl_algorithms import create_grpo_trainer

# åˆ›å»ºGRPOè®­ç»ƒå™¨
grpo_trainer = create_grpo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    robustness_coef=0.1
)

# ä¸ºä¸åŒç»„æ·»åŠ ç»éªŒ
grpo_trainer.add_experience(
    state={"user_count": 100, "engagement": 0.5},
    action="CREATE_POST",
    reward=2.5,
    done=False,
    group_id="young_users"  # æŒ‡å®šç»„
)

grpo_trainer.add_experience(
    state={"user_count": 50, "engagement": 0.3},
    action="LIKE_POST",
    reward=1.0,
    done=False,
    group_id="senior_users"  # æŒ‡å®šç»„
)

# æ›´æ–°ç­–ç•¥
result = grpo_trainer.update_policy()
print(f"GRPO update result: {result}")
```

## ğŸš€ å¢å¼ºç‰ˆç®—æ³•

### 1. å¢å¼ºç‰ˆRLè®­ç»ƒå™¨

å¢å¼ºç‰ˆç®—æ³•é›†æˆäº†AReaLæ¡†æ¶ï¼Œæä¾›æ›´å¥½çš„ç¼“å­˜ã€æ€§èƒ½å’Œç›‘æ§åŠŸèƒ½ã€‚

#### æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½ç¼“å­˜**: åŸºäºAReaLæ¡†æ¶çš„é«˜æ•ˆç¼“å­˜
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šçº¿ç¨‹æ‰¹å¤„ç†
- **å®æ—¶ç›‘æ§**: è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- **è‡ªé€‚åº”ä¼˜åŒ–**: åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°

#### é…ç½®å‚æ•°

```python
@dataclass
class EnhancedRLConfig(RLConfig):
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_size: int = 10000
    cache_policy: CachePolicy = CachePolicy.LRU
    cache_ttl: int = 3600
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    enable_batching: bool = True
    batch_prefetch: bool = True
    parallel_processing: bool = True
    max_workers: int = 4
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_interval: float = 1.0
    
    # æŒä¹…åŒ–é…ç½®
    enable_persistence: bool = True
    persistence_interval: int = 100
    persistence_path: str = "./cache/rl_cache"
    
    # å†…å­˜ç®¡ç†é…ç½®
    max_memory_usage: float = 0.8
    gc_threshold: int = 1000
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from sandgraph.core.enhanced_rl_algorithms import (
    create_enhanced_ppo_trainer,
    create_enhanced_grpo_trainer,
    create_optimized_rl_trainer,
    IntegrationLevel
)

# åˆ›å»ºå¢å¼ºç‰ˆPPOè®­ç»ƒå™¨
enhanced_ppo = create_enhanced_ppo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    enable_caching=True
)

# åˆ›å»ºå¢å¼ºç‰ˆGRPOè®­ç»ƒå™¨
enhanced_grpo = create_enhanced_grpo_trainer(
    llm_manager=llm_manager,
    learning_rate=3e-4,
    robustness_coef=0.1,
    enable_caching=True
)

# åˆ›å»ºä¼˜åŒ–çš„RLè®­ç»ƒå™¨
optimized_trainer = create_optimized_rl_trainer(
    llm_manager=llm_manager,
    algorithm=RLAlgorithm.GRPO,
    cache_size=10000,
    enable_parallel=True
)
```

### 2. ç¼“å­˜ç­–ç•¥

å¢å¼ºç‰ˆç®—æ³•æ”¯æŒå¤šç§ç¼“å­˜ç­–ç•¥ï¼š

```python
class CachePolicy(Enum):
    LRU = "lru"        # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"        # æœ€å°‘ä½¿ç”¨
    FIFO = "fifo"      # å…ˆè¿›å…ˆå‡º
    RANDOM = "random"  # éšæœºæ›¿æ¢
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”æ›¿æ¢
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
class EnhancedTrajectoryProcessor:
    """å¢å¼ºç‰ˆè½¨è¿¹å¤„ç†å™¨ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå¹¶è¡Œå¤„ç†"""
    
    def __init__(self, config: EnhancedRLConfig):
        self.config = config
        self.batch_queue = queue.Queue()
        self.processed_batches = deque(maxlen=100)
        
        if config.parallel_processing:
            self._start_worker_threads()
    
    def _process_batch(self, batch: List[TrajectoryStep]) -> Dict[str, Any]:
        """å¤„ç†æ‰¹æ¬¡æ•°æ®"""
        # æå–ç‰¹å¾
        states = [step.state for step in batch]
        actions = [step.action for step in batch]
        rewards = np.array([step.reward for step in batch])
        values = np.array([step.value for step in batch])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "mean_reward": float(np.mean(rewards)),
            "std_reward": float(np.std(rewards)),
            "batch_size": len(batch)
        }
        
        return {
            "states": states,
            "actions": actions,
            "rewards": rewards,
            "values": values,
            "stats": stats
        }
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### 1. è®­ç»ƒç»Ÿè®¡

```python
# è·å–è®­ç»ƒç»Ÿè®¡
stats = trainer.get_training_stats()
print(f"Algorithm: {stats['algorithm']}")
print(f"Training Step: {stats['training_step']}")
print(f"Recent Updates: {stats['recent_updates']}")

# è·å–å¢å¼ºç‰ˆç»Ÿè®¡
enhanced_stats = enhanced_trainer.get_enhanced_stats()
print(f"Cache Stats: {enhanced_stats['cache_stats']}")
print(f"Performance Stats: {enhanced_stats['performance_stats']}")
```

### 2. æ€§èƒ½æŒ‡æ ‡

```python
# ç¼“å­˜æ€§èƒ½
cache_stats = enhanced_stats['cache_stats']
print(f"Cache Hit Rate: {cache_stats['hit_rate']:.3f}")
print(f"Cache Size: {cache_stats['size']}")
print(f"Memory Usage: {cache_stats['memory_usage']:.2f} GB")

# è®­ç»ƒæ€§èƒ½
perf_stats = enhanced_stats['performance_stats']
print(f"Training Steps: {perf_stats['training_steps']}")
print(f"Total Training Time: {perf_stats['total_training_time']:.2f}s")
print(f"Average Update Time: {perf_stats['total_training_time'] / perf_stats['training_steps']:.3f}s")
```

## ğŸ¯ ç®—æ³•é€‰æ‹©æŒ‡å—

### 1. ä½•æ—¶ä½¿ç”¨PPO
- **ç®€å•ç¯å¢ƒ**: çŠ¶æ€ç©ºé—´ç›¸å¯¹ç®€å•
- **ç¨³å®šè®­ç»ƒ**: éœ€è¦ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
- **å¿«é€ŸåŸå‹**: å¿«é€ŸéªŒè¯æƒ³æ³•
- **èµ„æºæœ‰é™**: è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µ

### 2. ä½•æ—¶ä½¿ç”¨GRPO
- **å¤šç»„æ•°æ®**: å¤„ç†ä¸åŒç”¨æˆ·ç¾¤ä½“
- **é²æ£’æ€§è¦æ±‚**: éœ€è¦å¯¹æŠ—æ•°æ®åˆ†å¸ƒå˜åŒ–
- **å…¬å¹³æ€§**: ç¡®ä¿æ‰€æœ‰ç»„éƒ½å¾—åˆ°ä¼˜åŒ–
- **å¤æ‚ç¯å¢ƒ**: ç¯å¢ƒæ¡ä»¶å¤šå˜çš„æƒ…å†µ

### 3. ä½•æ—¶ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³•
- **å¤§è§„æ¨¡è®­ç»ƒ**: å¤„ç†å¤§é‡æ•°æ®
- **æ€§èƒ½è¦æ±‚**: éœ€è¦æœ€ä½³æ€§èƒ½
- **ç›‘æ§éœ€æ±‚**: éœ€è¦è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
- **ç”Ÿäº§ç¯å¢ƒ**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## ğŸ”§ æœ€ä½³å®è·µ

### 1. å‚æ•°è°ƒä¼˜

```python
# PPOå‚æ•°è°ƒä¼˜
ppo_config = RLConfig(
    algorithm=RLAlgorithm.PPO,
    learning_rate=3e-4,      # é€‚ä¸­å­¦ä¹ ç‡
    clip_ratio=0.2,          # æ ‡å‡†è£å‰ªæ¯”ç‡
    batch_size=64,           # è¾ƒå¤§æ‰¹æ¬¡
    ppo_epochs=4             # å¤šæ¬¡æ›´æ–°
)

# GRPOå‚æ•°è°ƒä¼˜
grpo_config = RLConfig(
    algorithm=RLAlgorithm.GRPO,
    learning_rate=2e-4,      # ç¨ä½å­¦ä¹ ç‡
    robustness_coef=0.1,     # é€‚ä¸­é²æ£’æ€§
    group_size=4,            # åˆç†ç»„å¤§å°
    batch_size=32            # é€‚ä¸­æ‰¹æ¬¡
)
```

### 2. ç¼“å­˜é…ç½®

```python
# é«˜æ€§èƒ½ç¼“å­˜é…ç½®
enhanced_config = EnhancedRLConfig(
    enable_caching=True,
    cache_size=50000,        # å¤§ç¼“å­˜
    cache_policy=CachePolicy.LRU,
    parallel_processing=True,
    max_workers=8
)

# å†…å­˜ä¼˜åŒ–é…ç½®
memory_optimized_config = EnhancedRLConfig(
    enable_caching=True,
    cache_size=5000,         # å°ç¼“å­˜
    cache_policy=CachePolicy.LFU,
    max_memory_usage=0.6,    # é™åˆ¶å†…å­˜ä½¿ç”¨
    enable_persistence=True  # å¯ç”¨æŒä¹…åŒ–
)
```

### 3. ç›‘æ§è®¾ç½®

```python
# è¯¦ç»†ç›‘æ§é…ç½®
monitoring_config = EnhancedRLConfig(
    enable_metrics=True,
    metrics_interval=0.5,    # é«˜é¢‘ç›‘æ§
    enable_persistence=True,
    persistence_interval=50  # é¢‘ç¹ä¿å­˜
)

# è½»é‡ç›‘æ§é…ç½®
lightweight_config = EnhancedRLConfig(
    enable_metrics=True,
    metrics_interval=5.0,    # ä½é¢‘ç›‘æ§
    enable_persistence=False # ä¸æŒä¹…åŒ–
)
```

## ğŸš€ è¿è¡Œæ¼”ç¤º

### 1. åŸºç¡€ç®—æ³•æ¼”ç¤º

```bash
# è¿è¡ŒPPOæ¼”ç¤º
python demo/enhanced_rl_cache_demo.py --algorithm ppo

# è¿è¡ŒGRPOæ¼”ç¤º
python demo/enhanced_rl_cache_demo.py --algorithm grpo

# è¿è¡Œæ€§èƒ½å¯¹æ¯”
python demo/enhanced_rl_cache_demo.py --compare
```

### 2. å¢å¼ºç‰ˆç®—æ³•æ¼”ç¤º

```bash
# è¿è¡Œå¢å¼ºç‰ˆæ¼”ç¤º
python demo/enhanced_areal_integration_demo.py --demo advanced

# è¿è¡Œæ€§èƒ½æµ‹è¯•
python demo/enhanced_areal_integration_demo.py --demo performance
```

## ğŸ“š ç¤ºä¾‹ä»£ç 

### 1. å®Œæ•´çš„è®­ç»ƒå¾ªç¯

```python
from sandgraph.core.enhanced_rl_algorithms import create_optimized_rl_trainer
from sandgraph.core.llm_interface import create_shared_llm_manager

# åˆ›å»ºè®­ç»ƒå™¨
llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
trainer = create_optimized_rl_trainer(
    llm_manager=llm_manager,
    algorithm=RLAlgorithm.GRPO,
    cache_size=10000,
    enable_parallel=True
)

# è®­ç»ƒå¾ªç¯
for episode in range(1000):
    # æ”¶é›†ç»éªŒ
    for step in range(100):
        state = get_current_state()
        action = select_action(state)
        reward = execute_action(action)
        done = check_done()
        
        trainer.add_experience(state, action, reward, done)
    
    # æ›´æ–°ç­–ç•¥
    if episode % 10 == 0:
        result = trainer.update_policy()
        print(f"Episode {episode}: {result}")
    
    # ç›‘æ§æ€§èƒ½
    if episode % 100 == 0:
        stats = trainer.get_enhanced_stats()
        print(f"Performance: {stats['performance_stats']}")
```

### 2. å¤šç»„è®­ç»ƒ

```python
# ä¸ºä¸åŒç”¨æˆ·ç»„è®­ç»ƒ
user_groups = ["young_users", "senior_users", "power_users"]

for group in user_groups:
    for episode in range(500):
        # æ”¶é›†è¯¥ç»„çš„ç»éªŒ
        state = get_group_state(group)
        action = select_action_for_group(state, group)
        reward = execute_group_action(action, group)
        done = check_group_done(group)
        
        trainer.add_experience(state, action, reward, done, group_id=group)
    
    # æ›´æ–°ç­–ç•¥
    result = trainer.update_policy()
    print(f"Group {group} update: {result}")
```

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç®—æ³•ï¼Ÿ
A: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©ï¼šç®€å•ä»»åŠ¡ç”¨PPOï¼Œå¤æ‚å¤šç»„ä»»åŠ¡ç”¨GRPOï¼Œé«˜æ€§èƒ½éœ€æ±‚ç”¨å¢å¼ºç‰ˆç®—æ³•ã€‚

### Q: å¦‚ä½•è°ƒä¼˜å­¦ä¹ ç‡ï¼Ÿ
A: ä»3e-4å¼€å§‹ï¼Œæ ¹æ®è®­ç»ƒç¨³å®šæ€§è°ƒæ•´ã€‚ä¸ç¨³å®šæ—¶é™ä½ï¼Œæ”¶æ•›æ…¢æ—¶æé«˜ã€‚

### Q: å¦‚ä½•è®¾ç½®ç¼“å­˜å¤§å°ï¼Ÿ
A: æ ¹æ®å¯ç”¨å†…å­˜å’Œè®¿é—®æ¨¡å¼è®¾ç½®ã€‚å†…å­˜å……è¶³æ—¶è®¾ç½®å¤§ç¼“å­˜ï¼Œè®¿é—®é¢‘ç¹æ—¶ä½¿ç”¨LRUç­–ç•¥ã€‚

### Q: å¦‚ä½•å¤„ç†è®­ç»ƒä¸ç¨³å®šï¼Ÿ
A: é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œè°ƒæ•´è£å‰ªæ¯”ç‡ï¼Œä½¿ç”¨æ¢¯åº¦è£å‰ªã€‚

### Q: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Ÿ
A: ä½¿ç”¨`get_enhanced_stats()`è·å–è¯¦ç»†ç»Ÿè®¡ï¼Œå®šæœŸæ£€æŸ¥æŸå¤±å’Œæ€§èƒ½æŒ‡æ ‡ã€‚

## ğŸ”— ç›¸å…³èµ„æº

- **[AReaLé›†æˆæŒ‡å—](areal_integration_guide.md)** - AReaLæ¡†æ¶é›†æˆ
- **[APIå‚è€ƒ](api_reference.md)** - å®Œæ•´APIæ–‡æ¡£
- **[ç›‘æ§æŒ‡å—](monitoring_guide.md)** - è®­ç»ƒç›‘æ§
- **[ç¤ºä¾‹æŒ‡å—](examples_guide.md)** - æ›´å¤šä½¿ç”¨ç¤ºä¾‹ 