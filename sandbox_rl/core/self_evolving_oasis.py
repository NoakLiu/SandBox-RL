"""
è‡ªè¿›åŒ–Oasisç³»ç»Ÿ - é›†æˆLoRAå‹ç¼©å’Œè‡ªè¿›åŒ–LLMåŠŸèƒ½
===============================================

åœ¨åŸå§‹OasisåŸºç¡€ä¸Šé›†æˆï¼š
1. LoRAæ¨¡å‹å‚æ•°å‹ç¼© - æ”¯æŒæ›´å¤šæ¨¡å‹åŒæ—¶è¿è¡Œ
2. KVç¼“å­˜å‹ç¼© - æé«˜æ¨ç†æ•ˆç‡
3. åœ¨çº¿æ¨¡å‹é€‚é… - æ ¹æ®ç¤¾äº¤ç½‘ç»œåŠ¨æ€è°ƒæ•´æ¨¡å‹
4. è‡ªè¿›åŒ–å­¦ä¹  - æ¨¡å‹åœ¨è¿è¡Œä¸­ä¸æ–­ä¼˜åŒ–
5. å¤šæ¨¡å‹ååŒ - ä¸åŒæ¨¡å‹å¤„ç†ä¸åŒä»»åŠ¡
"""

import logging
import time
import json
import os
import threading
import asyncio
import random
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
import math
import hashlib
from datetime import datetime, timedelta

# å¯¼å…¥Sandbox-RLæ ¸å¿ƒæ¨¡å—
from .llm_interface import (
    create_shared_llm_manager, SharedLLMManager, LLMConfig, LLMBackend
)
from .lora_compression import (
    create_lora_compressor, create_online_lora_manager,
    LoRACompressionConfig, CompressionType, LoRAConfig
)
from .rl_algorithms import RLTrainer, RLConfig, RLAlgorithm
from .monitoring import MonitoringConfig, create_monitor

logger = logging.getLogger(__name__)


class EvolutionStrategy(Enum):
    """è¿›åŒ–ç­–ç•¥"""
    GRADIENT_BASED = "gradient_based"  # åŸºäºæ¢¯åº¦çš„è¿›åŒ–
    META_LEARNING = "meta_learning"    # å…ƒå­¦ä¹ è¿›åŒ–
    ADAPTIVE_COMPRESSION = "adaptive_compression"  # è‡ªé€‚åº”å‹ç¼©
    MULTI_MODEL = "multi_model"        # å¤šæ¨¡å‹ååŒ


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹"""
    CONTENT_GENERATION = "content_generation"  # å†…å®¹ç”Ÿæˆ
    BEHAVIOR_ANALYSIS = "behavior_analysis"    # è¡Œä¸ºåˆ†æ
    NETWORK_OPTIMIZATION = "network_optimization"  # ç½‘ç»œä¼˜åŒ–
    TREND_PREDICTION = "trend_prediction"      # è¶‹åŠ¿é¢„æµ‹
    USER_ENGAGEMENT = "user_engagement"        # ç”¨æˆ·å‚ä¸åº¦


@dataclass
class SelfEvolvingConfig:
    """è‡ªè¿›åŒ–é…ç½®"""
    # åŸºç¡€é…ç½®
    evolution_strategy: EvolutionStrategy = EvolutionStrategy.MULTI_MODEL
    enable_lora: bool = True
    enable_kv_cache_compression: bool = True
    enable_online_adaptation: bool = True
    
    # LoRAé…ç½®
    lora_rank: int = 8
    lora_alpha: float = 16.0
    lora_dropout: float = 0.1
    
    # è¿›åŒ–é…ç½®
    adaptation_learning_rate: float = 1e-4
    evolution_interval: int = 10  # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡è¿›åŒ–
    performance_threshold: float = 0.7  # æ€§èƒ½é˜ˆå€¼
    
    # å¤šæ¨¡å‹é…ç½®
    model_pool_size: int = 3  # æ¨¡å‹æ± å¤§å°
    task_distribution: Dict[TaskType, str] = field(default_factory=lambda: {
        TaskType.CONTENT_GENERATION: "mistralai/Mistral-7B-Instruct-v0.2",
        TaskType.BEHAVIOR_ANALYSIS: "Qwen/Qwen-1_8B-Chat",
        TaskType.NETWORK_OPTIMIZATION: "microsoft/Phi-2"
    })
    
    # ç›‘æ§é…ç½®
    enable_monitoring: bool = True
    metrics_sampling_interval: float = 2.0
    performance_history_size: int = 100


class SelfEvolvingLLM:
    """è‡ªè¿›åŒ–LLM"""
    
    def __init__(self, config: SelfEvolvingConfig):
        self.config = config
        self.evolution_step = 0
        self.performance_history = deque(maxlen=config.performance_history_size)
        self.model_pool = {}
        self.task_assignments = {}
        self.lora_adapters = {}
        
        # åˆå§‹åŒ–æ¨¡å‹æ± 
        self._initialize_model_pool()
        
        # åˆå§‹åŒ–LoRAç®¡ç†å™¨
        if config.enable_lora:
            self._setup_lora_managers()
        
        # åˆå§‹åŒ–ç›‘æ§
        if config.enable_monitoring:
            self._setup_monitoring()
        
        # åˆå§‹åŒ–RLè®­ç»ƒå™¨
        self._setup_rl_trainer()
        
        logger.info("è‡ªè¿›åŒ–LLMåˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_model_pool(self):
        """åˆå§‹åŒ–æ¨¡å‹æ± """
        logger.info("åˆå§‹åŒ–æ¨¡å‹æ± ...")
        
        for task_type, model_name in self.config.task_distribution.items():
            try:
                # åˆ›å»ºä¸“é—¨çš„LLMç®¡ç†å™¨
                llm_manager = create_shared_llm_manager(
                    model_name=model_name,
                    backend="huggingface",
                    device="auto",
                    enable_lora=self.config.enable_lora,
                    lora_rank=self.config.lora_rank,
                    lora_alpha=self.config.lora_alpha,
                    lora_dropout=self.config.lora_dropout,
                    enable_kv_cache_compression=self.config.enable_kv_cache_compression
                )
                
                # æ³¨å†Œä»»åŠ¡èŠ‚ç‚¹
                node_id = f"{task_type.value}_node"
                llm_manager.register_node(node_id, {
                    "role": f"{task_type.value}ä¸“å®¶",
                    "temperature": 0.7,
                    "max_length": 512
                })
                
                self.model_pool[task_type] = {
                    "manager": llm_manager,
                    "node_id": node_id,
                    "model_name": model_name,
                    "performance": 0.0,
                    "usage_count": 0,
                    "last_used": None
                }
                
                logger.info(f"æ¨¡å‹æ± æ·»åŠ : {task_type.value} -> {model_name}")
                
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–æ¨¡å‹å¤±è´¥ {task_type.value}: {e}")
    
    def _setup_lora_managers(self):
        """è®¾ç½®LoRAç®¡ç†å™¨"""
        logger.info("è®¾ç½®LoRAç®¡ç†å™¨...")
        
        for task_type, model_info in self.model_pool.items():
            try:
                # åˆ›å»ºLoRAå‹ç¼©å™¨
                compressor = create_lora_compressor(
                    compression_type=CompressionType.HYBRID,
                    rank=self.config.lora_rank,
                    alpha=self.config.lora_alpha,
                    dropout=self.config.lora_dropout,
                    enable_online_adaptation=self.config.enable_online_adaptation
                )
                
                # åˆ›å»ºé€‚é…å™¨
                adapter_id = compressor.create_adapter(model_info["model_name"])
                
                self.lora_adapters[task_type] = {
                    "compressor": compressor,
                    "adapter_id": adapter_id,
                    "adaptation_count": 0
                }
                
                logger.info(f"LoRAé€‚é…å™¨åˆ›å»º: {task_type.value} -> {adapter_id}")
                
            except Exception as e:
                logger.error(f"LoRAè®¾ç½®å¤±è´¥ {task_type.value}: {e}")
    
    def _setup_monitoring(self):
        """è®¾ç½®ç›‘æ§"""
        logger.info("è®¾ç½®ç›‘æ§ç³»ç»Ÿ...")
        
        monitor_config = MonitoringConfig(
            enable_wandb=False,  # é»˜è®¤å…³é—­WandB
            enable_tensorboard=True,
            wandb_project_name="self-evolving-oasis",
            wandb_run_name=f"evolution_{int(time.time())}",
            tensorboard_log_dir="./logs/self_evolving_oasis",
            log_file_path="./logs/self_evolving_oasis_metrics.json",
            metrics_sampling_interval=self.config.metrics_sampling_interval
        )
        
        self.monitor = create_monitor(monitor_config)
        logger.info("ç›‘æ§ç³»ç»Ÿè®¾ç½®å®Œæˆ")
    
    def _setup_rl_trainer(self):
        """è®¾ç½®RLè®­ç»ƒå™¨"""
        logger.info("è®¾ç½®RLè®­ç»ƒå™¨...")
        
        rl_config = RLConfig(
            algorithm=RLAlgorithm.PPO,
            learning_rate=self.config.adaptation_learning_rate,
            batch_size=16,
            gamma=0.99,
            gae_lambda=0.95,
            clip_ratio=0.2,
            value_loss_coef=0.5,
            entropy_coef=0.01
        )
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºRLè®­ç»ƒå™¨çš„åŸºç¡€
        if self.model_pool:
            first_model = next(iter(self.model_pool.values()))
            self.rl_trainer = RLTrainer(rl_config, first_model["manager"])
        else:
            self.rl_trainer = None
            logger.warning("æ¨¡å‹æ± ä¸ºç©ºï¼ŒRLè®­ç»ƒå™¨æœªåˆå§‹åŒ–") 
    
    def process_task(self, task_type: TaskType, prompt: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†ä»»åŠ¡"""
        if task_type not in self.model_pool:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")
        
        model_info = self.model_pool[task_type]
        llm_manager = model_info["manager"]
        node_id = model_info["node_id"]
        
        # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
        model_info["usage_count"] += 1
        model_info["last_used"] = datetime.now()
        
        try:
            # ç”Ÿæˆå“åº”
            start_time = time.time()
            response = llm_manager.generate_for_node(node_id, prompt)
            generation_time = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance_score = self._calculate_performance_score(response, generation_time)
            model_info["performance"] = performance_score
            
            # è®°å½•æ€§èƒ½å†å²
            self.performance_history.append({
                "task_type": task_type.value,
                "performance": performance_score,
                "generation_time": generation_time,
                "timestamp": datetime.now()
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›åŒ–
            if self.evolution_step % self.config.evolution_interval == 0:
                self._trigger_evolution()
            
            self.evolution_step += 1
            
            return {
                "response": response,
                "task_type": task_type.value,
                "model_name": model_info["model_name"],
                "performance_score": performance_score,
                "generation_time": generation_time,
                "evolution_step": self.evolution_step
            }
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥ {task_type.value}: {e}")
            return {
                "error": str(e),
                "task_type": task_type.value,
                "model_name": model_info["model_name"]
            }
    
    def _calculate_performance_score(self, response, generation_time: float) -> float:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        # åŸºç¡€åˆ†æ•°ï¼ˆåŸºäºå“åº”è´¨é‡ï¼‰
        base_score = min(1.0, len(response.text) / 100)  # å“åº”é•¿åº¦åˆ†æ•°
        
        # æ—¶é—´åˆ†æ•°ï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
        time_score = max(0.0, 1.0 - generation_time / 10.0)  # 10ç§’å†…å®Œæˆå¾—æ»¡åˆ†
        
        # ç½®ä¿¡åº¦åˆ†æ•°
        confidence_score = response.confidence if hasattr(response, 'confidence') else 0.5
        
        # ç»¼åˆåˆ†æ•°
        performance_score = (base_score * 0.4 + time_score * 0.3 + confidence_score * 0.3)
        
        return performance_score
    
    def _trigger_evolution(self):
        """è§¦å‘è¿›åŒ–"""
        logger.info(f"è§¦å‘è¿›åŒ–æ­¥éª¤ {self.evolution_step}")
        
        if self.config.evolution_strategy == EvolutionStrategy.ADAPTIVE_COMPRESSION:
            self._adaptive_compression_evolution()
        elif self.config.evolution_strategy == EvolutionStrategy.MULTI_MODEL:
            self._multi_model_evolution()
        elif self.config.evolution_strategy == EvolutionStrategy.GRADIENT_BASED:
            self._gradient_based_evolution()
        elif self.config.evolution_strategy == EvolutionStrategy.META_LEARNING:
            self._meta_learning_evolution()
    
    def _adaptive_compression_evolution(self):
        """è‡ªé€‚åº”å‹ç¼©è¿›åŒ–"""
        logger.info("æ‰§è¡Œè‡ªé€‚åº”å‹ç¼©è¿›åŒ–...")
        
        for task_type, lora_info in self.lora_adapters.items():
            try:
                compressor = lora_info["compressor"]
                
                # æ ¹æ®æ€§èƒ½è°ƒæ•´å‹ç¼©å‚æ•°
                model_info = self.model_pool[task_type]
                performance = model_info["performance"]
                
                if performance < self.config.performance_threshold:
                    # æ€§èƒ½ä¸ä½³ï¼Œå¢åŠ å‹ç¼©
                    new_rank = max(4, self.config.lora_rank - 2)
                    logger.info(f"å¢åŠ å‹ç¼© {task_type.value}: rank {self.config.lora_rank} -> {new_rank}")
                else:
                    # æ€§èƒ½è‰¯å¥½ï¼Œå‡å°‘å‹ç¼©
                    new_rank = min(16, self.config.lora_rank + 1)
                    logger.info(f"å‡å°‘å‹ç¼© {task_type.value}: rank {self.config.lora_rank} -> {new_rank}")
                
                # æ›´æ–°é…ç½®
                self.config.lora_rank = new_rank
                
            except Exception as e:
                logger.error(f"è‡ªé€‚åº”å‹ç¼©è¿›åŒ–å¤±è´¥ {task_type.value}: {e}")
    
    def _multi_model_evolution(self):
        """å¤šæ¨¡å‹ååŒè¿›åŒ–"""
        logger.info("æ‰§è¡Œå¤šæ¨¡å‹ååŒè¿›åŒ–...")
        
        # åˆ†æå„æ¨¡å‹æ€§èƒ½
        model_performances = {}
        for task_type, model_info in self.model_pool.items():
            model_performances[task_type] = model_info["performance"]
        
        # æ‰¾å‡ºæ€§èƒ½æœ€å·®çš„æ¨¡å‹
        worst_task = min(model_performances, key=model_performances.get)
        worst_performance = model_performances[worst_task]
        
        if worst_performance < self.config.performance_threshold:
            logger.info(f"æ¨¡å‹æ€§èƒ½ä¸ä½³ï¼Œå°è¯•ä¼˜åŒ–: {worst_task.value}")
            
            # å°è¯•åœ¨çº¿é€‚é…
            if worst_task in self.lora_adapters:
                try:
                    lora_info = self.lora_adapters[worst_task]
                    compressor = lora_info["compressor"]
                    
                    # æ¨¡æ‹Ÿé€‚é…æ•°æ®
                    adaptation_data = [
                        {
                            "gradients": {
                                "lora_A": self._generate_fake_gradients(),
                                "lora_B": self._generate_fake_gradients()
                            }
                        }
                    ]
                    
                    # æ‰§è¡Œé€‚é…
                    lora_info["adaptation_count"] += 1
                    logger.info(f"æ‰§è¡Œåœ¨çº¿é€‚é… {worst_task.value}: ç¬¬{lora_info['adaptation_count']}æ¬¡")
                    
                except Exception as e:
                    logger.error(f"åœ¨çº¿é€‚é…å¤±è´¥ {worst_task.value}: {e}")
    
    def _gradient_based_evolution(self):
        """åŸºäºæ¢¯åº¦çš„è¿›åŒ–"""
        logger.info("æ‰§è¡ŒåŸºäºæ¢¯åº¦çš„è¿›åŒ–...")
        
        if self.rl_trainer is None:
            logger.warning("RLè®­ç»ƒå™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ¢¯åº¦è¿›åŒ–")
            return
        
        try:
            # æ”¶é›†ç»éªŒæ•°æ®
            if len(self.performance_history) > 10:
                recent_performances = list(self.performance_history)[-10:]
                avg_performance = sum(p["performance"] for p in recent_performances) / len(recent_performances)
                
                # è®¡ç®—å¥–åŠ±
                reward = avg_performance * 10
                
                # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
                state_features = {
                    "avg_performance": avg_performance,
                    "evolution_step": self.evolution_step,
                    "model_count": len(self.model_pool)
                }
                
                self.rl_trainer.add_experience(
                    state=state_features,
                    action="evolution",
                    reward=reward,
                    done=False
                )
                
                # æ›´æ–°ç­–ç•¥
                update_result = self.rl_trainer.update_policy()
                logger.info(f"RLç­–ç•¥æ›´æ–°: {update_result}")
                
        except Exception as e:
            logger.error(f"åŸºäºæ¢¯åº¦çš„è¿›åŒ–å¤±è´¥: {e}")
    
    def _meta_learning_evolution(self):
        """å…ƒå­¦ä¹ è¿›åŒ–"""
        logger.info("æ‰§è¡Œå…ƒå­¦ä¹ è¿›åŒ–...")
        
        # åˆ†æä»»åŠ¡åˆ†å¸ƒå’Œæ€§èƒ½
        task_performances = {}
        for task_type, model_info in self.model_pool.items():
            task_performances[task_type] = {
                "performance": model_info["performance"],
                "usage_count": model_info["usage_count"]
            }
        
        # æ‰¾å‡ºæœ€å¸¸ç”¨çš„ä»»åŠ¡ç±»å‹
        most_used_task = max(task_performances, key=lambda x: task_performances[x]["usage_count"])
        
        # ä¸ºæœ€å¸¸ç”¨çš„ä»»åŠ¡ä¼˜åŒ–æ¨¡å‹
        if most_used_task in self.lora_adapters:
            try:
                lora_info = self.lora_adapters[most_used_task]
                model_info = self.model_pool[most_used_task]
                
                # å¢åŠ è¯¥æ¨¡å‹çš„èµ„æºåˆ†é…
                logger.info(f"å…ƒå­¦ä¹ ä¼˜åŒ–: ä¸º{most_used_task.value}åˆ†é…æ›´å¤šèµ„æº")
                
                # å¯ä»¥åœ¨è¿™é‡Œå®ç°æ›´å¤æ‚çš„å…ƒå­¦ä¹ ç­–ç•¥
                
            except Exception as e:
                logger.error(f"å…ƒå­¦ä¹ è¿›åŒ–å¤±è´¥: {e}")
    
    def _generate_fake_gradients(self):
        """ç”Ÿæˆå‡æ¢¯åº¦ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        try:
            import torch
            return torch.randn(8, 512) * 0.01
        except ImportError:
            return None
    
    def get_evolution_stats(self) -> Dict[str, Any]:
        """è·å–è¿›åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "evolution_step": self.evolution_step,
            "evolution_strategy": self.config.evolution_strategy.value,
            "model_pool_size": len(self.model_pool),
            "performance_history_size": len(self.performance_history),
            "lora_enabled": self.config.enable_lora,
            "kv_cache_compression_enabled": self.config.enable_kv_cache_compression
        }
        
        # æ¨¡å‹æ€§èƒ½ç»Ÿè®¡
        model_stats = {}
        for task_type, model_info in self.model_pool.items():
            model_stats[task_type.value] = {
                "model_name": model_info["model_name"],
                "performance": model_info["performance"],
                "usage_count": model_info["usage_count"],
                "last_used": model_info["last_used"].isoformat() if model_info["last_used"] else None
            }
        stats["model_performances"] = model_stats
        
        # LoRAé€‚é…å™¨ç»Ÿè®¡
        if self.config.enable_lora:
            lora_stats = {}
            for task_type, lora_info in self.lora_adapters.items():
                lora_stats[task_type.value] = {
                    "adapter_id": lora_info["adapter_id"],
                    "adaptation_count": lora_info["adaptation_count"]
                }
            stats["lora_adapters"] = lora_stats
        
        # æ€§èƒ½å†å²ç»Ÿè®¡
        if self.performance_history:
            recent_performances = list(self.performance_history)[-10:]
            stats["recent_performance_avg"] = sum(p["performance"] for p in recent_performances) / len(recent_performances)
        
        return stats
    
    def save_evolution_state(self, path: str) -> bool:
        """ä¿å­˜è¿›åŒ–çŠ¶æ€"""
        try:
            os.makedirs(path, exist_ok=True)
            
            # ä¿å­˜é…ç½®
            config_path = os.path.join(path, "evolution_config.json")
            with open(config_path, 'w') as f:
                json.dump({
                    "evolution_strategy": self.config.evolution_strategy.value,
                    "lora_rank": self.config.lora_rank,
                    "lora_alpha": self.config.lora_alpha,
                    "evolution_step": self.evolution_step
                }, f, indent=2)
            
            # ä¿å­˜æ€§èƒ½å†å²
            history_path = os.path.join(path, "performance_history.json")
            with open(history_path, 'w') as f:
                json.dump(list(self.performance_history), f, indent=2, default=str)
            
            # ä¿å­˜æ¨¡å‹æ± çŠ¶æ€
            pool_path = os.path.join(path, "model_pool.json")
            pool_data = {}
            for task_type, model_info in self.model_pool.items():
                pool_data[task_type.value] = {
                    "model_name": model_info["model_name"],
                    "performance": model_info["performance"],
                    "usage_count": model_info["usage_count"],
                    "last_used": model_info["last_used"].isoformat() if model_info["last_used"] else None
                }
            
            with open(pool_path, 'w') as f:
                json.dump(pool_data, f, indent=2)
            
            logger.info(f"è¿›åŒ–çŠ¶æ€å·²ä¿å­˜åˆ°: {path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åŒ–çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def load_evolution_state(self, path: str) -> bool:
        """åŠ è½½è¿›åŒ–çŠ¶æ€"""
        try:
            # åŠ è½½é…ç½®
            config_path = os.path.join(path, "evolution_config.json")
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    config_data = json.load(f)
                
                self.config.evolution_strategy = EvolutionStrategy(config_data["evolution_strategy"])
                self.config.lora_rank = config_data["lora_rank"]
                self.config.lora_alpha = config_data["lora_alpha"]
                self.evolution_step = config_data["evolution_step"]
            
            # åŠ è½½æ€§èƒ½å†å²
            history_path = os.path.join(path, "performance_history.json")
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    history_data = json.load(f)
                
                self.performance_history.clear()
                for item in history_data:
                    item["timestamp"] = datetime.fromisoformat(item["timestamp"])
                    self.performance_history.append(item)
            
            logger.info(f"è¿›åŒ–çŠ¶æ€å·²ä» {path} åŠ è½½")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½è¿›åŒ–çŠ¶æ€å¤±è´¥: {e}")
            return False 


class SelfEvolvingOasisSandbox:
    """è‡ªè¿›åŒ–Oasisæ²™ç›’"""
    
    def __init__(self, 
                 evolution_config: SelfEvolvingConfig,
                 initial_users: int = 50,
                 max_users: int = 1000,
                 initial_posts: int = 20):
        
        self.evolution_config = evolution_config
        self.initial_users = initial_users
        self.max_users = max_users
        self.initial_posts = initial_posts
        
        # åˆå§‹åŒ–è‡ªè¿›åŒ–LLM
        self.evolving_llm = SelfEvolvingLLM(evolution_config)
        
        # ç¤¾äº¤ç½‘ç»œçŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.users = {}
        self.posts = {}
        self.interactions = []
        self.simulation_step = 0
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self._initialize_social_network()
        self._initialize_content()
        
        logger.info("è‡ªè¿›åŒ–Oasisæ²™ç›’åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_social_network(self):
        """åˆå§‹åŒ–ç¤¾äº¤ç½‘ç»œ"""
        logger.info("åˆå§‹åŒ–ç¤¾äº¤ç½‘ç»œ...")
        
        # åˆ›å»ºç”¨æˆ·
        for i in range(self.initial_users):
            user_id = f"user_{i}"
            self.users[user_id] = {
                "id": user_id,
                "interests": ["tech", "social", "news"],
                "activity_level": 0.7,
                "followers": [],
                "following": []
            }
        
        # åˆ›å»ºè¿æ¥
        for user_id in self.users:
            following_count = min(5, len(self.users) - 1)
            potential_follows = [uid for uid in self.users if uid != user_id]
            follows = random.sample(potential_follows, following_count)
            
            for follow_id in follows:
                self.users[user_id]["following"].append(follow_id)
                self.users[follow_id]["followers"].append(user_id)
    
    def _initialize_content(self):
        """åˆå§‹åŒ–å†…å®¹"""
        logger.info("åˆå§‹åŒ–å†…å®¹...")
        
        post_templates = [
            "Interesting discussion about AI and social networks! ğŸ¤–",
            "Great article on technology trends! ğŸ“±",
            "Amazing insights about social media dynamics! ğŸ“Š",
            "Fascinating research on online behavior! ğŸ”¬",
            "Thought-provoking content about digital culture! ğŸ’­"
        ]
        
        for i in range(self.initial_posts):
            post_id = f"post_{i}"
            author_id = random.choice(list(self.users.keys()))
            content = random.choice(post_templates)
            
            self.posts[post_id] = {
                "id": post_id,
                "author_id": author_id,
                "content": content,
                "likes": 0,
                "shares": 0,
                "comments": [],
                "created_at": datetime.now()
            }
    
    def simulate_step(self) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿä¸€ä¸ªæ­¥éª¤"""
        self.simulation_step += 1
        logger.info(f"æ‰§è¡Œæ¨¡æ‹Ÿæ­¥éª¤ {self.simulation_step}")
        
        # 1. å†…å®¹ç”Ÿæˆä»»åŠ¡
        content_result = self.evolving_llm.process_task(
            TaskType.CONTENT_GENERATION,
            "Generate an engaging social media post about technology trends",
            {"step": self.simulation_step, "user_count": len(self.users)}
        )
        
        # 2. è¡Œä¸ºåˆ†æä»»åŠ¡
        behavior_result = self.evolving_llm.process_task(
            TaskType.BEHAVIOR_ANALYSIS,
            "Analyze user engagement patterns in the social network",
            {"posts": len(self.posts), "interactions": len(self.interactions)}
        )
        
        # 3. ç½‘ç»œä¼˜åŒ–ä»»åŠ¡
        network_result = self.evolving_llm.process_task(
            TaskType.NETWORK_OPTIMIZATION,
            "Suggest ways to improve network connectivity and engagement",
            {"network_density": self._calculate_network_density()}
        )
        
        # 4. è¶‹åŠ¿é¢„æµ‹ä»»åŠ¡
        trend_result = self.evolving_llm.process_task(
            TaskType.TREND_PREDICTION,
            "Predict upcoming trends in social media content",
            {"current_trends": self._get_current_trends()}
        )
        
        # 5. ç”¨æˆ·å‚ä¸åº¦ä»»åŠ¡
        engagement_result = self.evolving_llm.process_task(
            TaskType.USER_ENGAGEMENT,
            "Recommend strategies to increase user engagement",
            {"active_users": self._get_active_users()}
        )
        
        # æ›´æ–°ç½‘ç»œçŠ¶æ€
        self._update_network_state()
        
        # è·å–è¿›åŒ–ç»Ÿè®¡
        evolution_stats = self.evolving_llm.get_evolution_stats()
        
        return {
            "step": self.simulation_step,
            "tasks": {
                "content_generation": content_result,
                "behavior_analysis": behavior_result,
                "network_optimization": network_result,
                "trend_prediction": trend_result,
                "user_engagement": engagement_result
            },
            "network_state": {
                "total_users": len(self.users),
                "total_posts": len(self.posts),
                "total_interactions": len(self.interactions),
                "network_density": self._calculate_network_density()
            },
            "evolution_stats": evolution_stats
        }
    
    def _calculate_network_density(self) -> float:
        """è®¡ç®—ç½‘ç»œå¯†åº¦"""
        total_connections = sum(len(user["following"]) for user in self.users.values())
        max_possible_connections = len(self.users) * (len(self.users) - 1)
        return total_connections / max_possible_connections if max_possible_connections > 0 else 0.0
    
    def _get_current_trends(self) -> List[str]:
        """è·å–å½“å‰è¶‹åŠ¿"""
        return ["AI", "social media", "technology", "digital culture"]
    
    def _get_active_users(self) -> int:
        """è·å–æ´»è·ƒç”¨æˆ·æ•°"""
        return len([user for user in self.users.values() if user["activity_level"] > 0.5])
    
    def _update_network_state(self):
        """æ›´æ–°ç½‘ç»œçŠ¶æ€"""
        # æ¨¡æ‹Ÿä¸€äº›ç½‘ç»œå˜åŒ–
        for post_id, post in self.posts.items():
            # éšæœºå¢åŠ ä¸€äº›äº’åŠ¨
            if random.random() < 0.3:
                post["likes"] += random.randint(1, 5)
            
            if random.random() < 0.1:
                post["shares"] += 1
        
        # è®°å½•äº’åŠ¨
        self.interactions.append({
            "type": "simulation_step",
            "step": self.simulation_step,
            "timestamp": datetime.now()
        })
    
    def get_network_stats(self) -> Dict[str, Any]:
        """è·å–ç½‘ç»œç»Ÿè®¡"""
        return {
            "total_users": len(self.users),
            "total_posts": len(self.posts),
            "total_interactions": len(self.interactions),
            "network_density": self._calculate_network_density(),
            "active_users": self._get_active_users(),
            "simulation_step": self.simulation_step
        }
    
    def save_state(self, path: str) -> bool:
        """ä¿å­˜çŠ¶æ€"""
        try:
            os.makedirs(path, exist_ok=True)
            
            # ä¿å­˜ç½‘ç»œçŠ¶æ€
            network_path = os.path.join(path, "network_state.json")
            with open(network_path, 'w') as f:
                json.dump({
                    "users": self.users,
                    "posts": self.posts,
                    "interactions": self.interactions,
                    "simulation_step": self.simulation_step
                }, f, indent=2, default=str)
            
            # ä¿å­˜è¿›åŒ–çŠ¶æ€
            evolution_path = os.path.join(path, "evolution")
            self.evolving_llm.save_evolution_state(evolution_path)
            
            logger.info(f"çŠ¶æ€å·²ä¿å­˜åˆ°: {path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def load_state(self, path: str) -> bool:
        """åŠ è½½çŠ¶æ€"""
        try:
            # åŠ è½½ç½‘ç»œçŠ¶æ€
            network_path = os.path.join(path, "network_state.json")
            if os.path.exists(network_path):
                with open(network_path, 'r') as f:
                    network_data = json.load(f)
                
                self.users = network_data["users"]
                self.posts = network_data["posts"]
                self.interactions = network_data["interactions"]
                self.simulation_step = network_data["simulation_step"]
            
            # åŠ è½½è¿›åŒ–çŠ¶æ€
            evolution_path = os.path.join(path, "evolution")
            self.evolving_llm.load_evolution_state(evolution_path)
            
            logger.info(f"çŠ¶æ€å·²ä» {path} åŠ è½½")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
            return False


# å·¥å‚å‡½æ•°
def create_self_evolving_oasis(
    evolution_strategy: Union[str, EvolutionStrategy] = "multi_model",
    enable_lora: bool = True,
    enable_kv_cache_compression: bool = True,
    **kwargs
) -> SelfEvolvingOasisSandbox:
    """åˆ›å»ºè‡ªè¿›åŒ–Oasisæ²™ç›’"""
    
    if isinstance(evolution_strategy, str):
        evolution_strategy = EvolutionStrategy(evolution_strategy)
    
    config = SelfEvolvingConfig(
        evolution_strategy=evolution_strategy,
        enable_lora=enable_lora,
        enable_kv_cache_compression=enable_kv_cache_compression,
        **kwargs
    )
    
    return SelfEvolvingOasisSandbox(config)


def run_self_evolving_oasis_demo(steps: int = 10, save_path: str = "./data/self_evolving_oasis"):
    """è¿è¡Œè‡ªè¿›åŒ–Oasisæ¼”ç¤º"""
    
    print("ğŸš€ è‡ªè¿›åŒ–Oasisç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    print("ç‰¹æ€§:")
    print("- LoRAæ¨¡å‹å‚æ•°å‹ç¼©")
    print("- KVç¼“å­˜å‹ç¼©")
    print("- åœ¨çº¿æ¨¡å‹é€‚é…")
    print("- å¤šæ¨¡å‹ååŒ")
    print("- è‡ªè¿›åŒ–å­¦ä¹ ")
    print("=" * 60)
    
    # åˆ›å»ºè‡ªè¿›åŒ–Oasisæ²™ç›’
    sandbox = create_self_evolving_oasis(
        evolution_strategy="multi_model",
        enable_lora=True,
        enable_kv_cache_compression=True,
        model_pool_size=3,
        evolution_interval=3
    )
    
    # æ‰§è¡Œæ¨¡æ‹Ÿæ­¥éª¤
    results = []
    for step in range(steps):
        print(f"\n--- æ­¥éª¤ {step + 1} ---")
        
        try:
            result = sandbox.simulate_step()
            results.append(result)
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"ç½‘ç»œçŠ¶æ€: {result['network_state']}")
            print(f"è¿›åŒ–æ­¥éª¤: {result['evolution_stats']['evolution_step']}")
            
            # æ˜¾ç¤ºä»»åŠ¡ç»“æœ
            for task_name, task_result in result['tasks'].items():
                if 'error' not in task_result:
                    print(f"{task_name}: æ€§èƒ½ {task_result['performance_score']:.3f}")
                else:
                    print(f"{task_name}: é”™è¯¯ {task_result['error']}")
            
        except Exception as e:
            print(f"æ­¥éª¤ {step + 1} æ‰§è¡Œå¤±è´¥: {e}")
    
    # ä¿å­˜çŠ¶æ€
    if save_path:
        sandbox.save_state(save_path)
        print(f"\nçŠ¶æ€å·²ä¿å­˜åˆ°: {save_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    final_stats = sandbox.get_network_stats()
    evolution_stats = sandbox.evolving_llm.get_evolution_stats()
    
    print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
    print(f"ç½‘ç»œç”¨æˆ·æ•°: {final_stats['total_users']}")
    print(f"ç½‘ç»œå¸–å­æ•°: {final_stats['total_posts']}")
    print(f"ç½‘ç»œå¯†åº¦: {final_stats['network_density']:.3f}")
    print(f"è¿›åŒ–æ­¥éª¤: {evolution_stats['evolution_step']}")
    print(f"æ¨¡å‹æ± å¤§å°: {evolution_stats['model_pool_size']}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è‡ªè¿›åŒ–Oasisç³»ç»Ÿæ¼”ç¤º")
    parser.add_argument("--steps", type=int, default=10, help="æ¨¡æ‹Ÿæ­¥æ•°")
    parser.add_argument("--save-path", type=str, default="./data/self_evolving_oasis", help="ä¿å­˜è·¯å¾„")
    parser.add_argument("--strategy", type=str, default="multi_model", 
                       choices=["gradient_based", "meta_learning", "adaptive_compression", "multi_model"],
                       help="è¿›åŒ–ç­–ç•¥")
    
    args = parser.parse_args()
    
    try:
        results = run_self_evolving_oasis_demo(args.steps, args.save_path)
        print("\nâœ… æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 