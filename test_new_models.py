#!/usr/bin/env python3
"""
æµ‹è¯•æ–°æ·»åŠ çš„ç«çƒ­é¢„è®­ç»ƒæ¨¡å‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨SandGraphä¸­æ–°å¢çš„å„ç§å¼€æºLLMæ¨¡å‹
"""

import sys
import os
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sandgraph.core.llm_interface import (
    create_shared_llm_manager,
    create_mistral_manager,
    create_gemma_manager,
    create_phi_manager,
    create_yi_manager,
    create_chatglm_manager,
    create_baichuan_manager,
    create_internlm_manager,
    create_falcon_manager,
    create_llama2_manager,
    create_codellama_manager,
    create_starcoder_manager,
    get_available_models,
    create_model_by_type
)


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")


def test_model_generation(llm_manager, model_name: str, prompt: str):
    """æµ‹è¯•æ¨¡å‹ç”Ÿæˆ"""
    print(f"\n--- æµ‹è¯• {model_name} ---")
    print(f"Prompt: {prompt}")
    
    try:
        start_time = time.time()
        response = llm_manager.generate_for_node("test_node", prompt)
        end_time = time.time()
        
        print(f"Response: {response.text}")
        print(f"Generation Time: {end_time - start_time:.2f}s")
        print(f"Confidence: {response.confidence:.3f}")
        
        return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


def test_available_models():
    """æµ‹è¯•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    print_section("Available Models")
    
    models = get_available_models()
    for model_type, model_list in models.items():
        print(f"\n{model_type.upper()}:")
        for model in model_list:
            print(f"  - {model}")


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print_section("Model Creation Test")
    
    # æµ‹è¯•æç¤º
    test_prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚"
    
    # æµ‹è¯•ä¸åŒçš„æ¨¡å‹ç±»å‹
    model_tests = [
        ("Mistral", "mistralai/Mistral-7B-Instruct-v0.2"),
        ("Gemma", "google/gemma-2b-it"),
        ("Phi", "microsoft/Phi-2"),
        ("Yi", "01-ai/Yi-6B-Chat"),
        ("ChatGLM", "THUDM/chatglm3-6b"),
        ("Baichuan", "baichuan-inc/Baichuan2-7B-Chat"),
        ("InternLM", "internlm/internlm-chat-7b"),
        ("Falcon", "tiiuae/falcon-7b-instruct"),
        ("LLaMA2", "meta-llama/Llama-2-7b-chat-hf"),
        ("CodeLLaMA", "codellama/CodeLlama-7b-Instruct-hf"),
        ("StarCoder", "bigcode/starcoder2-7b")
    ]
    
    success_count = 0
    total_count = len(model_tests)
    
    for model_name, model_path in model_tests:
        try:
            print(f"\næ­£åœ¨æµ‹è¯• {model_name}...")
            
            # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
            if model_name == "Mistral":
                llm_manager = create_mistral_manager(model_path)
            elif model_name == "Gemma":
                llm_manager = create_gemma_manager(model_path)
            elif model_name == "Phi":
                llm_manager = create_phi_manager(model_path)
            elif model_name == "Yi":
                llm_manager = create_yi_manager(model_path)
            elif model_name == "ChatGLM":
                llm_manager = create_chatglm_manager(model_path)
            elif model_name == "Baichuan":
                llm_manager = create_baichuan_manager(model_path)
            elif model_name == "InternLM":
                llm_manager = create_internlm_manager(model_path)
            elif model_name == "Falcon":
                llm_manager = create_falcon_manager(model_path)
            elif model_name == "LLaMA2":
                llm_manager = create_llama2_manager(model_path)
            elif model_name == "CodeLLaMA":
                llm_manager = create_codellama_manager(model_path)
            elif model_name == "StarCoder":
                llm_manager = create_starcoder_manager(model_path)
            
            # æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹
            llm_manager.register_node("test_node", {
                "role": "æµ‹è¯•èŠ‚ç‚¹",
                "temperature": 0.7,
                "max_length": 256
            })
            
            print(f"âœ… {model_name} åˆ›å»ºæˆåŠŸ")
            success_count += 1
            
        except Exception as e:
            print(f"âŒ {model_name} åˆ›å»ºå¤±è´¥: {e}")
    
    print(f"\næ¨¡å‹åˆ›å»ºæµ‹è¯•ç»“æœ: {success_count}/{total_count} æˆåŠŸ")


def test_model_by_type():
    """æµ‹è¯•é€šè¿‡ç±»å‹åˆ›å»ºæ¨¡å‹"""
    print_section("Model Creation by Type")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ¨¡å‹åˆ›å»º
    model_types = ["mistral", "gemma", "phi", "yi", "chatglm", "baichuan"]
    
    for model_type in model_types:
        try:
            print(f"\næ­£åœ¨åˆ›å»º {model_type} ç±»å‹æ¨¡å‹...")
            llm_manager = create_model_by_type(model_type)
            
            # æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹
            llm_manager.register_node("test_node", {
                "role": "æµ‹è¯•èŠ‚ç‚¹",
                "temperature": 0.7,
                "max_length": 256
            })
            
            print(f"âœ… {model_type} ç±»å‹æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ {model_type} ç±»å‹æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")


def test_model_comparison():
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
    print_section("Model Performance Comparison")
    
    # é€‰æ‹©å‡ ä¸ªè½»é‡çº§æ¨¡å‹è¿›è¡Œæµ‹è¯•
    test_models = [
        ("Phi-2", "microsoft/Phi-2"),
        ("Gemma-2B", "google/gemma-2b-it"),
        ("Qwen-1.8B", "Qwen/Qwen-1_8B-Chat")
    ]
    
    test_prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚"
    
    results = []
    
    for model_name, model_path in test_models:
        try:
            print(f"\næµ‹è¯• {model_name}...")
            
            # åˆ›å»ºæ¨¡å‹
            llm_manager = create_shared_llm_manager(
                model_name=model_path,
                backend="huggingface",
                device="auto"
            )
            
            # æ³¨å†ŒèŠ‚ç‚¹
            llm_manager.register_node("test_node", {
                "role": "æµ‹è¯•èŠ‚ç‚¹",
                "temperature": 0.7,
                "max_length": 128
            })
            
            # æµ‹è¯•ç”Ÿæˆ
            start_time = time.time()
            response = llm_manager.generate_for_node("test_node", test_prompt)
            end_time = time.time()
            
            generation_time = end_time - start_time
            
            results.append({
                "model": model_name,
                "response": response.text,
                "time": generation_time,
                "confidence": response.confidence
            })
            
            print(f"âœ… {model_name}: {generation_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print(f"\n{'='*50}")
    print("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒç»“æœ:")
    print(f"{'='*50}")
    
    for result in results:
        print(f"\n{result['model']}:")
        print(f"  å“åº”: {result['response']}")
        print(f"  ç”Ÿæˆæ—¶é—´: {result['time']:.2f}s")
        print(f"  ç½®ä¿¡åº¦: {result['confidence']:.3f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ SandGraph ç«çƒ­é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # 1. æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    test_available_models()
    
    # 2. æµ‹è¯•æ¨¡å‹åˆ›å»º
    test_model_creation()
    
    # 3. æµ‹è¯•é€šè¿‡ç±»å‹åˆ›å»ºæ¨¡å‹
    test_model_by_type()
    
    # 4. æµ‹è¯•æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
    test_model_comparison()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("\nä½¿ç”¨è¯´æ˜:")
    print("1. ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–: transformers, torch, accelerate")
    print("2. æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šæƒé™æˆ–é¢å¤–çš„ä¾èµ–")
    print("3. é¦–æ¬¡è¿è¡Œæ—¶ä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
    print("4. å»ºè®®åœ¨GPUç¯å¢ƒä¸‹è¿è¡Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")


if __name__ == "__main__":
    main() 