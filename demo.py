#!/usr/bin/env python3
"""
SandGraph æ¼”ç¤ºè„šæœ¬

å±•ç¤º SandGraph æ¡†æ¶çš„åŸºæœ¬åŠŸèƒ½å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ï¼š
1. å•ä¸€LLMçš„å‚æ•°å…±äº«æœºåˆ¶
2. å¤æ‚å·¥ä½œæµå›¾çš„æ„å»ºå’Œå¯è§†åŒ–
3. åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMä¼˜åŒ–è¿‡ç¨‹
"""

import sys
import json
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, '.')

from sandgraph.core.workflow import WorkflowGraph, WorkflowNode, NodeType
from sandgraph.core.rl_framework import create_rl_framework
from sandgraph.sandbox_implementations import Game24Sandbox, SummarizeSandbox
from sandgraph.examples import UserCaseExamples


def print_separator(title: str, width: int = 60):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "=" * width)
    print(f" {title} ".center(width))
    print("=" * width + "\n")


def create_complex_rl_workflow():
    """åˆ›å»ºå¤æ‚çš„RLå¢å¼ºå·¥ä½œæµå›¾"""
    print_separator("åˆ›å»ºå¤æ‚RLå·¥ä½œæµ")
    
    # åˆ›å»ºRLæ¡†æ¶ - å…¨å±€åªæœ‰ä¸€ä¸ªLLMæ¨¡å‹
    rl_framework = create_rl_framework("global_shared_llm")
    
    print("ğŸ§  åˆ›å»ºå…¨å±€å…±äº«LLMç®¡ç†å™¨")
    print(f"   æ¨¡å‹åç§°: {rl_framework.llm_manager.llm.model_name}")
    print(f"   æ‰€æœ‰LLMèŠ‚ç‚¹éƒ½å…±äº«è¿™ä¸€ä¸ªæ¨¡å‹çš„å‚æ•°")
    
    # åˆ›å»ºå¤æ‚å·¥ä½œæµå›¾
    graph = WorkflowGraph("complex_rl_workflow")
    
    # === è¾“å…¥å±‚ ===
    input_node = WorkflowNode("input", NodeType.INPUT)
    graph.add_node(input_node)
    
    # === ç¬¬ä¸€å±‚ï¼šä»»åŠ¡åˆ†æå’Œè§„åˆ’ ===
    # ä»»åŠ¡åˆ†æå™¨ï¼ˆLLMèŠ‚ç‚¹1 - å…¨å±€LLMçš„copyï¼‰
    task_analyzer_llm = rl_framework.create_rl_enabled_llm_node(
        "task_analyzer", 
        {"role": "ä»»åŠ¡åˆ†æ", "temperature": 0.7}
    )
    task_analyzer_node = WorkflowNode("task_analyzer", NodeType.LLM, llm_func=task_analyzer_llm)
    graph.add_node(task_analyzer_node)
    
    # ç­–ç•¥è§„åˆ’å™¨ï¼ˆLLMèŠ‚ç‚¹2 - å…¨å±€LLMçš„copyï¼‰
    strategy_planner_llm = rl_framework.create_rl_enabled_llm_node(
        "strategy_planner", 
        {"role": "ç­–ç•¥è§„åˆ’", "temperature": 0.5}
    )
    strategy_planner_node = WorkflowNode("strategy_planner", NodeType.LLM, llm_func=strategy_planner_llm)
    graph.add_node(strategy_planner_node)
    
    # === ç¬¬äºŒå±‚ï¼šå¹¶è¡Œæ‰§è¡Œ ===
    # Game24æ²™ç›’
    game24_sandbox = WorkflowNode("game24_sandbox", NodeType.SANDBOX, sandbox=Game24Sandbox())
    graph.add_node(game24_sandbox)
    
    # æ€»ç»“æ²™ç›’
    summary_sandbox = WorkflowNode("summary_sandbox", NodeType.SANDBOX, sandbox=SummarizeSandbox())
    graph.add_node(summary_sandbox)
    
    # === ç¬¬ä¸‰å±‚ï¼šä¸“é—¨æ‰§è¡Œå™¨ ===
    # æ•°å­¦æ±‚è§£å™¨ï¼ˆLLMèŠ‚ç‚¹3 - å…¨å±€LLMçš„copyï¼‰
    math_solver_llm = rl_framework.create_rl_enabled_llm_node(
        "math_solver", 
        {"role": "æ•°å­¦æ±‚è§£", "specialized": "mathematics"}
    )
    math_solver_node = WorkflowNode("math_solver", NodeType.LLM, llm_func=math_solver_llm)
    graph.add_node(math_solver_node)
    
    # æ–‡æœ¬å¤„ç†å™¨ï¼ˆLLMèŠ‚ç‚¹4 - å…¨å±€LLMçš„copyï¼‰
    text_processor_llm = rl_framework.create_rl_enabled_llm_node(
        "text_processor", 
        {"role": "æ–‡æœ¬å¤„ç†", "specialized": "text_analysis"}
    )
    text_processor_node = WorkflowNode("text_processor", NodeType.LLM, llm_func=text_processor_llm)
    graph.add_node(text_processor_node)
    
    # === ç¬¬å››å±‚ï¼šè´¨é‡æ§åˆ¶ ===
    # ç»“æœéªŒè¯å™¨ï¼ˆLLMèŠ‚ç‚¹5 - å…¨å±€LLMçš„copyï¼‰
    result_verifier_llm = rl_framework.create_rl_enabled_llm_node(
        "result_verifier", 
        {"role": "ç»“æœéªŒè¯", "temperature": 0.3}
    )
    result_verifier_node = WorkflowNode("result_verifier", NodeType.LLM, llm_func=result_verifier_llm)
    graph.add_node(result_verifier_node)
    
    # è´¨é‡è¯„ä¼°å™¨ï¼ˆLLMèŠ‚ç‚¹6 - å…¨å±€LLMçš„copyï¼‰
    quality_assessor_llm = rl_framework.create_rl_enabled_llm_node(
        "quality_assessor", 
        {"role": "è´¨é‡è¯„ä¼°", "temperature": 0.2}
    )
    quality_assessor_node = WorkflowNode("quality_assessor", NodeType.LLM, llm_func=quality_assessor_llm)
    graph.add_node(quality_assessor_node)
    
    # === ç¬¬äº”å±‚ï¼šèšåˆå’Œä¼˜åŒ– ===
    # ç»“æœèšåˆå™¨
    def result_aggregator(inputs: Dict[str, Any]) -> Dict[str, Any]:
        """èšåˆå¤šä¸ªè¾“å…¥çš„ç»“æœ"""
        results = []
        scores = []
        
        for key, value in inputs.items():
            if isinstance(value, dict):
                if "score" in value:
                    scores.append(value["score"])
                results.append(value)
        
        avg_score = sum(scores) / len(scores) if scores else 0.0
        
        return {
            "aggregated_results": results,
            "average_score": avg_score,
            "total_inputs": len(inputs),
            "node_id": "result_aggregator"
        }
    
    aggregator_node = WorkflowNode("result_aggregator", NodeType.AGGREGATOR, aggregator_func=result_aggregator)
    graph.add_node(aggregator_node)
    
    # === ç¬¬å…­å±‚ï¼šæœ€ç»ˆä¼˜åŒ– ===
    # æœ€ç»ˆä¼˜åŒ–å™¨ï¼ˆLLMèŠ‚ç‚¹7 - å…¨å±€LLMçš„copyï¼‰
    final_optimizer_llm = rl_framework.create_rl_enabled_llm_node(
        "final_optimizer", 
        {"role": "æœ€ç»ˆä¼˜åŒ–", "temperature": 0.4}
    )
    final_optimizer_node = WorkflowNode("final_optimizer", NodeType.LLM, llm_func=final_optimizer_llm)
    graph.add_node(final_optimizer_node)
    
    # === è¾“å‡ºå±‚ ===
    output_node = WorkflowNode("output", NodeType.OUTPUT)
    graph.add_node(output_node)
    
    # === æ„å»ºå¤æ‚çš„è¾¹è¿æ¥ ===
    # ç¬¬ä¸€å±‚è¿æ¥
    graph.add_edge("input", "task_analyzer")
    graph.add_edge("input", "strategy_planner")
    
    # ç¬¬äºŒå±‚è¿æ¥
    graph.add_edge("task_analyzer", "game24_sandbox")
    graph.add_edge("strategy_planner", "summary_sandbox")
    
    # ç¬¬ä¸‰å±‚è¿æ¥
    graph.add_edge("game24_sandbox", "math_solver")
    graph.add_edge("summary_sandbox", "text_processor")
    
    # ç¬¬å››å±‚è¿æ¥
    graph.add_edge("math_solver", "result_verifier")
    graph.add_edge("text_processor", "quality_assessor")
    
    # ç¬¬äº”å±‚è¿æ¥
    graph.add_edge("result_verifier", "result_aggregator")
    graph.add_edge("quality_assessor", "result_aggregator")
    
    # ç¬¬å…­å±‚è¿æ¥
    graph.add_edge("result_aggregator", "final_optimizer")
    
    # è¾“å‡ºè¿æ¥
    graph.add_edge("final_optimizer", "output")
    
    # äº¤å‰è¿æ¥å¢åŠ å¤æ‚æ€§
    graph.add_edge("task_analyzer", "math_solver")  # ç›´æ¥è·¯å¾„
    graph.add_edge("strategy_planner", "text_processor")  # ç›´æ¥è·¯å¾„
    
    print(f"âœ… åˆ›å»ºå¤æ‚å·¥ä½œæµå›¾:")
    print(f"   èŠ‚ç‚¹æ€»æ•°: {len(graph.nodes)}")
    print(f"   è¾¹æ€»æ•°: {len(graph.edges)}")
    print(f"   LLMèŠ‚ç‚¹æ•°: 7 (éƒ½å…±äº«åŒä¸€ä¸ªå…¨å±€æ¨¡å‹)")
    print(f"   æ²™ç›’èŠ‚ç‚¹æ•°: 2")
    print(f"   èšåˆèŠ‚ç‚¹æ•°: 1")
    
    return rl_framework, graph


def visualize_workflow_graph(graph: WorkflowGraph):
    """å¯è§†åŒ–å·¥ä½œæµå›¾ç»“æ„"""
    print_separator("å·¥ä½œæµå›¾å¯è§†åŒ–")
    
    # æŒ‰å±‚çº§ç»„ç»‡èŠ‚ç‚¹
    layers = {
        0: ["input"],
        1: ["task_analyzer", "strategy_planner"],
        2: ["game24_sandbox", "summary_sandbox"],
        3: ["math_solver", "text_processor"],
        4: ["result_verifier", "quality_assessor"],
        5: ["result_aggregator"],
        6: ["final_optimizer"],
        7: ["output"]
    }
    
    print("ğŸ“Š å·¥ä½œæµå›¾å±‚çº§ç»“æ„:")
    for layer, nodes in layers.items():
        layer_name = {
            0: "è¾“å…¥å±‚", 1: "åˆ†æè§„åˆ’å±‚", 2: "æ²™ç›’æ‰§è¡Œå±‚", 
            3: "ä¸“é—¨å¤„ç†å±‚", 4: "è´¨é‡æ§åˆ¶å±‚", 5: "èšåˆå±‚", 
            6: "ä¼˜åŒ–å±‚", 7: "è¾“å‡ºå±‚"
        }[layer]
        
        print(f"  ç¬¬{layer}å±‚ ({layer_name}):")
        for node in nodes:
            node_obj = graph.nodes[node]
            node_type = node_obj.node_type.value
            
            # æ ‡è®°LLMèŠ‚ç‚¹
            if node_type == "llm":
                print(f"    ğŸ§  {node} (LLM-å…±äº«æ¨¡å‹)")
            elif node_type == "sandbox":
                print(f"    ğŸï¸  {node} (æ²™ç›’)")
            elif node_type == "aggregator":
                print(f"    ğŸ”„ {node} (èšåˆå™¨)")
            else:
                print(f"    ğŸ“„ {node} ({node_type})")
    
    print(f"\nğŸ”— è¾¹è¿æ¥å…³ç³»:")
    for from_node, to_node in graph.edges:
        print(f"    {from_node} â†’ {to_node}")
    
    # æ‹“æ‰‘æ’åº
    execution_order = graph.topological_sort()
    print(f"\nâš¡ æ‰§è¡Œé¡ºåº:")
    print(f"    {' â†’ '.join(execution_order)}")


def run_rl_training_cycles(rl_framework, graph: WorkflowGraph, num_cycles: int = 5):
    """è¿è¡Œå¤šè½®RLè®­ç»ƒå¾ªç¯"""
    print_separator("å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯")
    
    print(f"ğŸ”„ å¼€å§‹ {num_cycles} è½®RLè®­ç»ƒ")
    print(f"   å…¨å±€LLMæ¨¡å‹: {rl_framework.llm_manager.llm.model_name}")
    print(f"   å…±äº«è¯¥æ¨¡å‹çš„èŠ‚ç‚¹æ•°: {len(rl_framework.llm_manager.registered_nodes)}")
    
    training_history = []
    
    for cycle in range(num_cycles):
        print(f"\n--- ç¬¬ {cycle + 1} è½®è®­ç»ƒ ---")
        
        # å¼€å§‹æ–°çš„è®­ç»ƒå›åˆ
        episode_id = rl_framework.start_new_episode()
        
        try:
            # æ‰§è¡Œå·¥ä½œæµ
            start_time = time.time()
            result = graph.execute({
                "action": "full_cycle",
                "cycle": cycle + 1,
                "training_mode": True
            })
            execution_time = time.time() - start_time
            
            # æ¨¡æ‹Ÿæ€§èƒ½è¯„ä¼°å’Œå¥–åŠ±è®¡ç®—
            base_score = 0.6 + cycle * 0.05  # æ¨¡æ‹Ÿæ€§èƒ½é€æ¸æå‡
            noise = (hash(str(cycle)) % 100) / 1000  # æ·»åŠ ä¸€äº›éšæœºæ€§
            cycle_score = min(1.0, base_score + noise)
            
            # ä¸ºæ¯ä¸ªLLMèŠ‚ç‚¹åˆ›å»ºè®­ç»ƒç»éªŒ
            llm_nodes = [
                "task_analyzer", "strategy_planner", "math_solver", 
                "text_processor", "result_verifier", "quality_assessor", "final_optimizer"
            ]
            
            total_reward = 0
            for node_id in llm_nodes:
                # åˆ›å»ºæ¨¡æ‹Ÿçš„è®­ç»ƒç»éªŒ
                evaluation_result = {
                    "score": cycle_score + (hash(node_id) % 50) / 1000,  # æ¯ä¸ªèŠ‚ç‚¹ç•¥æœ‰ä¸åŒ
                    "response": f"Cycle {cycle + 1} response from {node_id}",
                    "improvement": cycle * 0.02,
                    "execution_time": execution_time / len(llm_nodes)
                }
                
                # è®¡ç®—å¥–åŠ±
                rewards = rl_framework.reward_calculator.calculate_reward(
                    evaluation_result,
                    {"cycle": cycle + 1, "node_role": node_id}
                )
                
                # æ·»åŠ ç»éªŒåˆ°RLæ¡†æ¶
                rl_framework.rl_trainer.add_experience(
                    state={"cycle": cycle + 1, "node_id": node_id, "task_type": "complex_workflow"},
                    action=f"Generated response for {node_id}",
                    reward=rewards["total"],
                    done=(cycle == num_cycles - 1),
                    group_id=node_id
                )
                total_reward += rewards["total"]
            
            # è®°å½•è®­ç»ƒå†å²
            cycle_stats = {
                "cycle": cycle + 1,
                "episode_id": episode_id,
                "execution_time": execution_time,
                "average_score": cycle_score,
                "total_reward": total_reward,
                "experience_buffer_size": rl_framework.experience_buffer.size(),
                "status": "success"
            }
            
            training_history.append(cycle_stats)
            
            print(f"   âœ… æ‰§è¡ŒæˆåŠŸ")
            print(f"   ğŸ“Š å¹³å‡æ€§èƒ½åˆ†æ•°: {cycle_score:.3f}")
            print(f"   ğŸ æ€»å¥–åŠ±: {total_reward:.2f}")
            print(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
            print(f"   ğŸ“š ç»éªŒç¼“å†²åŒºå¤§å°: {cycle_stats['experience_buffer_size']}")
            
        except Exception as e:
            print(f"   âŒ æ‰§è¡Œå¤±è´¥: {e}")
            training_history.append({
                "cycle": cycle + 1,
                "status": "failed",
                "error": str(e)
            })
    
    return training_history


def analyze_rl_training_results(rl_framework, training_history):
    """åˆ†æRLè®­ç»ƒç»“æœ"""
    print_separator("RLè®­ç»ƒç»“æœåˆ†æ")
    
    # è·å–RLç»Ÿè®¡ä¿¡æ¯
    rl_stats = rl_framework.get_rl_stats()
    
    print("ğŸ§  å…¨å±€LLMå…±äº«ç»Ÿè®¡:")
    llm_info = rl_stats['llm_manager_info']
    print(f"   æ¨¡å‹åç§°: {llm_info['llm_model']}")
    print(f"   åç«¯ç±»å‹: {llm_info['llm_backend']}")
    print(f"   æ³¨å†ŒèŠ‚ç‚¹æ•°: {llm_info['registered_nodes_count']}")
    print(f"   æ€»ç”Ÿæˆæ¬¡æ•°: {llm_info['total_generations']}")
    print(f"   å‚æ•°æ›´æ–°æ¬¡æ•°: {llm_info['total_updates']}")
    
    print(f"\nğŸ“ˆ å„LLMèŠ‚ç‚¹ç»Ÿè®¡ (å…±äº«åŒä¸€æ¨¡å‹å‚æ•°):")
    for node_id, stats in llm_info['node_usage_stats'].items():
        print(f"   {node_id}: {stats['generation_count']} æ¬¡ç”Ÿæˆ")
    
    print(f"\nğŸ¯ è®­ç»ƒè¿‡ç¨‹ç»Ÿè®¡:")
    training_stats = rl_stats['training_stats']
    print(f"   è®­ç»ƒæ­¥éª¤: {training_stats['training_step']}")
    print(f"   å½“å‰å›åˆ: {rl_stats['current_episode']}")
    print(f"   ç»éªŒç¼“å†²åŒºå¤§å°: {rl_stats['experience_buffer_size']}")
    
    # åˆ†ææ€§èƒ½è¶‹åŠ¿
    successful_cycles = [h for h in training_history if h.get("status") == "success"]
    if successful_cycles:
        scores = [h["average_score"] for h in successful_cycles]
        rewards = [h["total_reward"] for h in successful_cycles]
        
        print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        print(f"   æˆåŠŸè½®æ¬¡: {len(successful_cycles)}/{len(training_history)}")
        print(f"   åˆå§‹æ€§èƒ½: {scores[0]:.3f}")
        print(f"   æœ€ç»ˆæ€§èƒ½: {scores[-1]:.3f}")
        print(f"   æ€§èƒ½æå‡: {(scores[-1] - scores[0]):.3f}")
        print(f"   å¹³å‡å¥–åŠ±: {sum(rewards) / len(rewards):.2f}")
        
        print(f"\nğŸ”„ å¼ºåŒ–å­¦ä¹ æ•ˆæœéªŒè¯:")
        if scores[-1] > scores[0]:
            print(f"   âœ… æ€§èƒ½æå‡: {((scores[-1] - scores[0]) / scores[0] * 100):.1f}%")
        print(f"   âœ… ç»éªŒç§¯ç´¯: {rl_stats['experience_buffer_size']} æ¡ç»éªŒè®°å½•")
        print(f"   âœ… å‚æ•°æ›´æ–°: {llm_info['total_updates']} æ¬¡å…¨å±€æ¨¡å‹æ›´æ–°")
        print(f"   âœ… å…±äº«å­¦ä¹ : 7ä¸ªLLMèŠ‚ç‚¹å…±äº«åŒä¸€æ¨¡å‹çš„å­¦ä¹ æˆæœ")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print_separator("ğŸ§© SandGraph RLå¢å¼ºæ¼”ç¤º", 80)
    print("å±•ç¤ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å•ä¸€LLMä¼˜åŒ– - å¤šèŠ‚ç‚¹å‚æ•°å…±äº«æ¶æ„")
    
    try:
        # 1. åˆ›å»ºå¤æ‚çš„RLå·¥ä½œæµ
        rl_framework, complex_graph = create_complex_rl_workflow()
        
        # 2. å¯è§†åŒ–å·¥ä½œæµå›¾
        visualize_workflow_graph(complex_graph)
        
        # 3. è¿è¡ŒRLè®­ç»ƒå¾ªç¯
        training_history = run_rl_training_cycles(rl_framework, complex_graph, num_cycles=5)
        
        # 4. åˆ†æè®­ç»ƒç»“æœ
        analyze_rl_training_results(rl_framework, training_history)
        
        # 5. åŸæœ‰æ¼”ç¤ºï¼ˆåŸºç¡€åŠŸèƒ½ï¼‰
        print_separator("åŸºç¡€åŠŸèƒ½éªŒè¯")
        
        # æ²™ç›’åŸºç¡€æ¼”ç¤º
        game24 = Game24Sandbox(seed=42)
        case = game24.case_generator()
        prompt = game24.prompt_func(case)
        response = "æ¨¡æ‹ŸLLMå“åº”"
        score = game24.verify_score(response, case)
        
        print(f"âœ… æ²™ç›’åŠŸèƒ½: ç”Ÿæˆä»»åŠ¡å¹¶è¯„åˆ† (åˆ†æ•°: {score})")
        
        # ç®€å•å·¥ä½œæµæ¼”ç¤º
        simple_graph = WorkflowGraph("simple_demo")
        input_node = WorkflowNode("input", NodeType.INPUT)
        sandbox_node = WorkflowNode("game24", NodeType.SANDBOX, sandbox=Game24Sandbox())
        output_node = WorkflowNode("output", NodeType.OUTPUT)
        
        simple_graph.add_node(input_node)
        simple_graph.add_node(sandbox_node)
        simple_graph.add_node(output_node)
        simple_graph.add_edge("input", "game24")
        simple_graph.add_edge("game24", "output")
        
        simple_result = simple_graph.execute({"action": "full_cycle"})
        print(f"âœ… åŸºç¡€å·¥ä½œæµ: {len(simple_result)} ä¸ªè¾“å‡ºèŠ‚ç‚¹")
        
        # MCPåè®®æ¼”ç¤º
        from sandgraph.core.mcp import MCPSandboxServer, check_mcp_availability
        mcp_info = check_mcp_availability()
        print(f"âœ… MCPåè®®: {'å¯ç”¨' if mcp_info['available'] else 'ä¸å¯ç”¨'}")
        
        # æ€»ç»“
        print_separator("æ¼”ç¤ºæ€»ç»“", 80)
        print("âœ… å¤æ‚RLå·¥ä½œæµæ„å»ºå®Œæˆ - 7ä¸ªLLMèŠ‚ç‚¹å…±äº«1ä¸ªæ¨¡å‹")
        print("âœ… å·¥ä½œæµå›¾å¯è§†åŒ–å®Œæˆ - å¤šå±‚çº§å¤æ‚ç»“æ„")
        print("âœ… RLè®­ç»ƒå¾ªç¯å®Œæˆ - å±•ç¤ºå‚æ•°å…±äº«ä¼˜åŒ–è¿‡ç¨‹")
        print("âœ… è®­ç»ƒç»“æœåˆ†æå®Œæˆ - éªŒè¯æ€§èƒ½æå‡æ•ˆæœ")
        print("âœ… åŸºç¡€åŠŸèƒ½éªŒè¯å®Œæˆ - ç¡®ä¿å‘åå…¼å®¹")
        
        print(f"\nğŸ¯ æ ¸å¿ƒåˆ›æ–°éªŒè¯:")
        print(f"   âœ“ å•ä¸€LLMæ¶æ„ï¼šå…¨å±€åªæœ‰1ä¸ªæ¨¡å‹è¢«è®­ç»ƒä¼˜åŒ–")
        print(f"   âœ“ å‚æ•°å…±äº«æœºåˆ¶ï¼š7ä¸ªLLMèŠ‚ç‚¹å…±äº«åŒä¸€æ¨¡å‹å‚æ•°")
        print(f"   âœ“ å¤æ‚æ‰§è¡Œå›¾ï¼š8å±‚å¤šè·¯å¾„å·¥ä½œæµå›¾")
        print(f"   âœ“ RLä¼˜åŒ–å¾ªç¯ï¼šç»éªŒå›æ”¾â†’æ¢¯åº¦èšåˆâ†’å‚æ•°æ›´æ–°")
        print(f"   âœ“ æ€§èƒ½å¯è§†åŒ–ï¼šè®­ç»ƒè¿‡ç¨‹å’Œç»“æœçš„è¯¦ç»†åˆ†æ")
        
        return {
            "rl_framework": rl_framework,
            "complex_graph": complex_graph,
            "training_history": training_history,
            "basic_demos": {
                "sandbox": {"case": case, "score": score},
                "simple_workflow": simple_result,
                "mcp": mcp_info
            }
        }
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


if __name__ == "__main__":
    result = main()
    
    # å¯é€‰ï¼šä¿å­˜æ¼”ç¤ºç»“æœåˆ°æ–‡ä»¶
    # with open("demo_results.json", "w", encoding="utf-8") as f:
    #     json.dump(result, f, ensure_ascii=False, indent=2, default=str) 