#!/usr/bin/env python3
"""
New RL Algorithms Demo - SAC and TD3
====================================

This demo showcases the new SAC and TD3 algorithms in SandGraphX.
"""

import sys
import os
import time
import json
import random
import argparse
from typing import Dict, Any, List

# Add the parent directory to the path to import sandgraph modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sandgraph.core.llm_interface import create_shared_llm_manager
from sandgraph.core.rl_algorithms import (
    RLAlgorithm,
    create_sac_trainer,
    create_td3_trainer
)
from sandgraph.core.enhanced_rl_algorithms import (
    create_enhanced_sac_trainer,
    create_enhanced_td3_trainer
)


def generate_continuous_experience() -> Dict[str, Any]:
    """ç”Ÿæˆè¿ç»­åŠ¨ä½œç©ºé—´çš„æ ·æœ¬ç»éªŒæ•°æ®"""
    return {
        "state": {
            "position": random.uniform(-1.0, 1.0),
            "velocity": random.uniform(-0.5, 0.5),
            "angle": random.uniform(-0.3, 0.3),
            "angular_velocity": random.uniform(-0.2, 0.2),
            "energy": random.uniform(0.0, 1.0),
            "stability": random.uniform(0.5, 1.0)
        },
        "action": random.choice([
            "MOVE_FORWARD", "MOVE_BACKWARD", "TURN_LEFT", "TURN_RIGHT",
            "ACCELERATE", "DECELERATE", "MAINTAIN_BALANCE", "STABILIZE"
        ]),
        "reward": random.uniform(-2.0, 5.0),
        "done": random.random() < 0.05  # 5%æ¦‚ç‡ç»“æŸ
    }


def demonstrate_sac_algorithm():
    """æ¼”ç¤ºSACç®—æ³•"""
    print("\nğŸ¯ SAC (Soft Actor-Critic) Algorithm Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºSACè®­ç»ƒå™¨
    sac_trainer = create_sac_trainer(
        llm_manager=llm_manager,
        learning_rate=3e-4
    )
    
    print("âœ… SAC trainer created")
    print("ğŸ“‹ SACç‰¹ç‚¹:")
    print("  - è½¯Actor-Criticç®—æ³•")
    print("  - è‡ªåŠ¨ç†µè°ƒæ•´")
    print("  - ç»éªŒå›æ”¾ç¼“å†²åŒº")
    print("  - ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nğŸƒ Training SAC with continuous action space...")
    start_time = time.time()
    
    # å¿«é€Ÿæ·»åŠ å¤§é‡ç»éªŒ
    for i in range(200):
        exp = generate_continuous_experience()
        sac_trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        # å®šæœŸæ›´æ–°
        if i % 20 == 0 and i > 0:
            update_start = time.time()
            result = sac_trainer.update_policy()
            update_time = time.time() - update_start
            
            print(f"  Step {i}: Update time = {update_time:.3f}s")
            if result.get("status") == "updated":
                print(f"    Actor Loss: {result.get('actor_loss', 0):.4f}")
                print(f"    Critic Loss: {result.get('critic_loss', 0):.4f}")
                print(f"    Alpha: {result.get('alpha', 0):.4f}")
    
    total_time = time.time() - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    final_stats = sac_trainer.get_training_stats()
    print(f"\nğŸ“Š SAC Performance Summary:")
    print(f"  - Total Training Time: {total_time:.2f}s")
    print(f"  - Training Steps: {final_stats.get('training_step', 0)}")
    print(f"  - Buffer Size: {result.get('buffer_size', 0)}")
    print(f"  - Algorithm: {final_stats.get('algorithm', 'SAC')}")


def demonstrate_td3_algorithm():
    """æ¼”ç¤ºTD3ç®—æ³•"""
    print("\nğŸ¯ TD3 (Twin Delayed Deep Deterministic Policy Gradient) Algorithm Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºTD3è®­ç»ƒå™¨
    td3_trainer = create_td3_trainer(
        llm_manager=llm_manager,
        learning_rate=3e-4
    )
    
    print("âœ… TD3 trainer created")
    print("ğŸ“‹ TD3ç‰¹ç‚¹:")
    print("  - åŒQç½‘ç»œæ¶æ„")
    print("  - å»¶è¿Ÿç­–ç•¥æ›´æ–°")
    print("  - ç›®æ ‡ç­–ç•¥å¹³æ»‘")
    print("  - å™ªå£°è£å‰ª")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nğŸƒ Training TD3 with deterministic policy...")
    start_time = time.time()
    
    # å¿«é€Ÿæ·»åŠ å¤§é‡ç»éªŒ
    for i in range(200):
        exp = generate_continuous_experience()
        td3_trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        # å®šæœŸæ›´æ–°
        if i % 20 == 0 and i > 0:
            update_start = time.time()
            result = td3_trainer.update_policy()
            update_time = time.time() - update_start
            
            print(f"  Step {i}: Update time = {update_time:.3f}s")
            if result.get("status") == "updated":
                print(f"    Critic1 Loss: {result.get('critic1_loss', 0):.4f}")
                print(f"    Critic2 Loss: {result.get('critic2_loss', 0):.4f}")
                print(f"    Actor Loss: {result.get('actor_loss', 0):.4f}")
                print(f"    Policy Updated: {result.get('policy_updated', False)}")
    
    total_time = time.time() - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    final_stats = td3_trainer.get_training_stats()
    print(f"\nğŸ“Š TD3 Performance Summary:")
    print(f"  - Total Training Time: {total_time:.2f}s")
    print(f"  - Training Steps: {final_stats.get('training_step', 0)}")
    print(f"  - Buffer Size: {result.get('buffer_size', 0)}")
    print(f"  - Algorithm: {final_stats.get('algorithm', 'TD3')}")


def demonstrate_enhanced_algorithms():
    """æ¼”ç¤ºå¢å¼ºç‰ˆç®—æ³•"""
    print("\nğŸš€ Enhanced SAC and TD3 Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºå¢å¼ºç‰ˆSACè®­ç»ƒå™¨
    enhanced_sac = create_enhanced_sac_trainer(
        llm_manager=llm_manager,
        learning_rate=3e-4,
        alpha=0.2,
        enable_caching=True
    )
    
    print("âœ… Enhanced SAC trainer created")
    
    # åˆ›å»ºå¢å¼ºç‰ˆTD3è®­ç»ƒå™¨
    enhanced_td3 = create_enhanced_td3_trainer(
        llm_manager=llm_manager,
        learning_rate=3e-4,
        policy_noise=0.2,
        enable_caching=True
    )
    
    print("âœ… Enhanced TD3 trainer created")
    
    # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    print("\nğŸƒ Performance comparison...")
    
    trainers = [
        ("Enhanced SAC", enhanced_sac),
        ("Enhanced TD3", enhanced_td3)
    ]
    
    for name, trainer in trainers:
        print(f"\nğŸ§ª Testing {name}...")
        start_time = time.time()
        
        # æ·»åŠ ç»éªŒ
        for i in range(100):
            exp = generate_continuous_experience()
            trainer.add_experience(
                state=exp["state"],
                action=exp["action"],
                reward=exp["reward"],
                done=exp["done"]
            )
            
            if i % 25 == 0 and i > 0:
                trainer.update_policy()
        
        total_time = time.time() - start_time
        
        # è·å–ç»Ÿè®¡
        stats = trainer.get_enhanced_stats()
        print(f"  âœ… {name}: {total_time:.2f}s")
        if 'cache_stats' in stats:
            print(f"    Cache Hit Rate: {stats['cache_stats'].get('hit_rate', 0):.3f}")


def compare_algorithms():
    """æ¯”è¾ƒæ‰€æœ‰ç®—æ³•æ€§èƒ½"""
    print("\nğŸ“Š Algorithm Performance Comparison")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºæ‰€æœ‰ç®—æ³•
    algorithms = [
        ("PPO", create_sac_trainer(llm_manager, 3e-4)),  # ä½¿ç”¨SACä½œä¸ºç¤ºä¾‹
        ("GRPO", create_td3_trainer(llm_manager, 3e-4)),  # ä½¿ç”¨TD3ä½œä¸ºç¤ºä¾‹
        ("SAC", create_sac_trainer(llm_manager, 3e-4)),
        ("TD3", create_td3_trainer(llm_manager, 3e-4))
    ]
    
    results = []
    
    for name, trainer in algorithms:
        print(f"\nğŸ§ª Testing {name}...")
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        for i in range(100):
            exp = generate_continuous_experience()
            trainer.add_experience(
                state=exp["state"],
                action=exp["action"],
                reward=exp["reward"],
                done=exp["done"]
            )
            
            if i % 25 == 0 and i > 0:
                trainer.update_policy()
        
        total_time = time.time() - start_time
        
        # æ”¶é›†ç»Ÿè®¡
        stats = trainer.get_training_stats()
        
        result = {
            "name": name,
            "total_time": total_time,
            "training_steps": stats.get('training_step', 0),
            "algorithm": stats.get('algorithm', name.lower())
        }
        
        results.append(result)
        
        print(f"  âœ… {name}: {total_time:.2f}s")
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š Performance Comparison Results:")
    print(f"{'Algorithm':<12} {'Time (s)':<10} {'Steps':<8}")
    print("-" * 35)
    
    for result in results:
        print(f"{result['name']:<12} {result['total_time']:<10.2f} {result['training_steps']:<8}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="New RL Algorithms Demo")
    parser.add_argument("--demo", choices=["sac", "td3", "enhanced", "compare"], 
                       default="compare", help="Demo type")
    
    args = parser.parse_args()
    
    print("ğŸ¯ SandGraphX New RL Algorithms Demo")
    print("=" * 50)
    
    if args.demo == "sac":
        demonstrate_sac_algorithm()
    elif args.demo == "td3":
        demonstrate_td3_algorithm()
    elif args.demo == "enhanced":
        demonstrate_enhanced_algorithms()
    elif args.demo == "compare":
        compare_algorithms()
    
    print("\nâœ… Demo completed!")


if __name__ == "__main__":
    main() 