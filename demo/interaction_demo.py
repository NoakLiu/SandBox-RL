#!/usr/bin/env python3
"""
Sandbox-RL äº¤äº’æ—¥å¿—æ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¢å¼ºçš„æ—¥å¿—åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. LLMæ€è€ƒè¿‡ç¨‹è®°å½•
2. æ²™ç›’çŠ¶æ€å’ŒåŠ¨ä½œè®°å½•
3. å¥–åŠ±è®¡ç®—è¯¦æƒ…è®°å½•
4. åŠ¨ä½œåºåˆ—è®°å½•
5. äº¤äº’è¿‡ç¨‹ç»Ÿè®¡åˆ†æ
"""

import sys
import json
import time
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, '.')

from demo import TrainingLogger

def demonstrate_interaction_logging():
    """æ¼”ç¤ºè¯¦ç»†çš„äº¤äº’æ—¥å¿—åŠŸèƒ½"""
    print("ğŸ” Sandbox-RL Enhanced Interaction Logging Demo")
    print("=" * 60)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = TrainingLogger("interaction_demo_logs")
    
    # 1. æ¼”ç¤ºLLMäº¤äº’è®°å½•
    print("\n1. ğŸ“ LLM Interaction Logging Demo")
    print("-" * 40)
    
    llm_nodes = ["task_analyzer", "strategy_planner", "math_solver", "text_processor"]
    
    for i, node_id in enumerate(llm_nodes):
        thinking_process = f"""
        Step {i+1} Analysis:
        - Current task: {node_id.replace('_', ' ').title()}
        - Context: Processing complex workflow
        - Strategy: Apply specialized reasoning for {node_id}
        - Expected outcome: High-quality response with confidence > 0.8
        """
        
        prompt = f"Process task for {node_id} with optimization focus"
        response = f"Optimized response from {node_id} with enhanced reasoning and improved performance metrics"
        
        logger.log_llm_interaction(
            node_id=node_id,
            prompt=prompt,
            response=response,
            thinking_process=thinking_process,
            confidence=0.7 + i * 0.05,
            metadata={
                "step": i + 1,
                "task_type": "workflow_processing",
                "optimization_level": "high"
            }
        )
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    # 2. æ¼”ç¤ºæ²™ç›’äº¤äº’è®°å½•
    print("\n2. ğŸï¸ Sandbox Interaction Logging Demo")
    print("-" * 40)
    
    sandbox_nodes = ["game24_sandbox", "summary_sandbox"]
    
    for i, sandbox_id in enumerate(sandbox_nodes):
        # æ¨¡æ‹Ÿæ²™ç›’äº¤äº’åºåˆ—
        states = ["ready", "processing", "completed"]
        actions = ["initialize", "execute_task", "finalize_result"]
        rewards = [0.1, 0.8, 0.3]  # æ‰§è¡Œé˜¶æ®µå¥–åŠ±æœ€é«˜
        
        for j, (state, action, reward) in enumerate(zip(states, actions, rewards)):
            next_state = states[j + 1] if j < len(states) - 1 else "idle"
            
            logger.log_sandbox_interaction(
                sandbox_id=sandbox_id,
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                done=(j == len(states) - 1),
                case_data={
                    "task_id": f"{sandbox_id}_task_{i+1}",
                    "complexity": 3 + i,
                    "input_size": 100 + i * 50
                },
                result_data={
                    "success": True,
                    "quality_score": 0.8 + i * 0.1,
                    "execution_time": 0.5 + j * 0.2
                },
                metadata={
                    "sandbox_type": sandbox_id.split('_')[0],
                    "step": j + 1,
                    "total_steps": len(states)
                }
            )
            time.sleep(0.05)
    
    # 3. æ¼”ç¤ºå¥–åŠ±è®¡ç®—è®°å½•
    print("\n3. ğŸ Reward Calculation Logging Demo")
    print("-" * 40)
    
    for i, node_id in enumerate(llm_nodes):
        raw_score = 0.6 + i * 0.1
        reward_components = {
            "base_reward": raw_score,
            "performance_bonus": raw_score * 0.2,
            "efficiency_bonus": 0.1,
            "quality_bonus": raw_score * 0.15,
            "consistency_penalty": -0.05 if i % 2 == 0 else 0
        }
        total_reward = sum(reward_components.values())
        
        logger.log_reward_calculation(
            node_id=node_id,
            raw_score=raw_score,
            reward_components=reward_components,
            total_reward=total_reward,
            calculation_details={
                "calculation_method": "weighted_sum",
                "bonus_multiplier": 0.2,
                "penalty_applied": i % 2 == 0,
                "normalization": "none"
            }
        )
    
    # 4. æ¼”ç¤ºåŠ¨ä½œåºåˆ—è®°å½•
    print("\n4. ğŸ”„ Action Sequence Logging Demo")
    print("-" * 40)
    
    # åˆ›å»ºå¤æ‚çš„åŠ¨ä½œåºåˆ—
    workflow_actions = []
    for i in range(3):  # 3ä¸ªå·¥ä½œæµæ­¥éª¤
        step_actions = []
        for j, node_id in enumerate(llm_nodes[:2]):  # æ¯æ­¥2ä¸ªèŠ‚ç‚¹
            action = {
                "action_type": "llm_processing",
                "node_id": node_id,
                "step": i + 1,
                "substep": j + 1,
                "input": f"Step {i+1} input for {node_id}",
                "output": f"Step {i+1} output from {node_id}",
                "duration": 0.3 + j * 0.1,
                "success": True
            }
            step_actions.append(action)
        workflow_actions.extend(step_actions)
    
    logger.log_action_sequence(
        sequence_id="complex_workflow_demo",
        actions=workflow_actions,
        sequence_type="multi_step_workflow",
        metadata={
            "total_steps": 3,
            "nodes_per_step": 2,
            "workflow_type": "parallel_processing"
        }
    )
    
    # 5. æ˜¾ç¤ºäº¤äº’ç»Ÿè®¡
    print("\n5. ğŸ“Š Interaction Statistics")
    print("-" * 40)
    
    summary = logger.get_interaction_summary()
    
    print("LLM Interactions:")
    llm_stats = summary["llm_interactions"]
    print(f"  - Total interactions: {llm_stats['total_count']}")
    print(f"  - Nodes involved: {llm_stats['nodes_involved']}")
    print(f"  - Average confidence: {llm_stats['avg_confidence']:.3f}")
    print(f"  - Total tokens processed: {llm_stats['total_tokens_processed']}")
    
    print("\nSandbox Interactions:")
    sandbox_stats = summary["sandbox_interactions"]
    print(f"  - Total interactions: {sandbox_stats['total_count']}")
    print(f"  - Sandboxes involved: {sandbox_stats['sandboxes_involved']}")
    print(f"  - Total reward: {sandbox_stats['total_reward']:.3f}")
    print(f"  - Average reward: {sandbox_stats['avg_reward']:.3f}")
    
    print("\nReward Calculations:")
    reward_stats = summary["reward_calculations"]
    print(f"  - Total calculations: {reward_stats['total_count']}")
    print(f"  - Total reward distributed: {reward_stats['total_reward']:.3f}")
    print(f"  - Average components per calculation: {reward_stats['avg_components_per_calc']:.1f}")
    
    print("\nAction Sequences:")
    action_stats = summary["action_sequences"]
    print(f"  - Total sequences: {action_stats['total_sequences']}")
    print(f"  - Total actions: {action_stats['total_actions']}")
    
    # 6. ä¿å­˜æ—¥å¿—
    print("\n6. ğŸ’¾ Saving Interaction Logs")
    print("-" * 40)
    
    timestamp = logger.save_logs()
    
    print("âœ… Interaction logs saved successfully!")
    print(f"ğŸ“ Log directory: interaction_demo_logs/")
    print(f"ğŸ“ Files created:")
    print(f"   - text_logs_{timestamp}.json")
    print(f"   - llm_interactions_{timestamp}.json")
    print(f"   - sandbox_interactions_{timestamp}.json")
    print(f"   - reward_details_{timestamp}.json")
    print(f"   - action_sequences_{timestamp}.json")
    print(f"   - interaction_summary_{timestamp}.json")
    
    # 7. å±•ç¤ºéƒ¨åˆ†æ—¥å¿—å†…å®¹
    print("\n7. ğŸ” Sample Log Content Preview")
    print("-" * 40)
    
    if logger.llm_interactions:
        print("Sample LLM Interaction:")
        sample_llm = logger.llm_interactions[0]
        print(f"  Node: {sample_llm['node_id']}")
        print(f"  Confidence: {sample_llm['confidence']:.3f}")
        print(f"  Thinking: {sample_llm['thinking_process'][:100]}...")
        print(f"  Response: {sample_llm['response'][:80]}...")
    
    if logger.sandbox_interactions:
        print("\nSample Sandbox Interaction:")
        sample_sandbox = logger.sandbox_interactions[0]
        print(f"  Sandbox: {sample_sandbox['sandbox_id']}")
        print(f"  State Transition: {sample_sandbox['state']} -> {sample_sandbox['next_state']}")
        print(f"  Action: {sample_sandbox['action']}")
        print(f"  Reward: {sample_sandbox['reward']:.3f}")
    
    if logger.reward_details:
        print("\nSample Reward Calculation:")
        sample_reward = logger.reward_details[0]
        print(f"  Node: {sample_reward['node_id']}")
        print(f"  Raw Score: {sample_reward['raw_score']:.3f}")
        print(f"  Components: {list(sample_reward['reward_components'].keys())}")
        print(f"  Total Reward: {sample_reward['total_reward']:.3f}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Enhanced Interaction Logging Demo Completed!")
    print("âœ… All interaction types successfully logged and analyzed")
    print("âœ… Detailed logs saved for further analysis")
    print("âœ… Statistics and summaries generated")
    
    return logger, timestamp

if __name__ == "__main__":
    logger, timestamp = demonstrate_interaction_logging()
    print(f"\nğŸ“‹ Demo completed at: {timestamp}") 