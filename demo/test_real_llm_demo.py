#!/usr/bin/env python3
"""
Test Real LLM Demo
==================

ç®€å•çš„çœŸå®LLMæµ‹è¯•è„šæœ¬ï¼Œç”¨äºéªŒè¯HuggingFaceæ¨¡å‹çš„åŠŸèƒ½
"""

import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sandbox_rl.core.llm_interface import create_llm_config, create_llm, LLMBackend
from sandbox_rl.core.llm_frozen_adaptive import (
    FrozenAdaptiveLLM, create_frozen_config, UpdateStrategy
)

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def test_real_llm():
    """æµ‹è¯•çœŸå®LLM"""
    print("ğŸš€ æµ‹è¯•çœŸå®LLMåŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºçœŸå®LLMé…ç½®
        print("ğŸ“‹ åˆ›å»ºHuggingFace LLMé…ç½®...")
        config = create_llm_config(
            backend="huggingface",
            model_name="Qwen/Qwen-1_8B-Chat",
            device="auto",
            max_length=256,
            temperature=0.7
        )
        
        # åˆ›å»ºLLM
        print("ğŸ”§ åˆ›å»ºLLMå®ä¾‹...")
        base_llm = create_llm(config)
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        base_llm.load_model()
        
        # æµ‹è¯•åŸºç¡€ç”Ÿæˆ
        print("ğŸ§ª æµ‹è¯•åŸºç¡€ç”Ÿæˆ...")
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
        response = base_llm.generate(test_prompt)
        print(f"æç¤º: {test_prompt}")
        print(f"å“åº”: {response.text[:200]}...")
        print(f"ç½®ä¿¡åº¦: {response.confidence:.3f}")
        
        # åˆ›å»ºå†»ç»“è‡ªé€‚åº”LLM
        print("\nğŸ”’ åˆ›å»ºå†»ç»“è‡ªé€‚åº”LLM...")
        frozen_config = create_frozen_config(
            strategy="adaptive",
            frozen_layers=["embedding"],
            adaptive_learning_rate=True
        )
        
        frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
        
        # æµ‹è¯•å†»ç»“è‡ªé€‚åº”ç”Ÿæˆ
        print("ğŸ§ª æµ‹è¯•å†»ç»“è‡ªé€‚åº”ç”Ÿæˆ...")
        response = frozen_llm.generate("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
        print(f"å“åº”: {response.text[:200]}...")
        print(f"ç½®ä¿¡åº¦: {response.confidence:.3f}")
        
        # è·å–å‚æ•°ä¿¡æ¯
        print("\nğŸ“Š å‚æ•°ä¿¡æ¯:")
        param_info = frozen_llm.get_parameter_info()
        for name, info in list(param_info.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå‚æ•°
            print(f"   {name}: é‡è¦æ€§={info.importance.value}, å†»ç»“={info.frozen}")
        
        print("\nâœ… çœŸå®LLMæµ‹è¯•æˆåŠŸï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        logging.exception("æµ‹è¯•å¼‚å¸¸")
        return False
    
    return True

def test_mock_llm():
    """æµ‹è¯•MockLLMä½œä¸ºå¯¹æ¯”"""
    print("\n" + "=" * 50)
    print("ğŸ­ æµ‹è¯•MockLLMåŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºMockLLMé…ç½®
        config = create_llm_config(backend="mock", model_name="test_mock")
        base_llm = create_llm(config)
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
        response = base_llm.generate(test_prompt)
        print(f"æç¤º: {test_prompt}")
        print(f"å“åº”: {response.text}")
        print(f"ç½®ä¿¡åº¦: {response.confidence:.3f}")
        
        # åˆ›å»ºå†»ç»“è‡ªé€‚åº”LLM
        frozen_config = create_frozen_config(strategy="adaptive")
        frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
        
        # æµ‹è¯•å†»ç»“è‡ªé€‚åº”ç”Ÿæˆ
        response = frozen_llm.generate("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
        print(f"å†»ç»“è‡ªé€‚åº”å“åº”: {response.text}")
        print(f"ç½®ä¿¡åº¦: {response.confidence:.3f}")
        
        print("\nâœ… MockLLMæµ‹è¯•æˆåŠŸï¼")
        
    except Exception as e:
        print(f"\nâŒ MockLLMæµ‹è¯•å¤±è´¥: {e}")
        logging.exception("MockLLMæµ‹è¯•å¼‚å¸¸")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    print("ğŸ§ª LLMåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•MockLLM
    mock_success = test_mock_llm()
    
    # æµ‹è¯•çœŸå®LLM
    real_success = test_real_llm()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   MockLLM: {'âœ… æˆåŠŸ' if mock_success else 'âŒ å¤±è´¥'}")
    print(f"   çœŸå®LLM: {'âœ… æˆåŠŸ' if real_success else 'âŒ å¤±è´¥'}")
    
    if real_success:
        print("\nğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --use-real-llm å‚æ•°è¿è¡Œå®Œæ•´æ¼”ç¤º")
        print("   ç¤ºä¾‹: python llm_frozen_adaptive_demo.py --use-real-llm")
    
    print("=" * 50)

if __name__ == "__main__":
    main() 