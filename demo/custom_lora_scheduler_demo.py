#!/usr/bin/env python3
"""
è‡ªå®šä¹‰LoRAè°ƒåº¦å™¨æ¼”ç¤º

æµ‹è¯•ä¸ä¾èµ–vLLM LoRA adapterçš„è‡ªå®šä¹‰LoRAè°ƒåº¦å’Œæ›´æ–°åŠŸèƒ½
"""

import os
import time
import json
import logging
import numpy as np
from typing import Dict, List, Any

# SandGraph Core imports
try:
    from sandgraph.core.custom_lora_scheduler import (
        LoRAUpdateStrategy,
        LoRALoadingStatus,
        CustomLoRAConfig,
        CustomLoRAScheduler,
        CustomLoRAUpdater,
        create_custom_lora_scheduler,
        create_custom_lora_updater
    )
    HAS_SANDGRAPH = True
    print("âœ… SandGraph custom LoRA scheduler imported successfully")
except ImportError as e:
    HAS_SANDGRAPH = False
    print(f"âŒ SandGraph custom LoRA scheduler not available: {e}")
    print("Will use mock implementations")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_sample_lora_configs() -> Dict[int, CustomLoRAConfig]:
    """åˆ›å»ºç¤ºä¾‹LoRAé…ç½®"""
    lora_configs = {}
    
    # åˆ›å»º8ä¸ªLoRAé…ç½®
    for i in range(8):
        config = CustomLoRAConfig(
            lora_id=i,
            name=f"lora_{i}",
            base_model_path=f"/path/to/base/model_{i}",
            lora_weights_path=f"/path/to/lora/weights_{i}.bin",
            rank=16 + (i % 4) * 4,  # 16, 20, 24, 28
            alpha=32.0 + i * 2.0,
            dropout=0.1 + (i % 3) * 0.05,
            priority=1.0 + i * 0.1,
            max_concurrent_requests=10 + i * 2
        )
        lora_configs[i] = config
    
    return lora_configs


def demonstrate_scheduling_strategies():
    """æ¼”ç¤ºä¸åŒçš„è°ƒåº¦ç­–ç•¥"""
    print("\nğŸ¯ è°ƒåº¦ç­–ç•¥æ¼”ç¤º")
    print("=" * 50)
    
    if not HAS_SANDGRAPH:
        print("âŒ SandGraphä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # åˆ›å»ºLoRAé…ç½®
    lora_configs = create_sample_lora_configs()
    
    # æµ‹è¯•ä¸åŒçš„è°ƒåº¦ç­–ç•¥
    strategies = [
        LoRAUpdateStrategy.ROUND_ROBIN,
        LoRAUpdateStrategy.WEIGHTED_RANDOM,
        LoRAUpdateStrategy.PERFORMANCE_BASED,
        LoRAUpdateStrategy.ADAPTIVE
    ]
    
    for strategy in strategies:
        print(f"\nğŸ“Š æµ‹è¯•ç­–ç•¥: {strategy.value}")
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = create_custom_lora_scheduler(lora_configs, strategy)
        
        # æ¨¡æ‹Ÿå¤šæ¬¡é€‰æ‹©
        selections = []
        for _ in range(20):
            try:
                selected_lora = scheduler.select_lora()
                selections.append(selected_lora)
            except Exception as e:
                print(f"é€‰æ‹©å¤±è´¥: {e}")
                break
        
        # åˆ†æé€‰æ‹©ç»“æœ
        if selections:
            unique_selections = set(selections)
            print(f"  é€‰æ‹©æ¬¡æ•°: {len(selections)}")
            print(f"  å”¯ä¸€é€‰æ‹©: {len(unique_selections)}")
            print(f"  é€‰æ‹©åˆ†å¸ƒ: {dict(zip(*np.unique(selections, return_counts=True)))}")
            
            # è®¡ç®—è´Ÿè½½å‡è¡¡æ€§
            selection_counts = np.bincount(selections)
            load_balance = 1.0 - np.std(selection_counts) / np.mean(selection_counts)
            print(f"  è´Ÿè½½å‡è¡¡æ€§: {load_balance:.3f}")


def demonstrate_lora_processing():
    """æ¼”ç¤ºLoRAå¤„ç†åŠŸèƒ½"""
    print("\nğŸš€ LoRAå¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    if not HAS_SANDGRAPH:
        print("âŒ SandGraphä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # åˆ›å»ºLoRAé…ç½®
    lora_configs = create_sample_lora_configs()
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = create_custom_lora_scheduler(
        lora_configs, 
        strategy=LoRAUpdateStrategy.ADAPTIVE
    )
    
    print(f"åˆ›å»ºäº†åŒ…å«{len(scheduler.lora_configs)}ä¸ªLoRAçš„è°ƒåº¦å™¨")
    
    # æ¨¡æ‹Ÿå¤„ç†è¯·æ±‚
    num_requests = 10
    print(f"\nå¤„ç†{num_requests}ä¸ªè¯·æ±‚...")
    
    for i in range(num_requests):
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
            input_data = np.random.randn(1, 768).astype(np.float32)
            
            # åˆ›å»ºè¯·æ±‚ä¿¡æ¯
            request_info = {
                'request_id': f'req_{i}',
                'priority': 'high' if i % 3 == 0 else 'normal',
                'timestamp': time.time()
            }
            
            # é€‰æ‹©LoRA
            selected_lora = scheduler.select_lora(request_info)
            print(f"  è¯·æ±‚ {i+1}: é€‰æ‹©LoRA {selected_lora}")
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(0.1)
            
        except Exception as e:
            print(f"  è¯·æ±‚ {i+1} å¤„ç†å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºçŠ¶æ€ç»Ÿè®¡
    print("\nğŸ“Š LoRAçŠ¶æ€ç»Ÿè®¡:")
    status = scheduler.get_lora_status()
    for lora_id, stats in status.items():
        print(f"  LoRA {lora_id}:")
        print(f"    - çŠ¶æ€: {stats['loading_status']}")
        print(f"    - è¯·æ±‚æ•°: {stats['request_count']}")
        print(f"    - æˆåŠŸç‡: {stats['success_rate']:.3f}")
        print(f"    - å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}")
    
    # æ˜¾ç¤ºè°ƒåº¦å™¨ç»Ÿè®¡
    print("\nğŸ“ˆ è°ƒåº¦å™¨ç»Ÿè®¡:")
    scheduler_stats = scheduler.get_scheduler_stats()
    for key, value in scheduler_stats.items():
        print(f"  {key}: {value}")


def demonstrate_lora_updating():
    """æ¼”ç¤ºLoRAæ›´æ–°åŠŸèƒ½"""
    print("\nğŸ”„ LoRAæ›´æ–°æ¼”ç¤º")
    print("=" * 50)
    
    if not HAS_SANDGRAPH:
        print("âŒ SandGraphä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # åˆ›å»ºLoRAé…ç½®
    lora_configs = create_sample_lora_configs()
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = create_custom_lora_scheduler(lora_configs)
    
    # åˆ›å»ºæ›´æ–°å™¨
    updater = create_custom_lora_updater(scheduler, update_interval=5.0)
    
    print("å¯åŠ¨LoRAæ›´æ–°å™¨...")
    updater.start()
    
    # æ¨¡æ‹Ÿæ‰‹åŠ¨æ›´æ–°
    print("\næ¨¡æ‹Ÿæ‰‹åŠ¨æ›´æ–°LoRA 0...")
    try:
        updater.manual_update(0, "/path/to/new/weights_0.bin")
        print("âœ… æ‰‹åŠ¨æ›´æ–°æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ‰‹åŠ¨æ›´æ–°å¤±è´¥: {e}")
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    print("ç­‰å¾…5ç§’...")
    time.sleep(5)
    
    # åœæ­¢æ›´æ–°å™¨
    print("åœæ­¢LoRAæ›´æ–°å™¨...")
    updater.stop()
    
    print("âœ… LoRAæ›´æ–°æ¼”ç¤ºå®Œæˆ")


def demonstrate_integration_with_rl():
    """æ¼”ç¤ºä¸RLç³»ç»Ÿçš„é›†æˆ"""
    print("\nğŸ¤– ä¸RLç³»ç»Ÿé›†æˆæ¼”ç¤º")
    print("=" * 50)
    
    if not HAS_SANDGRAPH:
        print("âŒ SandGraphä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    try:
        from sandgraph.core.rl_algorithms import (
            CooperationType, CompetenceType,
            CooperationFactor, CompetenceFactor,
            MultiAgentOnPolicyRL
        )
        
        # åˆ›å»ºLoRAé…ç½®
        lora_configs = create_sample_lora_configs()
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = create_custom_lora_scheduler(lora_configs)
        
        # åˆ›å»ºRLæ™ºèƒ½ä½“é…ç½®
        cooperation_configs = []
        competence_configs = []
        
        for i in range(8):
            # åˆä½œé…ç½®
            cooperation_config = CooperationFactor(
                cooperation_type=CooperationType.TEAM_BASED if i < 4 else CooperationType.SHARED_REWARDS,
                cooperation_strength=0.3 + i * 0.1,
                team_size=4,
                shared_reward_ratio=0.6 + i * 0.05
            )
            cooperation_configs.append(cooperation_config)
            
            # èƒ½åŠ›é…ç½®
            competence_config = CompetenceFactor(
                competence_type=CompetenceType.ADAPTIVE,
                base_capability=0.4 + i * 0.1,
                learning_rate=0.02 + i * 0.01,
                adaptation_speed=0.15 + i * 0.05
            )
            competence_configs.append(competence_config)
        
        # åˆ›å»ºå¤šæ™ºèƒ½ä½“RLç³»ç»Ÿ
        multi_agent_rl = MultiAgentOnPolicyRL(
            num_agents=8,
            cooperation_configs=cooperation_configs,
            competence_configs=competence_configs
        )
        
        print(f"åˆ›å»ºäº†åŒ…å«{len(multi_agent_rl.agents)}ä¸ªRLæ™ºèƒ½ä½“çš„ç³»ç»Ÿ")
        print(f"åˆ›å»ºäº†åŒ…å«{len(scheduler.lora_configs)}ä¸ªLoRAçš„è°ƒåº¦å™¨")
        
        # æ¨¡æ‹Ÿé›†æˆåœºæ™¯
        print("\næ¨¡æ‹ŸRLæ™ºèƒ½ä½“ä¸LoRAè°ƒåº¦å™¨çš„åä½œ...")
        
        for i in range(5):
            # RLæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            agent_id = f"agent_{i % 8}"
            state = {"position": [i, i, i], "energy": 1.0}
            
            try:
                action, log_prob, value = multi_agent_rl.step(agent_id, state)
                print(f"  æ™ºèƒ½ä½“ {agent_id} é€‰æ‹©åŠ¨ä½œ: {action}")
                
                # æ ¹æ®RLæ™ºèƒ½ä½“çš„å†³ç­–é€‰æ‹©LoRA
                request_info = {
                    'agent_id': agent_id,
                    'action': action,
                    'priority': 'high' if value > 0.5 else 'normal'
                }
                
                selected_lora = scheduler.select_lora(request_info)
                print(f"  ä¸ºæ™ºèƒ½ä½“ {agent_id} é€‰æ‹©LoRA: {selected_lora}")
                
            except Exception as e:
                print(f"  æ™ºèƒ½ä½“ {agent_id} å¤„ç†å¤±è´¥: {e}")
        
        print("âœ… RLé›†æˆæ¼”ç¤ºå®Œæˆ")
        
    except ImportError as e:
        print(f"âŒ RLæ¨¡å—ä¸å¯ç”¨: {e}")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ è‡ªå®šä¹‰LoRAè°ƒåº¦å™¨æ¼”ç¤º")
    print("=" * 60)
    print("æœ¬æ¼”ç¤ºå±•ç¤º:")
    print("- è‡ªå®šä¹‰LoRAè°ƒåº¦ç­–ç•¥")
    print("- LoRAæƒé‡æ›´æ–°æœºåˆ¶")
    print("- ä¸RLç³»ç»Ÿçš„é›†æˆ")
    print("- ä¸ä¾èµ–vLLM LoRA adapterçš„ç‹¬ç«‹å®ç°")
    
    # æ¼”ç¤ºè°ƒåº¦ç­–ç•¥
    demonstrate_scheduling_strategies()
    
    # æ¼”ç¤ºLoRAå¤„ç†
    demonstrate_lora_processing()
    
    # æ¼”ç¤ºLoRAæ›´æ–°
    demonstrate_lora_updating()
    
    # æ¼”ç¤ºRLé›†æˆ
    demonstrate_integration_with_rl()
    
    print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“ æ€»ç»“:")
    print("- æˆåŠŸå®ç°äº†ä¸ä¾èµ–vLLMçš„è‡ªå®šä¹‰LoRAè°ƒåº¦å™¨")
    print("- æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ï¼šè½®è¯¢ã€åŠ æƒéšæœºã€åŸºäºæ€§èƒ½ã€è‡ªé€‚åº”")
    print("- æ”¯æŒLoRAæƒé‡çš„è‡ªåŠ¨å’Œæ‰‹åŠ¨æ›´æ–°")
    print("- å¯ä»¥ä¸RLç³»ç»Ÿé›†æˆï¼Œå®ç°æ™ºèƒ½è°ƒåº¦")
    print("- æä¾›äº†å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½")


if __name__ == "__main__":
    main()
