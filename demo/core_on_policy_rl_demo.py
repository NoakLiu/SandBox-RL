#!/usr/bin/env python3
"""
Core On-Policy RL Demo

æµ‹è¯•coreæ¨¡å—ä¸­çš„åˆä½œå› å­å’Œèƒ½åŠ›å› å­åŠŸèƒ½
"""

import numpy as np
import time
import logging
import json
from typing import Dict, List, Any

# Sandbox-RL Core imports
try:
    from sandbox_rl.core.rl_algorithms import (
        CooperationType,
        CompetenceType,
        CooperationFactor,
        CompetenceFactor,
        RLConfig,
        TrajectoryStep,
        OnPolicyRLAgent,
        MultiAgentOnPolicyRL
    )
    HAS_SANDGRAPH = True
    print("âœ… Sandbox-RL core RL algorithms imported successfully")
except ImportError as e:
    HAS_SANDGRAPH = False
    print(f"âŒ Sandbox-RL core RL algorithms not available: {e}")
    print("Will use mock implementations")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def demonstrate_cooperation_factors():
    """æ¼”ç¤ºä¸åŒçš„åˆä½œå› å­é…ç½®"""
    print("\nğŸ”— åˆä½œå› å­æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸åŒçš„åˆä½œé…ç½®
    cooperation_configs = [
        ("æ— åˆä½œ", CooperationFactor(
            cooperation_type=CooperationType.NONE,
            cooperation_strength=0.0
        )),
        ("å›¢é˜Ÿåˆä½œ", CooperationFactor(
            cooperation_type=CooperationType.TEAM_BASED,
            cooperation_strength=0.3,
            team_size=4,
            shared_reward_ratio=0.6
        )),
        ("å…±äº«å¥–åŠ±", CooperationFactor(
            cooperation_type=CooperationType.SHARED_REWARDS,
            cooperation_strength=0.2,
            shared_reward_ratio=0.8
        )),
        ("çŸ¥è¯†è½¬ç§»", CooperationFactor(
            cooperation_type=CooperationType.KNOWLEDGE_TRANSFER,
            cooperation_strength=0.4,
            knowledge_transfer_rate=0.15
        ))
    ]
    
    for i, (description, config) in enumerate(cooperation_configs):
        print(f"\n{i+1}. {description}")
        print(f"   - åˆä½œç±»å‹: {config.cooperation_type.value}")
        print(f"   - åˆä½œå¼ºåº¦: {config.cooperation_strength}")
        print(f"   - å›¢é˜Ÿå¤§å°: {config.team_size}")
        print(f"   - å…±äº«å¥–åŠ±æ¯”ä¾‹: {config.shared_reward_ratio}")
        print(f"   - çŸ¥è¯†è½¬ç§»ç‡: {config.knowledge_transfer_rate}")

def demonstrate_competence_factors():
    """æ¼”ç¤ºä¸åŒçš„èƒ½åŠ›å› å­é…ç½®"""
    print("\nğŸ¯ èƒ½åŠ›å› å­æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸åŒçš„èƒ½åŠ›é…ç½®
    competence_configs = [
        ("æ–°æ‰‹æ™ºèƒ½ä½“", CompetenceFactor(
            competence_type=CompetenceType.NOVICE,
            base_capability=0.3,
            learning_rate=0.01,
            adaptation_speed=0.05
        )),
        ("é€šç”¨æ™ºèƒ½ä½“", CompetenceFactor(
            competence_type=CompetenceType.GENERAL,
            base_capability=0.5,
            learning_rate=0.02,
            adaptation_speed=0.1
        )),
        ("ä¸“ä¸šæ™ºèƒ½ä½“", CompetenceFactor(
            competence_type=CompetenceType.SPECIALIZED,
            base_capability=0.6,
            learning_rate=0.03,
            specialization_level=0.4
        )),
        ("è‡ªé€‚åº”æ™ºèƒ½ä½“", CompetenceFactor(
            competence_type=CompetenceType.ADAPTIVE,
            base_capability=0.4,
            learning_rate=0.025,
            adaptation_speed=0.15
        )),
        ("ä¸“å®¶æ™ºèƒ½ä½“", CompetenceFactor(
            competence_type=CompetenceType.EXPERT,
            base_capability=0.8,
            learning_rate=0.01,
            specialization_level=0.6
        ))
    ]
    
    for i, (description, config) in enumerate(competence_configs):
        print(f"\n{i+1}. {description}")
        print(f"   - èƒ½åŠ›ç±»å‹: {config.competence_type.value}")
        print(f"   - åŸºç¡€èƒ½åŠ›: {config.base_capability}")
        print(f"   - å­¦ä¹ ç‡: {config.learning_rate}")
        print(f"   - é€‚åº”é€Ÿåº¦: {config.adaptation_speed}")
        print(f"   - ä¸“ä¸šåŒ–æ°´å¹³: {config.specialization_level}")

def run_multi_agent_demo():
    """è¿è¡Œå¤šæ™ºèƒ½ä½“æ¼”ç¤º"""
    print("\nğŸš€ å¤šæ™ºèƒ½ä½“On-Policy RLæ¼”ç¤º")
    print("=" * 50)
    
    if not HAS_SANDGRAPH:
        print("âŒ Sandbox-RLä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒæ¼”ç¤º")
        return
    
    # åˆ›å»ºåˆä½œé…ç½®
    cooperation_configs = []
    for i in range(8):
        if i < 4:
            # å‰4ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨å›¢é˜Ÿåˆä½œ
            cooperation_configs.append(CooperationFactor(
                cooperation_type=CooperationType.TEAM_BASED,
                cooperation_strength=0.3 + 0.1 * (i % 2),
                team_size=4,
                shared_reward_ratio=0.6
            ))
        else:
            # å4ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨å…±äº«å¥–åŠ±
            cooperation_configs.append(CooperationFactor(
                cooperation_type=CooperationType.SHARED_REWARDS,
                cooperation_strength=0.2,
                shared_reward_ratio=0.7
            ))
    
    # åˆ›å»ºèƒ½åŠ›é…ç½®
    competence_configs = []
    for i in range(8):
        competence_configs.append(CompetenceFactor(
            competence_type=CompetenceType.ADAPTIVE,
            base_capability=0.4 + 0.1 * (i % 3),
            learning_rate=0.02 + 0.01 * (i % 2),
            adaptation_speed=0.15 + 0.05 * (i % 2)
        ))
    
    # åˆ›å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    multi_agent_system = MultiAgentOnPolicyRL(
        num_agents=8,
        state_dim=64,
        action_dim=10,
        cooperation_configs=cooperation_configs,
        competence_configs=competence_configs
    )
    
    print(f"åˆ›å»ºäº†åŒ…å«{len(multi_agent_system.agents)}ä¸ªæ™ºèƒ½ä½“çš„ç³»ç»Ÿ")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    num_episodes = 20
    max_steps_per_episode = 50
    
    print(f"å¼€å§‹{num_episodes}ä¸ªepisodeçš„è®­ç»ƒ...")
    
    episode_rewards = []
    agent_stats_history = []
    
    for episode in range(num_episodes):
        episode_reward = 0.0
        
        for step in range(max_steps_per_episode):
            # éšæœºé€‰æ‹©æ™ºèƒ½ä½“
            agent_id = f"agent_{step % 8}"
            
            # åˆ›å»ºæ¨¡æ‹ŸçŠ¶æ€
            state = {
                "position": np.random.randn(3),
                "velocity": np.random.randn(3),
                "energy": np.random.uniform(0.5, 1.0),
                "step": step
            }
            
            # æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œ
            action, log_prob, value = multi_agent_system.step(agent_id, state)
            
            # æ¨¡æ‹Ÿå¥–åŠ±
            reward = np.sin(step * 0.1) + np.random.normal(0, 0.1)
            reward += 0.1 * (int(action.split('_')[1]) / 10)  # åŠ¨ä½œå¥–åŠ±
            
            # åˆ›å»ºè½¨è¿¹æ­¥éª¤
            trajectory_step = TrajectoryStep(
                state=state,
                action=action,
                reward=reward,
                value=value,
                log_prob=log_prob,
                done=(step == max_steps_per_episode - 1)
            )
            
            # æ›´æ–°æ™ºèƒ½ä½“
            multi_agent_system.update_agent(agent_id, trajectory_step)
            
            episode_reward += reward
        
        episode_rewards.append(episode_reward)
        
        # æ¯5ä¸ªepisodeæ”¶é›†ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
        if episode % 5 == 0:
            agent_stats = multi_agent_system.get_agent_stats()
            agent_stats_history.append({
                'episode': episode,
                'stats': agent_stats
            })
            print(f"Episode {episode + 1}: æ€»å¥–åŠ± = {episode_reward:.3f}")
    
    print(f"è®­ç»ƒå®Œæˆï¼å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    final_stats = multi_agent_system.get_agent_stats()
    print("\nğŸ“Š æœ€ç»ˆæ™ºèƒ½ä½“ç»Ÿè®¡:")
    for agent_id, stats in final_stats.items():
        print(f"  {agent_id}:")
        print(f"    - èƒ½åŠ›: {stats['capability']:.3f}")
        print(f"    - ç»éªŒæ•°: {stats['experience_count']}")
        print(f"    - åˆä½œç±»å‹: {stats['cooperation_type']}")
        print(f"    - èƒ½åŠ›ç±»å‹: {stats['competence_type']}")
        print(f"    - å¹³å‡å¥–åŠ±: {stats['avg_reward']:.3f}")
    
    # æ˜¾ç¤ºå›¢é˜Ÿä¿¡æ¯
    team_stats = multi_agent_system.get_team_stats()
    print("\nğŸ‘¥ å›¢é˜Ÿä¿¡æ¯:")
    for team_id, members in team_stats.items():
        print(f"  {team_id}: {members}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'episode_rewards': episode_rewards,
        'final_agent_stats': final_stats,
        'team_stats': team_stats,
        'training_config': {
            'num_agents': 8,
            'num_episodes': num_episodes,
            'max_steps_per_episode': max_steps_per_episode
        }
    }
    
    with open('core_on_policy_rl_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: core_on_policy_rl_results.json")
    
    return multi_agent_system, episode_rewards, agent_stats_history

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ Core On-Policy RL æ¼”ç¤º")
    print("=" * 60)
    print("æœ¬æ¼”ç¤ºå±•ç¤ºcoreæ¨¡å—ä¸­çš„:")
    print("- åˆä½œå› å­ï¼šæ§åˆ¶å¤šæ™ºèƒ½ä½“åä½œè¡Œä¸º")
    print("- èƒ½åŠ›å› å­ï¼šæ§åˆ¶ä¸ªä½“æ™ºèƒ½ä½“èƒ½åŠ›å’Œå­¦ä¹ ")
    print("- On-Policy RLï¼šæ”¯æŒåˆä½œå’Œèƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ")
    
    # æ¼”ç¤ºåˆä½œå› å­
    demonstrate_cooperation_factors()
    
    # æ¼”ç¤ºèƒ½åŠ›å› å­
    demonstrate_competence_factors()
    
    # è¿è¡Œå¤šæ™ºèƒ½ä½“æ¼”ç¤º
    if HAS_SANDGRAPH:
        multi_agent_system, episode_rewards, agent_stats_history = run_multi_agent_demo()
        
        print(f"\nğŸ“ˆ è®­ç»ƒç»“æœæ‘˜è¦:")
        print(f"  - æ€»episodeæ•°: {len(episode_rewards)}")
        print(f"  - å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f}")
        print(f"  - æœ€é«˜å¥–åŠ±: {max(episode_rewards):.3f}")
        print(f"  - æœ€ä½å¥–åŠ±: {min(episode_rewards):.3f}")
        print(f"  - å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.3f}")
        
        print("\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
    else:
        print("\nâš ï¸ å¤šæ™ºèƒ½ä½“æ¼”ç¤ºå› ç¼ºå°‘ä¾èµ–è€Œè·³è¿‡")
        print("   ç¡®ä¿Sandbox-RLå¯ç”¨ä»¥è·å¾—å®Œæ•´æ¼”ç¤º")

if __name__ == "__main__":
    main()
