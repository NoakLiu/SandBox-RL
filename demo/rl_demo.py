#!/usr/bin/env python3
"""
Sandbox-RLå¼ºåŒ–å­¦ä¹ æ¼”ç¤º

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¼˜åŒ–LLMï¼ŒåŒ…æ‹¬ï¼š
1. å‚æ•°å…±äº«çš„å¤šLLMèŠ‚ç‚¹ç®¡ç†
2. ç»éªŒå›æ”¾å’Œå¥–åŠ±æœºåˆ¶
3. åŸºäºå¥–åŠ±çš„æ¢¯åº¦æ›´æ–°
4. å¤šæ™ºèƒ½ä½“åä½œçš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹
"""

import sys
import json
import time
import asyncio
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, '.')

from sandbox_rl.core.rl_framework import (
    create_rl_framework, 
    SharedLLMManager, 
    Experience, 
    RewardType
)
from sandbox_rl.core.workflow import WorkflowGraph, WorkflowNode, NodeType
from sandbox_rl.sandbox_implementations import Game24Sandbox
from sandbox_rl.core.llm_interface import SharedLLMManager, create_shared_llm_manager


def print_separator(title: str, width: int = 60):
    """æ‰“å°åˆ†éš”çº¿"""
    print(f"\n{'=' * width}")
    print(f" {title} ".center(width))
    print(f"{'=' * width}\n")


def demo_shared_llm_manager():
    """æ¼”ç¤ºå…±äº«LLMç®¡ç†å™¨"""
    print_separator("å…±äº«LLMç®¡ç†å™¨æ¼”ç¤º")
    
    # åˆ›å»ºå…±äº«LLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager(
        model_name="demo_shared_llm",
        backend="mock",
        temperature=0.7,
        max_length=512
    )
    
    # æ³¨å†Œå¤šä¸ªLLMèŠ‚ç‚¹
    nodes = ["planner", "executor", "reviewer", "critic"]
    for node_id in nodes:
        llm_manager.register_node(node_id, {"role": node_id})
    
    print(f"æ³¨å†Œçš„LLMèŠ‚ç‚¹: {nodes}")
    
    # å¤šä¸ªèŠ‚ç‚¹æ‰§è¡Œæ¨ç†ï¼ˆå…±äº«åŒä¸€æ¨¡å‹ï¼‰
    responses = {}
    for node_id in nodes:
        prompt = f"ä½œä¸º{node_id}ï¼Œè¯·å¤„ç†è¿™ä¸ªä»»åŠ¡ï¼šè§£å†³æ•°å­¦é—®é¢˜"
        response = llm_manager.generate_for_node(node_id, prompt)
        responses[node_id] = response
        print(f"{node_id}: {response.text}")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    stats = llm_manager.get_global_stats()
    print(f"\næ¨¡å‹ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»ç”Ÿæˆæ¬¡æ•°: {stats['total_generations']}")
    print(f"  å‚æ•°æ›´æ–°æ¬¡æ•°: {stats['total_updates']}")
    print(f"  èŠ‚ç‚¹ç»Ÿè®¡: {json.dumps(stats['node_usage_stats'], indent=2, ensure_ascii=False)}")
    
    return llm_manager, responses


def demo_experience_and_rewards():
    """æ¼”ç¤ºç»éªŒè®°å½•å’Œå¥–åŠ±è®¡ç®—"""
    print_separator("ç»éªŒè®°å½•å’Œå¥–åŠ±è®¡ç®—æ¼”ç¤º")
    
    # åˆ›å»ºRLæ¡†æ¶
    rl_framework = create_rl_framework("demo_rl_llm")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„æ²™ç›’ä»»åŠ¡ç»éªŒ
    sandbox = Game24Sandbox(seed=42)
    case = sandbox.case_generator()
    prompt = sandbox.prompt_func(case)
    
    print(f"ç”Ÿæˆä»»åŠ¡: {case}")
    print(f"ä»»åŠ¡æç¤º: {prompt}")
    
    # æ¨¡æ‹ŸLLMå“åº”
    response = "æˆ‘éœ€è¦ä½¿ç”¨æ•°å­—82, 15, 4, 95æ¥å¾—åˆ°36ã€‚ç»è¿‡è®¡ç®—ï¼š\\boxed{(95-82)*4-15+3}"
    
    # è¯„ä¼°å“åº”
    score = sandbox.verify_score(response, case)
    evaluation_result = {
        "score": score,
        "response": response,
        "case": case,
        "correct": score > 0.5
    }
    
    print(f"LLMå“åº”: {response}")
    print(f"è¯„ä¼°åˆ†æ•°: {score}")
    
    # è®¡ç®—å¥–åŠ±
    rewards = rl_framework.reward_calculator.calculate_reward(
        evaluation_result,
        {"cycle": 1, "node_role": "executor"}
    )
    
    print(f"å¥–åŠ±åˆ†è§£: {json.dumps(rewards, indent=2, ensure_ascii=False)}")
    
    # åˆ›å»ºç»éªŒè®°å½•
    rl_framework.rl_trainer.add_experience(
        {"cycle": 1, "node_id": "executor", "task_type": "complex_workflow"},
        f"Generated response for executor",
        rewards["total"],
        True,
        "executor"
    )
    
    print(f"ç»éªŒç¼“å†²åŒºå¤§å°: {rl_framework.experience_buffer.size()}")
    
    return rl_framework, evaluation_result


def demo_rl_workflow():
    """æ¼”ç¤ºRLå¢å¼ºçš„å·¥ä½œæµ"""
    print_separator("RLå¢å¼ºå·¥ä½œæµæ¼”ç¤º")
    
    # åˆ›å»ºRLæ¡†æ¶
    rl_framework = create_rl_framework("workflow_rl_llm")
    
    # å¼€å§‹æ–°çš„è®­ç»ƒå›åˆ
    episode_id = rl_framework.start_new_episode()
    print(f"å¼€å§‹è®­ç»ƒå›åˆ: {episode_id}")
    
    # åˆ›å»ºRLå¢å¼ºçš„å·¥ä½œæµ
    graph = WorkflowGraph("rl_enhanced_workflow")
    
    # æ·»åŠ è¾“å…¥èŠ‚ç‚¹
    input_node = WorkflowNode("input", NodeType.INPUT)
    graph.add_node(input_node)
    
    # åˆ›å»ºæ”¯æŒRLçš„LLMèŠ‚ç‚¹
    planner_llm = rl_framework.create_rl_enabled_llm_node("planner", {"role": "ä»»åŠ¡è§„åˆ’"})
    executor_llm = rl_framework.create_rl_enabled_llm_node("executor", {"role": "ä»»åŠ¡æ‰§è¡Œ"})
    reviewer_llm = rl_framework.create_rl_enabled_llm_node("reviewer", {"role": "ç»“æœå®¡æ ¸"})
    
    # æ·»åŠ LLMèŠ‚ç‚¹åˆ°å·¥ä½œæµ
    planner_node = WorkflowNode("planner", NodeType.LLM, llm_func=planner_llm)
    executor_node = WorkflowNode("executor", NodeType.LLM, llm_func=executor_llm)
    reviewer_node = WorkflowNode("reviewer", NodeType.LLM, llm_func=reviewer_llm)
    
    graph.add_node(planner_node)
    graph.add_node(executor_node)
    graph.add_node(reviewer_node)
    
    # æ·»åŠ æ²™ç›’èŠ‚ç‚¹
    sandbox_node = WorkflowNode("sandbox", NodeType.SANDBOX, sandbox=Game24Sandbox())
    graph.add_node(sandbox_node)
    
    # æ·»åŠ è¾“å‡ºèŠ‚ç‚¹
    output_node = WorkflowNode("output", NodeType.OUTPUT)
    graph.add_node(output_node)
    
    # è¿æ¥èŠ‚ç‚¹
    graph.add_edge("input", "planner")
    graph.add_edge("planner", "sandbox")
    graph.add_edge("sandbox", "executor")
    graph.add_edge("executor", "reviewer")
    graph.add_edge("reviewer", "output")
    
    print(f"åˆ›å»ºRLå¢å¼ºå·¥ä½œæµï¼ŒèŠ‚ç‚¹æ•°: {len(graph.nodes)}")
    
    # æ‰§è¡Œå·¥ä½œæµï¼ˆæ¨¡æ‹Ÿå¤šè½®è®­ç»ƒï¼‰
    training_results = []
    
    for round_num in range(3):
        print(f"\n--- è®­ç»ƒè½®æ¬¡ {round_num + 1} ---")
        print(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...")
        
        try:
            # æ‰§è¡Œå·¥ä½œæµ
            result = graph.execute({"action": "full_cycle"})
            
            # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœå¹¶æ·»åŠ åˆ°RLæ¡†æ¶
            sandbox_result = result.get("output", {}).get("sandbox", {})
            if sandbox_result:
                evaluation_result = {
                    "score": 0.7 + round_num * 0.1,  # æ¨¡æ‹Ÿé€æ¸æ”¹å–„
                    "response": f"Round {round_num + 1} response",
                    "improvement": round_num * 0.1
                }
                
                print(f"\nèŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…:")
                # ä¸ºæ¯ä¸ªLLMèŠ‚ç‚¹åˆ›å»ºç»éªŒ
                for node_id in ["planner", "executor", "reviewer"]:
                    context = {
                        "evaluation_result": evaluation_result,
                        "done": True,
                        "round": round_num + 1
                    }
                    
                    # è¿™ä¼šè§¦å‘ç»éªŒè®°å½•å’Œå¯èƒ½çš„å‚æ•°æ›´æ–°
                    response = rl_framework.llm_manager.generate_for_node(node_id, f"Round {round_num + 1} task")
                    
                    # è®¡ç®—å¥–åŠ±
                    rewards = rl_framework.reward_calculator.calculate_reward(
                        evaluation_result,
                        {"cycle": round_num + 1, "node_role": node_id}
                    )
                    
                    print(f"  {node_id}:")
                    print(f"    - å“åº”: {response.text[:50]}...")
                    print(f"    - è¯„ä¼°åˆ†æ•°: {evaluation_result['score']:.3f}")
                    print(f"    - å¥–åŠ±åˆ†è§£: {json.dumps(rewards, indent=2, ensure_ascii=False)}")
                    
                    # å°è¯•æ›´æ–°ç­–ç•¥
                    update_result = rl_framework.rl_trainer.update_policy()
                    if update_result.get("status") == "updated":
                        print(f"    - ç­–ç•¥æ›´æ–°: æŸå¤± {update_result.get('total_loss', 0):.4f}")
            
            training_results.append({
                "round": round_num + 1,
                "status": "success",
                "result_keys": list(result.keys()) if result else []
            })
            
            # æ‰“å°å½“å‰è½®æ¬¡çš„ç»Ÿè®¡ä¿¡æ¯
            current_stats = rl_framework.get_rl_stats()
            print(f"\nå½“å‰è½®æ¬¡ç»Ÿè®¡:")
            print(f"  - ç»éªŒç¼“å†²åŒºå¤§å°: {current_stats['experience_buffer_size']}")
            print(f"  - ç­–ç•¥æ›´æ–°æ¬¡æ•°: {current_stats['training_stats'].get('training_step', 0)}")
            print(f"  - LLMæ¨ç†æ¬¡æ•°: {current_stats['llm_manager_info']['total_generations']}")
            print(f"  - å‚æ•°æ›´æ–°æ¬¡æ•°: {current_stats['llm_manager_info'].get('total_updates', 0)}")
            
        except Exception as e:
            print(f"è®­ç»ƒè½®æ¬¡ {round_num + 1} å¤±è´¥: {e}")
            training_results.append({
                "round": round_num + 1,
                "status": "failed",
                "error": str(e)
            })
    
    # è·å–è®­ç»ƒç»Ÿè®¡
    rl_stats = rl_framework.get_rl_stats()
    print(f"\nRLè®­ç»ƒç»Ÿè®¡:")
    print(f"  å½“å‰å›åˆ: {rl_stats['current_episode']}")
    print(f"  ç»éªŒç¼“å†²åŒºå¤§å°: {rl_stats['experience_buffer_size']}")
    print(f"  ç­–ç•¥æ›´æ–°æ¬¡æ•°: {rl_stats['training_stats'].get('training_step', 0)}")
    print(f"  LLMæ¨ç†æ¬¡æ•°: {rl_stats['llm_manager_info']['total_generations']}")
    print(f"  å‚æ•°æ›´æ–°æ¬¡æ•°: {rl_stats['llm_manager_info'].get('total_updates', 0)}")
    
    return rl_framework, training_results


def demo_multi_agent_collaboration():
    """æ¼”ç¤ºå¤šæ™ºèƒ½ä½“åä½œçš„å¼ºåŒ–å­¦ä¹ """
    print_separator("å¤šæ™ºèƒ½ä½“åä½œå¼ºåŒ–å­¦ä¹ æ¼”ç¤º")
    
    # åˆ›å»ºRLæ¡†æ¶
    rl_framework = create_rl_framework("collaborative_rl_llm")
    
    # æ³¨å†Œè‡ªå®šä¹‰åä½œå¥–åŠ±
    def collaboration_improvement_reward(context: Dict[str, Any]) -> float:
        """åä½œæ”¹å–„å¥–åŠ±"""
        solo_score = context.get("solo_performance", 0.0)
        collab_score = context.get("collaborative_performance", 0.0)
        improvement = collab_score - solo_score
        return max(0, improvement * 5.0)  # åä½œæ”¹å–„å¥–åŠ±
    
    rl_framework.reward_calculator.register_custom_reward(
        "collaboration_improvement", 
        collaboration_improvement_reward
    )
    
    # åˆ›å»ºå¤šä¸ªåä½œæ™ºèƒ½ä½“
    agents = ["agent_1", "agent_2", "agent_3"]
    agent_llms = {}
    
    for agent_id in agents:
        agent_llm = rl_framework.create_rl_enabled_llm_node(
            agent_id, 
            {"role": f"åä½œæ™ºèƒ½ä½“{agent_id[-1]}"}
        )
        agent_llms[agent_id] = agent_llm
    
    print(f"åˆ›å»ºåä½œæ™ºèƒ½ä½“: {agents}")
    
    # æ¨¡æ‹Ÿåä½œä»»åŠ¡
    tasks = [
        "è§£å†³å¤æ‚æ•°å­¦é—®é¢˜",
        "åˆ†ææ–‡æœ¬å†…å®¹",
        "åˆ¶å®šè§£å†³æ–¹æ¡ˆ"
    ]
    
    collaboration_results = []
    
    for task_idx, task in enumerate(tasks):
        print(f"\nåä½œä»»åŠ¡ {task_idx + 1}: {task}")
        
        # å•ç‹¬æ‰§è¡Œæ€§èƒ½
        solo_performances = []
        for agent_id in agents:
            response = agent_llms[agent_id](f"å•ç‹¬å¤„ç†: {task}")
            solo_score = 0.5 + task_idx * 0.1  # æ¨¡æ‹Ÿæ€§èƒ½
            solo_performances.append(solo_score)
            print(f"  {agent_id} å•ç‹¬æ€§èƒ½: {solo_score:.2f}")
        
        avg_solo_performance = sum(solo_performances) / len(solo_performances)
        
        # åä½œæ‰§è¡Œ
        collaboration_context = {
            "task": task,
            "agents": agents,
            "is_collaboration": True
        }
        
        collaborative_responses = []
        for agent_id in agents:
            prompt = f"ä¸å…¶ä»–æ™ºèƒ½ä½“åä½œå¤„ç†: {task}ã€‚å…¶ä»–æ™ºèƒ½ä½“çš„æ„è§æ˜¯ï¼š{collaborative_responses}"
            response = agent_llms[agent_id](prompt, collaboration_context)
            collaborative_responses.append(response)
        
        # æ¨¡æ‹Ÿåä½œæ€§èƒ½æ”¹å–„
        collaborative_performance = avg_solo_performance + 0.2 + task_idx * 0.05
        
        # è®¡ç®—åä½œå¥–åŠ±
        collab_context = {
            "solo_performance": avg_solo_performance,
            "collaborative_performance": collaborative_performance,
            "is_collaboration": True,
            "improvement_over_solo": collaborative_performance - avg_solo_performance
        }
        
        rewards = rl_framework.reward_calculator.calculate_reward(
            {"score": collaborative_performance},
            collab_context
        )
        
        collaboration_results.append({
            "task": task,
            "solo_avg": avg_solo_performance,
            "collaborative": collaborative_performance,
            "improvement": collaborative_performance - avg_solo_performance,
            "rewards": rewards
        })
        
        print(f"  åä½œæ€§èƒ½: {collaborative_performance:.2f}")
        print(f"  æ€§èƒ½æ”¹å–„: {collaborative_performance - avg_solo_performance:.2f}")
        print(f"  æ€»å¥–åŠ±: {rewards['total']:.2f}")
    
    # è·å–æœ€ç»ˆç»Ÿè®¡
    final_stats = rl_framework.get_rl_stats()
    
    print(f"\nåä½œå­¦ä¹ ç»Ÿè®¡:")
    print(f"  å‚ä¸æ™ºèƒ½ä½“: {len(agents)}")
    print(f"  åä½œä»»åŠ¡æ•°: {len(tasks)}")
    print(f"  æ€»ç»éªŒè®°å½•: {final_stats['experience_buffer_size']}")
    print(f"  å¹³å‡æ€§èƒ½æ”¹å–„: {sum(r['improvement'] for r in collaboration_results) / len(collaboration_results):.3f}")
    
    return rl_framework, collaboration_results


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print_separator("ğŸ¤– Sandbox-RLå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¼”ç¤º", 80)
    print("å±•ç¤ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMä¼˜åŒ–ï¼ŒåŒ…æ‹¬å‚æ•°å…±äº«å’Œå¤šæ™ºèƒ½ä½“åä½œ")
    
    try:
        # 1. å…±äº«LLMç®¡ç†å™¨æ¼”ç¤º
        llm_manager, responses = demo_shared_llm_manager()
        
        # 2. ç»éªŒå’Œå¥–åŠ±æ¼”ç¤º
        rl_framework_1, experience = demo_experience_and_rewards()
        
        # 3. RLå·¥ä½œæµæ¼”ç¤º
        rl_framework_2, training_results = demo_rl_workflow()
        
        # 4. å¤šæ™ºèƒ½ä½“åä½œæ¼”ç¤º
        rl_framework_3, collaboration_results = demo_multi_agent_collaboration()
        
        # æ€»ç»“
        print_separator("å¼ºåŒ–å­¦ä¹ æ¼”ç¤ºæ€»ç»“", 80)
        print("âœ… å…±äº«LLMç®¡ç†å™¨æ¼”ç¤ºå®Œæˆ - å±•ç¤ºäº†å‚æ•°å…±äº«æœºåˆ¶")
        print("âœ… ç»éªŒå’Œå¥–åŠ±æ¼”ç¤ºå®Œæˆ - å±•ç¤ºäº†å¥–åŠ±è®¡ç®—å’Œç»éªŒè®°å½•")
        print("âœ… RLå·¥ä½œæµæ¼”ç¤ºå®Œæˆ - å±•ç¤ºäº†è®­ç»ƒè¿‡ç¨‹é›†æˆ")
        print("âœ… å¤šæ™ºèƒ½ä½“åä½œæ¼”ç¤ºå®Œæˆ - å±•ç¤ºäº†åä½œå­¦ä¹ ")
        
        print(f"\nğŸ¯ æ ¸å¿ƒç‰¹æ€§éªŒè¯:")
        print(f"   âœ“ å‚æ•°å…±äº«ï¼šå¤šä¸ªLLMèŠ‚ç‚¹å…±äº«åŒä¸€æ¨¡å‹å‚æ•°")
        print(f"   âœ“ ç»éªŒå›æ”¾ï¼šç´¯ç§¯ç»éªŒç”¨äºæ‰¹é‡è®­ç»ƒ")
        print(f"   âœ“ å¥–åŠ±æœºåˆ¶ï¼šå¤šç»´åº¦å¥–åŠ±è®¡ç®—ï¼ˆå‡†ç¡®æ€§ã€æ•ˆç‡ã€åä½œï¼‰")
        print(f"   âœ“ æ¢¯åº¦æ›´æ–°ï¼šåŸºäºå¥–åŠ±çš„å‚æ•°ä¼˜åŒ–")
        print(f"   âœ“ å¤šæ™ºèƒ½ä½“ï¼šåä½œå­¦ä¹ å’Œæ€§èƒ½æ”¹å–„")
        
        print(f"\nğŸ”„ å¼ºåŒ–å­¦ä¹ å¾ªç¯éªŒè¯:")
        print(f"   çŠ¶æ€(State) â†’ åŠ¨ä½œ(Action) â†’ å¥–åŠ±(Reward) â†’ ç»éªŒ(Experience)")
        print(f"   â†’ æ‰¹é‡é‡‡æ · â†’ æ¢¯åº¦è®¡ç®— â†’ å‚æ•°æ›´æ–° â†’ æ€§èƒ½æ”¹å–„")
        
        return {
            "shared_llm": {"manager": llm_manager, "responses": responses},
            "experience_demo": {"framework": rl_framework_1, "experience": experience},
            "rl_workflow": {"framework": rl_framework_2, "results": training_results},
            "collaboration": {"framework": rl_framework_3, "results": collaboration_results}
        }
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


if __name__ == "__main__":
    result = main()
    
    # å¯é€‰ï¼šä¿å­˜æ¼”ç¤ºç»“æœåˆ°æ–‡ä»¶
    # with open("rl_demo_results.json", "w", encoding="utf-8") as f:
    #     json.dump(result, f, ensure_ascii=False, indent=2, default=str) 