#!/usr/bin/env python3
"""
æœåŠ¡å™¨ç¯å¢ƒå¤šæ¨¡å‹è®­ç»ƒè¿è¡Œè„šæœ¬
é…ç½®å¤§å†…å­˜å’Œæœ¬åœ°å­˜å‚¨ï¼Œä¿å­˜è¿è¡Œä¿¡æ¯å’Œæ£€æŸ¥ç‚¹
"""

import os
import sys
import json
import time
import logging
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
import gc

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/training_runner.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def setup_environment():
    """è®¾ç½®è¿è¡Œç¯å¢ƒ"""
    logger.info("ğŸ”§ è®¾ç½®è¿è¡Œç¯å¢ƒ...")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    directories = [
        "./training_outputs",
        "./checkpoints", 
        "./logs",
        "./temp"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env_vars = {
        "PYTHONPATH": f"{os.getcwd()}:{os.environ.get('PYTHONPATH', '')}",
        "CUDA_VISIBLE_DEVICES": "0,1,2,3",  # ä½¿ç”¨æ‰€æœ‰GPU
        "OMP_NUM_THREADS": "16",  # è®¾ç½®çº¿ç¨‹æ•°
        "MKL_NUM_THREADS": "16",
        "NUMEXPR_NUM_THREADS": "16",
        "TOKENIZERS_PARALLELISM": "false",
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:32768",  # 32GBå†…å­˜é™åˆ¶
        "TMPDIR": "./temp"  # ä½¿ç”¨æœ¬åœ°ä¸´æ—¶ç›®å½•
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        logger.info(f"   è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    
    logger.info("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    logger.info("ğŸ’¾ æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
    
    try:
        import psutil
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        
        logger.info(f"   å†…å­˜: {available_gb:.1f}GB å¯ç”¨ / {memory_gb:.1f}GB æ€»è®¡")
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count()
        logger.info(f"   CPU: {cpu_count} æ ¸å¿ƒ")
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('.')
        disk_gb = disk.total / (1024**3)
        free_gb = disk.free / (1024**3)
        
        logger.info(f"   ç£ç›˜: {free_gb:.1f}GB å¯ç”¨ / {disk_gb:.1f}GB æ€»è®¡")
        
        # GPUä¿¡æ¯
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                logger.info(f"   GPU: {gpu_count} ä¸ªGPUå¯ç”¨")
                
                for i in range(gpu_count):
                    device = torch.cuda.get_device_properties(i)
                    memory_gb = device.total_memory / (1024**3)
                    logger.info(f"     GPU {i}: {device.name}, {memory_gb:.1f}GB")
            else:
                logger.warning("   GPU: ä¸å¯ç”¨")
        except ImportError:
            logger.warning("   GPU: PyTorchæœªå®‰è£…")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {e}")
        return False

def run_training():
    """è¿è¡Œè®­ç»ƒ"""
    logger.info("ğŸš€ å¯åŠ¨å¤šæ¨¡å‹è®­ç»ƒ...")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    if not check_system_resources():
        logger.error("âŒ ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œé€€å‡ºè®­ç»ƒ")
        return False
    
    # åˆ›å»ºè®­ç»ƒè„šæœ¬
    script_content = '''#!/usr/bin/env python3
"""
å¤šæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
import json
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/training.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å¤šæ¨¡å‹è®­ç»ƒ...")
    
    try:
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        from demo.multi_model_single_env_simple import main as training_main
        
        # è¿è¡Œè®­ç»ƒ
        import asyncio
        asyncio.run(training_main())
        
        logger.info("âœ… è®­ç»ƒå®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
    
    script_path = "./temp/run_training.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod(script_path, 0o755)
    
    logger.info(f"ğŸ“ è®­ç»ƒè„šæœ¬å·²åˆ›å»º: {script_path}")
    
    # è¿è¡Œè®­ç»ƒ
    try:
        logger.info("ğŸ“‹ å¼€å§‹æ‰§è¡Œè®­ç»ƒ...")
        logger.info("=" * 80)
        
        # ä½¿ç”¨subprocessè¿è¡Œè®­ç»ƒ
        process = subprocess.Popen(
            [sys.executable, script_path],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        
        # ç­‰å¾…è¿›ç¨‹å®Œæˆ
        return_code = process.wait()
        
        logger.info("=" * 80)
        
        if return_code == 0:
            logger.info("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ")
            return True
        else:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def save_checkpoint():
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    logger.info("ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    checkpoint_dir = f"./checkpoints/checkpoint_{timestamp}"
    
    try:
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¿è¡Œé…ç½®
        config = {
            "timestamp": datetime.now().isoformat(),
            "checkpoint_dir": checkpoint_dir,
            "environment": dict(os.environ)
        }
        
        config_file = f"{checkpoint_dir}/config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # å¤åˆ¶è®­ç»ƒç»“æœ
        result_files = list(Path(".").glob("*.json"))
        for result_file in result_files:
            if "training" in result_file.name or "model" in result_file.name:
                shutil.copy2(result_file, checkpoint_dir)
        
        # å¤åˆ¶æ—¥å¿—æ–‡ä»¶
        log_files = list(Path("./logs").glob("*.log"))
        for log_file in log_files:
            shutil.copy2(log_file, checkpoint_dir)
        
        logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
        return checkpoint_dir
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None

def cleanup():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    logger.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    
    try:
        # æ¸…ç†ä¸´æ—¶è„šæœ¬
        temp_script = "./temp/run_training.py"
        if os.path.exists(temp_script):
            os.remove(temp_script)
            logger.info(f"   åˆ é™¤ä¸´æ—¶è„šæœ¬: {temp_script}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        temp_dir = "./temp"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            logger.info(f"   æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        logger.info("âœ… æ¸…ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœåŠ¡å™¨ç¯å¢ƒå¤šæ¨¡å‹è®­ç»ƒè¿è¡Œå™¨")
    print("=" * 60)
    
    start_time = datetime.now()
    
    try:
        # è¿è¡Œè®­ç»ƒ
        success = run_training()
        
        if success:
            print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = save_checkpoint()
            if checkpoint_dir:
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
            
            # æ˜¾ç¤ºè¾“å‡ºç›®å½•
            print(f"ğŸ“ è¾“å‡ºç›®å½•: ./training_outputs")
            print(f"ğŸ“‹ æ—¥å¿—ç›®å½•: ./logs")
            
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥ï¼")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = save_checkpoint()
            if checkpoint_dir:
                print(f"ğŸ’¾ å¤±è´¥æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
            
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        # ä¿å­˜ä¸­æ–­æ£€æŸ¥ç‚¹
        checkpoint_dir = save_checkpoint()
        if checkpoint_dir:
            print(f"ğŸ’¾ ä¸­æ–­æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
            
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå™¨é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
        
    finally:
        # æ¸…ç†
        cleanup()
        
        # æ˜¾ç¤ºè¿è¡Œæ—¶é—´
        end_time = datetime.now()
        duration = end_time - start_time
        print(f"\nâ±ï¸ æ€»è¿è¡Œæ—¶é—´: {duration}")

if __name__ == "__main__":
    main()
