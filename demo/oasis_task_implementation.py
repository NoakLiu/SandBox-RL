#!/usr/bin/env python3
"""
Oasisä»»åŠ¡å®ç°æ¼”ç¤º - é›†æˆSandGraphXè‡ªè¿›åŒ–LLM
==========================================

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Oasisä»»åŠ¡å®šä¹‰æ–‡æ¡£ä¸­æè¿°çš„ä»»åŠ¡ï¼Œ
ç»“åˆSandGraphXçš„è‡ªè¿›åŒ–LLMåŠŸèƒ½æ¥å®ç°æ™ºèƒ½çš„ç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿã€‚
"""

import sys
import os
import time
import json
import asyncio
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥SandGraphXè‡ªè¿›åŒ–Oasisæ¨¡å—
from sandgraph.core.self_evolving_oasis import (
    create_self_evolving_oasis,
    SelfEvolvingLLM,
    TaskType,
    EvolutionStrategy,
    SelfEvolvingConfig
)


@dataclass
class OasisTaskConfig:
    """Oasisä»»åŠ¡é…ç½®"""
    # åŸºç¡€é…ç½®
    enable_self_evolution: bool = True
    evolution_strategy: str = "multi_model"
    enable_lora: bool = True
    enable_kv_cache_compression: bool = True
    
    # ä»»åŠ¡ç‰¹å®šé…ç½®
    content_generation_config: dict = field(default_factory=lambda: {
        "model": "mistralai/Mistral-7B-Instruct-v0.2",
        "max_length": 512,
        "temperature": 0.7
    })
    
    behavior_analysis_config: dict = field(default_factory=lambda: {
        "model": "Qwen/Qwen-1_8B-Chat",
        "analysis_depth": "comprehensive",
        "update_frequency": "real_time"
    })
    
    network_optimization_config: dict = field(default_factory=lambda: {
        "model": "microsoft/Phi-2",
        "optimization_goal": "engagement_maximization",
        "constraint_type": "resource_limited"
    })
    
    # è¿›åŒ–é…ç½®
    evolution_interval: int = 10
    performance_threshold: float = 0.7
    adaptation_learning_rate: float = 1e-4
    model_pool_size: int = 5


@dataclass
class TaskPerformanceMetrics:
    """ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡"""
    # å‡†ç¡®æ€§æŒ‡æ ‡
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    
    # æ•ˆç‡æŒ‡æ ‡
    response_time: float = 0.0
    throughput: float = 0.0
    resource_usage: float = 0.0
    
    # è´¨é‡æŒ‡æ ‡
    content_quality: float = 0.0
    user_satisfaction: float = 0.0
    engagement_rate: float = 0.0
    
    # è¿›åŒ–æŒ‡æ ‡
    evolution_progress: float = 0.0
    adaptation_speed: float = 0.0
    learning_efficiency: float = 0.0


class ContentGenerationTask:
    """å†…å®¹ç”Ÿæˆä»»åŠ¡"""
    
    def __init__(self, evolving_llm: SelfEvolvingLLM):
        self.evolving_llm = evolving_llm
        self.task_type = TaskType.CONTENT_GENERATION
    
    async def generate_content(self, agent_profile: dict, context: dict) -> str:
        """ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹"""
        prompt = self._build_content_prompt(agent_profile, context)
        
        start_time = time.time()
        result = self.evolving_llm.process_task(
            self.task_type,
            prompt,
            {
                "agent_profile": agent_profile,
                "platform_context": context,
                "content_type": "post"
            }
        )
        response_time = time.time() - start_time
        
        if "error" not in result:
            content = result["response"].text
            performance_score = result.get("performance_score", 0.5)
        else:
            content = "AI technology is evolving rapidly! ğŸ¤–"
            performance_score = 0.3
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self._record_performance(response_time, performance_score)
        
        return content
    
    def _build_content_prompt(self, agent_profile: dict, context: dict) -> str:
        return f"""
        ä½œä¸º{agent_profile.get('personality', 'tech_enthusiast')}ç±»å‹çš„ç”¨æˆ·ï¼Œ
        åœ¨{context.get('platform', 'reddit')}å¹³å°ä¸Šç”Ÿæˆä¸€æ¡å…³äº{context.get('topic', 'AI technology')}çš„å†…å®¹ã€‚
        
        ç”¨æˆ·ç‰¹å¾ï¼š
        - æ€§æ ¼: {agent_profile.get('personality', 'tech_enthusiast')}
        - å…´è¶£: {agent_profile.get('interests', ['technology', 'AI'])}
        - æ´»è·ƒåº¦: {agent_profile.get('activity_level', 0.7)}
        
        å¹³å°ä¸Šä¸‹æ–‡ï¼š
        - å¹³å°: {context.get('platform', 'reddit')}
        - è¯é¢˜: {context.get('topic', 'AI technology')}
        - å½“å‰è¶‹åŠ¿: {context.get('trends', ['AI', 'social media'])}
        
        è¦æ±‚ï¼š
        1. ç¬¦åˆç”¨æˆ·æ€§æ ¼ç‰¹å¾
        2. é€‚åˆå¹³å°é£æ ¼
        3. å…·æœ‰äº’åŠ¨æ€§
        4. é•¿åº¦é€‚ä¸­ï¼ˆ100-200å­—ï¼‰
        5. åŒ…å«ç›¸å…³è¡¨æƒ…ç¬¦å·
        """
    
    def _record_performance(self, response_time: float, performance_score: float):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        logger.info(f"å†…å®¹ç”Ÿæˆä»»åŠ¡ - å“åº”æ—¶é—´: {response_time:.2f}s, æ€§èƒ½åˆ†æ•°: {performance_score:.3f}")


class BehaviorAnalysisTask:
    """è¡Œä¸ºåˆ†æä»»åŠ¡"""
    
    def __init__(self, evolving_llm: SelfEvolvingLLM):
        self.evolving_llm = evolving_llm
        self.task_type = TaskType.BEHAVIOR_ANALYSIS
    
    async def analyze_behavior(self, agent_actions: list, network_state: dict) -> dict:
        """åˆ†ææ™ºèƒ½ä½“è¡Œä¸ºæ¨¡å¼"""
        prompt = self._build_analysis_prompt(agent_actions, network_state)
        
        start_time = time.time()
        result = self.evolving_llm.process_task(
            self.task_type,
            prompt,
            {
                "agent_actions": agent_actions,
                "network_state": network_state,
                "analysis_type": "behavior_pattern"
            }
        )
        response_time = time.time() - start_time
        
        if "error" not in result:
            analysis_result = self._parse_analysis_result(result)
            performance_score = result.get("performance_score", 0.5)
        else:
            analysis_result = self._generate_default_analysis(agent_actions, network_state)
            performance_score = 0.3
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self._record_performance(response_time, performance_score)
        
        return analysis_result
    
    def _build_analysis_prompt(self, agent_actions: list, network_state: dict) -> str:
        return f"""
        åˆ†æä»¥ä¸‹æ™ºèƒ½ä½“è¡Œä¸ºæ•°æ®ï¼š
        
        1. è¡Œä¸ºåºåˆ—: {agent_actions[:10]}  # æ˜¾ç¤ºå‰10ä¸ªè¡Œä¸º
        2. ç½‘ç»œçŠ¶æ€: {network_state}
        
        è¯·åˆ†æï¼š
        1. è¡Œä¸ºæ¨¡å¼ç‰¹å¾
        2. ç¤¾äº¤å½±å“åŠ›
        3. å‚ä¸åº¦æ°´å¹³
        4. æ½œåœ¨è¶‹åŠ¿
        5. å»ºè®®æ”¹è¿›æ–¹å‘
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
        """
    
    def _parse_analysis_result(self, result: dict) -> dict:
        """è§£æåˆ†æç»“æœ"""
        try:
            # å°è¯•è§£æJSONæ ¼å¼çš„ç»“æœ
            response_text = result["response"].text
            if "{" in response_text and "}" in response_text:
                start = response_text.find("{")
                end = response_text.rfind("}") + 1
                json_str = response_text[start:end]
                return json.loads(json_str)
            else:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ
                return {
                    "behavior_pattern": "analyzed",
                    "social_influence": 0.6,
                    "engagement_level": 0.7,
                    "trends": ["positive"],
                    "suggestions": ["increase interaction frequency"]
                }
        except Exception as e:
            logger.warning(f"è§£æåˆ†æç»“æœå¤±è´¥: {e}")
            return self._generate_default_analysis([], {})
    
    def _generate_default_analysis(self, agent_actions: list, network_state: dict) -> dict:
        """ç”Ÿæˆé»˜è®¤åˆ†æç»“æœ"""
        return {
            "behavior_pattern": "standard",
            "social_influence": len(agent_actions) / 100.0,
            "engagement_level": network_state.get("active_users", 0) / max(network_state.get("total_users", 1), 1),
            "trends": ["stable"],
            "suggestions": ["maintain current activity level"]
        }
    
    def _record_performance(self, response_time: float, performance_score: float):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        logger.info(f"è¡Œä¸ºåˆ†æä»»åŠ¡ - å“åº”æ—¶é—´: {response_time:.2f}s, æ€§èƒ½åˆ†æ•°: {performance_score:.3f}")


class SocialDynamicsTask:
    """ç¤¾äº¤åŠ¨æ€ä»»åŠ¡"""
    
    def __init__(self, evolving_llm: SelfEvolvingLLM):
        self.evolving_llm = evolving_llm
        self.task_type = TaskType.NETWORK_OPTIMIZATION
    
    async def optimize_social_dynamics(self, network_graph: dict, agent_states: dict) -> dict:
        """ä¼˜åŒ–ç¤¾äº¤åŠ¨æ€"""
        prompt = self._build_dynamics_prompt(network_graph, agent_states)
        
        start_time = time.time()
        result = self.evolving_llm.process_task(
            self.task_type,
            prompt,
            {
                "network_graph": network_graph,
                "agent_states": agent_states,
                "optimization_goal": "engagement_maximization"
            }
        )
        response_time = time.time() - start_time
        
        if "error" not in result:
            optimization_result = self._parse_optimization_result(result)
            performance_score = result.get("performance_score", 0.5)
        else:
            optimization_result = self._generate_default_optimization(network_graph, agent_states)
            performance_score = 0.3
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self._record_performance(response_time, performance_score)
        
        return optimization_result
    
    def _build_dynamics_prompt(self, network_graph: dict, agent_states: dict) -> str:
        return f"""
        åˆ†æç¤¾äº¤ç½‘ç»œåŠ¨æ€ï¼š
        
        1. ç½‘ç»œç»“æ„: {network_graph}
        2. æ™ºèƒ½ä½“çŠ¶æ€: {agent_states}
        
        è¯·æä¾›ï¼š
        1. ç½‘ç»œä¼˜åŒ–å»ºè®®
        2. è¿æ¥ç­–ç•¥
        3. äº’åŠ¨ä¿ƒè¿›æ–¹æ¡ˆ
        4. ç¤¾åŒºå»ºè®¾ç­–ç•¥
        5. é¢„æœŸæ•ˆæœè¯„ä¼°
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ä¼˜åŒ–å»ºè®®ã€‚
        """
    
    def _parse_optimization_result(self, result: dict) -> dict:
        """è§£æä¼˜åŒ–ç»“æœ"""
        try:
            response_text = result["response"].text
            if "{" in response_text and "}" in response_text:
                start = response_text.find("{")
                end = response_text.rfind("}") + 1
                json_str = response_text[start:end]
                return json.loads(json_str)
            else:
                return {
                    "network_optimization": "suggested",
                    "connection_strategy": "enhance",
                    "interaction_promotion": "active",
                    "community_building": "focused",
                    "expected_impact": "positive"
                }
        except Exception as e:
            logger.warning(f"è§£æä¼˜åŒ–ç»“æœå¤±è´¥: {e}")
            return self._generate_default_optimization({}, {})
    
    def _generate_default_optimization(self, network_graph: dict, agent_states: dict) -> dict:
        """ç”Ÿæˆé»˜è®¤ä¼˜åŒ–å»ºè®®"""
        return {
            "network_optimization": "standard",
            "connection_strategy": "maintain",
            "interaction_promotion": "moderate",
            "community_building": "gradual",
            "expected_impact": "stable"
        }
    
    def _record_performance(self, response_time: float, performance_score: float):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        logger.info(f"ç¤¾äº¤åŠ¨æ€ä»»åŠ¡ - å“åº”æ—¶é—´: {response_time:.2f}s, æ€§èƒ½åˆ†æ•°: {performance_score:.3f}")


class OasisTaskScheduler:
    """Oasisä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self, evolving_llm: SelfEvolvingLLM, config: OasisTaskConfig):
        self.evolving_llm = evolving_llm
        self.config = config
        
        # åˆå§‹åŒ–ä»»åŠ¡å¤„ç†å™¨
        self.task_handlers = {
            "content_generation": ContentGenerationTask(evolving_llm),
            "behavior_analysis": BehaviorAnalysisTask(evolving_llm),
            "social_dynamics": SocialDynamicsTask(evolving_llm)
        }
        
        # æ€§èƒ½ç›‘æ§
        self.performance_history = []
        self.evolution_stats = []
        
        logger.info("Oasisä»»åŠ¡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def execute_task(self, task_type: str, task_data: dict) -> dict:
        """æ‰§è¡Œä»»åŠ¡"""
        if task_type not in self.task_handlers:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}")
        
        handler = self.task_handlers[task_type]
        
        try:
            if task_type == "content_generation":
                result = await handler.generate_content(
                    task_data.get("agent_profile", {}),
                    task_data.get("context", {})
                )
            elif task_type == "behavior_analysis":
                result = await handler.analyze_behavior(
                    task_data.get("agent_actions", []),
                    task_data.get("network_state", {})
                )
            elif task_type == "social_dynamics":
                result = await handler.optimize_social_dynamics(
                    task_data.get("network_graph", {}),
                    task_data.get("agent_states", {})
                )
            else:
                result = {"error": f"æœªå®ç°çš„ä»»åŠ¡ç±»å‹: {task_type}"}
            
            # è®°å½•æ€§èƒ½
            self._record_task_performance(task_type, result)
            
            return {
                "task_type": task_type,
                "result": result,
                "timestamp": datetime.now().isoformat(),
                "success": "error" not in result
            }
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task_type}: {e}")
            return {
                "task_type": task_type,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "success": False
            }
    
    async def execute_task_batch(self, tasks: list) -> list:
        """æ‰¹é‡æ‰§è¡Œä»»åŠ¡"""
        results = []
        for task in tasks:
            result = await self.execute_task(task["type"], task["data"])
            results.append(result)
        return results
    
    def _record_task_performance(self, task_type: str, result: Any):
        """è®°å½•ä»»åŠ¡æ€§èƒ½"""
        self.performance_history.append({
            "task_type": task_type,
            "result": result,
            "timestamp": datetime.now()
        })
    
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.performance_history:
            return {"total_tasks": 0, "success_rate": 0.0}
        
        total_tasks = len(self.performance_history)
        successful_tasks = len([p for p in self.performance_history if p.get("success", False)])
        success_rate = successful_tasks / total_tasks if total_tasks > 0 else 0.0
        
        return {
            "total_tasks": total_tasks,
            "successful_tasks": successful_tasks,
            "success_rate": success_rate,
            "recent_tasks": self.performance_history[-10:] if len(self.performance_history) >= 10 else self.performance_history
        }


class TaskMonitor:
    """ä»»åŠ¡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.performance_history = []
        self.evolution_stats = []
        self.alert_thresholds = {
            "success_rate": 0.7,
            "response_time": 5.0,
            "error_rate": 0.3
        }
    
    def record_task_performance(self, task_type: str, performance: dict):
        """è®°å½•ä»»åŠ¡æ€§èƒ½"""
        self.performance_history.append({
            "task_type": task_type,
            "performance": performance,
            "timestamp": datetime.now()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘è­¦æŠ¥
        self._check_alerts(task_type, performance)
    
    def analyze_performance_trends(self) -> dict:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.performance_history) < 5:
            return {"trend": "insufficient_data"}
        
        recent_performance = self.performance_history[-10:]
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_performance = sum(p["performance"].get("score", 0) for p in recent_performance) / len(recent_performance)
        
        # è®¡ç®—è¶‹åŠ¿
        if len(recent_performance) >= 2:
            first_half = recent_performance[:len(recent_performance)//2]
            second_half = recent_performance[len(recent_performance)//2:]
            
            first_avg = sum(p["performance"].get("score", 0) for p in first_half) / len(first_half)
            second_avg = sum(p["performance"].get("score", 0) for p in second_half) / len(second_half)
            
            if second_avg > first_avg * 1.1:
                trend = "improving"
            elif second_avg < first_avg * 0.9:
                trend = "declining"
            else:
                trend = "stable"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "average_performance": avg_performance,
            "recent_tasks": len(recent_performance)
        }
    
    def trigger_evolution(self, performance_threshold: float = 0.7) -> bool:
        """è§¦å‘è¿›åŒ–"""
        if len(self.performance_history) < 10:
            return False
        
        recent_performance = self.performance_history[-10:]
        avg_performance = sum(p["performance"].get("score", 0) for p in recent_performance) / len(recent_performance)
        
        return avg_performance < performance_threshold
    
    def _check_alerts(self, task_type: str, performance: dict):
        """æ£€æŸ¥è­¦æŠ¥"""
        score = performance.get("score", 0)
        response_time = performance.get("response_time", 0)
        
        if score < self.alert_thresholds["success_rate"]:
            logger.warning(f"ä»»åŠ¡æ€§èƒ½è­¦æŠ¥: {task_type} æ€§èƒ½åˆ†æ•° {score:.3f} ä½äºé˜ˆå€¼ {self.alert_thresholds['success_rate']}")
        
        if response_time > self.alert_thresholds["response_time"]:
            logger.warning(f"å“åº”æ—¶é—´è­¦æŠ¥: {task_type} å“åº”æ—¶é—´ {response_time:.2f}s è¶…è¿‡é˜ˆå€¼ {self.alert_thresholds['response_time']}s")


async def run_oasis_task_demo():
    """è¿è¡ŒOasisä»»åŠ¡æ¼”ç¤º"""
    
    print("ğŸš€ Oasisä»»åŠ¡å®ç°æ¼”ç¤º")
    print("=" * 60)
    print("ç‰¹æ€§:")
    print("- å†…å®¹ç”Ÿæˆä»»åŠ¡")
    print("- è¡Œä¸ºåˆ†æä»»åŠ¡")
    print("- ç¤¾äº¤åŠ¨æ€ä¼˜åŒ–ä»»åŠ¡")
    print("- è‡ªè¿›åŒ–LLMé›†æˆ")
    print("- æ€§èƒ½ç›‘æ§")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = OasisTaskConfig(
        enable_self_evolution=True,
        evolution_strategy="multi_model",
        enable_lora=True,
        enable_kv_cache_compression=True,
        evolution_interval=5,
        performance_threshold=0.7
    )
    
    # åˆ›å»ºè‡ªè¿›åŒ–LLM
    sandbox = create_self_evolving_oasis(
        evolution_strategy=config.evolution_strategy,
        enable_lora=config.enable_lora,
        enable_kv_cache_compression=config.enable_kv_cache_compression,
        model_pool_size=config.model_pool_size,
        evolution_interval=config.evolution_interval
    )
    
    # è·å–è‡ªè¿›åŒ–LLMå®ä¾‹
    evolving_llm = sandbox.evolving_llm
    
    # åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨
    task_scheduler = OasisTaskScheduler(evolving_llm, config)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = TaskMonitor()
    
    # æ¨¡æ‹Ÿæ•°æ®
    agent_profiles = [
        {"personality": "tech_enthusiast", "interests": ["AI", "technology"], "activity_level": 0.8},
        {"personality": "social_butterfly", "interests": ["social", "entertainment"], "activity_level": 0.9},
        {"personality": "news_reader", "interests": ["news", "politics"], "activity_level": 0.6}
    ]
    
    network_states = [
        {"total_users": 1000, "active_users": 800, "posts": 5000, "interactions": 15000},
        {"total_users": 1200, "active_users": 900, "posts": 6000, "interactions": 18000},
        {"total_users": 1500, "active_users": 1100, "posts": 8000, "interactions": 25000}
    ]
    
    # æ‰§è¡Œä»»åŠ¡æ¼”ç¤º
    for step in range(5):
        print(f"\n--- æ­¥éª¤ {step + 1} ---")
        
        # 1. å†…å®¹ç”Ÿæˆä»»åŠ¡
        print("æ‰§è¡Œå†…å®¹ç”Ÿæˆä»»åŠ¡...")
        content_result = await task_scheduler.execute_task("content_generation", {
            "agent_profile": random.choice(agent_profiles),
            "context": {
                "platform": "reddit",
                "topic": "AI technology",
                "trends": ["AI", "social media", "technology"]
            }
        })
        
        if content_result["success"]:
            print(f"âœ“ å†…å®¹ç”ŸæˆæˆåŠŸ: {content_result['result'][:100]}...")
        else:
            print(f"âœ— å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result.get('error', 'Unknown error')}")
        
        # 2. è¡Œä¸ºåˆ†æä»»åŠ¡
        print("æ‰§è¡Œè¡Œä¸ºåˆ†æä»»åŠ¡...")
        behavior_result = await task_scheduler.execute_task("behavior_analysis", {
            "agent_actions": [
                {"type": "post", "content": "Hello world", "timestamp": time.time()},
                {"type": "like", "target": "post_123", "timestamp": time.time()},
                {"type": "comment", "content": "Great post!", "timestamp": time.time()}
            ],
            "network_state": random.choice(network_states)
        })
        
        if behavior_result["success"]:
            print(f"âœ“ è¡Œä¸ºåˆ†ææˆåŠŸ: {behavior_result['result']}")
        else:
            print(f"âœ— è¡Œä¸ºåˆ†æå¤±è´¥: {behavior_result.get('error', 'Unknown error')}")
        
        # 3. ç¤¾äº¤åŠ¨æ€ä»»åŠ¡
        print("æ‰§è¡Œç¤¾äº¤åŠ¨æ€ä»»åŠ¡...")
        dynamics_result = await task_scheduler.execute_task("social_dynamics", {
            "network_graph": {
                "nodes": 1000,
                "edges": 5000,
                "density": 0.01,
                "clustering_coefficient": 0.3
            },
            "agent_states": {
                "active": 800,
                "inactive": 200,
                "engaged": 600,
                "disengaged": 400
            }
        })
        
        if dynamics_result["success"]:
            print(f"âœ“ ç¤¾äº¤åŠ¨æ€ä¼˜åŒ–æˆåŠŸ: {dynamics_result['result']}")
        else:
            print(f"âœ— ç¤¾äº¤åŠ¨æ€ä¼˜åŒ–å¤±è´¥: {dynamics_result.get('error', 'Unknown error')}")
        
        # è®°å½•æ€§èƒ½
        for result in [content_result, behavior_result, dynamics_result]:
            if result["success"]:
                monitor.record_task_performance(result["task_type"], {
                    "score": 0.8,  # æ¨¡æ‹Ÿæ€§èƒ½åˆ†æ•°
                    "response_time": 1.5,  # æ¨¡æ‹Ÿå“åº”æ—¶é—´
                    "success": True
                })
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        trends = monitor.analyze_performance_trends()
        print(f"æ€§èƒ½è¶‹åŠ¿: {trends['trend']}, å¹³å‡æ€§èƒ½: {trends['average_performance']:.3f}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›åŒ–
        if monitor.trigger_evolution(0.7):
            print("âš ï¸ æ£€æµ‹åˆ°æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®è§¦å‘æ¨¡å‹è¿›åŒ–")
        
        # è·å–è°ƒåº¦å™¨ç»Ÿè®¡
        scheduler_stats = task_scheduler.get_performance_stats()
        print(f"ä»»åŠ¡ç»Ÿè®¡: æ€»æ•°{scheduler_stats['total_tasks']}, æˆåŠŸç‡{scheduler_stats['success_rate']:.3f}")
        
        # è·å–è¿›åŒ–ç»Ÿè®¡
        evolution_stats = sandbox.evolving_llm.get_evolution_stats()
        print(f"è¿›åŒ–ç»Ÿè®¡: æ­¥éª¤{evolution_stats['evolution_step']}, æ¨¡å‹æ± {evolution_stats['model_pool_size']}")
    
    # ä¿å­˜ç»“æœ
    results = {
        "scheduler_stats": task_scheduler.get_performance_stats(),
        "evolution_stats": sandbox.evolving_llm.get_evolution_stats(),
        "performance_trends": monitor.analyze_performance_trends(),
        "config": config.__dict__
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs("./data", exist_ok=True)
    with open("./data/oasis_task_demo_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n=== æ¼”ç¤ºå®Œæˆ ===")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: ./data/oasis_task_demo_results.json")
    print(f"æ€»ä»»åŠ¡æ•°: {results['scheduler_stats']['total_tasks']}")
    print(f"æˆåŠŸç‡: {results['scheduler_stats']['success_rate']:.3f}")
    print(f"è¿›åŒ–æ­¥éª¤: {results['evolution_stats']['evolution_step']}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Oasisä»»åŠ¡å®ç°æ¼”ç¤º")
    parser.add_argument("--steps", type=int, default=5, help="æ¼”ç¤ºæ­¥æ•°")
    parser.add_argument("--strategy", type=str, default="multi_model", 
                       choices=["gradient_based", "meta_learning", "adaptive_compression", "multi_model"],
                       help="è¿›åŒ–ç­–ç•¥")
    
    args = parser.parse_args()
    
    try:
        results = asyncio.run(run_oasis_task_demo())
        print("\nâœ… æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 