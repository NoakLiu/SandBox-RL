#!/usr/bin/env python3
"""
SandGraph ç¤¾äº¤ç½‘ç»œç¯å¢ƒæ¼”ç¤º - åŸºäºRLçš„LLMå†³ç­–æ¶æ„

æ–°çš„æ¶æ„è®¾è®¡ï¼š
1. Sandboxä½œä¸ºç¯å¢ƒèŠ‚ç‚¹
2. LLMä½œä¸ºå†³ç­–å™¨ï¼ˆä¸æ˜¯èŠ‚ç‚¹ï¼‰
3. RLç®—æ³•æ›´æ–°LLMæƒé‡
4. çŠ¶æ€è½¬ç§»ç”±LLMå†³ç­–é©±åŠ¨
"""

import sys
import os
import time
import json
import argparse
import random
from typing import Dict, Any, List, Union
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sandgraph.core.llm_interface import create_shared_llm_manager
from sandgraph.core.sg_workflow import (
    SG_Workflow, WorkflowMode, EnhancedWorkflowNode,
    NodeType, NodeCondition, NodeLimits, GameState
)
from sandgraph.core.rl_algorithms import RLTrainer, RLConfig, RLAlgorithm
from sandgraph.sandbox_implementations import SocialNetworkSandbox


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")


class LLMDecisionMaker:
    """LLMå†³ç­–å™¨ - ä¸æ˜¯èŠ‚ç‚¹ï¼Œè€Œæ˜¯å†³ç­–å¼•æ“"""
    
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        self.decision_count = 0
        
        # å†å²æ•°æ®ç®¡ç†
        self.decision_history = []  # å†³ç­–å†å²
        self.network_history = []   # ç½‘ç»œçŠ¶æ€å†å²
        self.user_history = []      # ç”¨æˆ·è¡Œä¸ºå†å²
        self.performance_history = [] # è¡¨ç°å†å²
        
        # æ³¨å†Œå†³ç­–èŠ‚ç‚¹
        self.llm_manager.register_node("social_decision", {
            "role": "ç¤¾äº¤ç½‘ç»œç­–ç•¥ä¸“å®¶",
            "reasoning_type": "strategic",
            "temperature": 0.7,
            "max_length": 512
        })
    
    def make_decision(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºå½“å‰çŠ¶æ€åšå‡ºç¤¾äº¤ç½‘ç»œå†³ç­–"""
        self.decision_count += 1
        
        # æ„é€ å†³ç­–æç¤ºï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰
        prompt = self._construct_decision_prompt(state)
        print(f"\n{'='*80}")
        print(f"Decision {self.decision_count} - Complete Prompt Content:")
        print(f"{'='*80}")
        print(prompt)
        print(f"{'='*80}")
        
        # ä½¿ç”¨LLMç”Ÿæˆå†³ç­–
        try:
            response = self.llm_manager.generate_for_node(
                "social_decision", 
                prompt,
                temperature=0.7,
                max_new_tokens=128,
                do_sample=True,
                pad_token_id=self.llm_manager.tokenizer.eos_token_id if hasattr(self.llm_manager, 'tokenizer') else None
            )
            print(f"\nLLM Response Status: {response.status if hasattr(response, 'status') else 'unknown'}")
            print(f"LLM Complete Response: {response.text}")
        except Exception as e:
            print(f"LLM Call Error: {e}")
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™å†³ç­–
            return self._fallback_decision(state)
        
        # è§£æå†³ç­–
        decision = self._parse_decision(response.text, state)
        
        # æ›´æ–°å†å²æ•°æ®
        self._update_history(state, decision, response.text)
        
        return {
            "decision": decision,
            "llm_response": response.text,
            "prompt": prompt,
            "decision_count": self.decision_count
        }
    
    def _update_history(self, state: Dict[str, Any], decision: Dict[str, Any], llm_response: str):
        """æ›´æ–°å†å²æ•°æ®"""
        # è®°å½•å†³ç­–å†å²
        decision_record = {
            "step": self.decision_count,
            "timestamp": datetime.now().isoformat(),
            "decision": decision,
            "llm_response": llm_response,
            "network_state": state.get("network_state", {}),
            "user_behavior": state.get("user_behavior", {}),
            "content_metrics": state.get("content_metrics", {})
        }
        self.decision_history.append(decision_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.decision_history) > 50:
            self.decision_history = self.decision_history[-50:]
        
        # è®°å½•ç½‘ç»œçŠ¶æ€å†å²
        network_record = {
            "step": self.decision_count,
            "network_state": state.get("network_state", {}),
            "user_behavior": state.get("user_behavior", {})
        }
        self.network_history.append(network_record)
        
        # è®°å½•ç”¨æˆ·è¡Œä¸ºå†å²
        user_record = {
            "step": self.decision_count,
            "user_behavior": state.get("user_behavior", {}),
            "engagement_score": self._calculate_engagement_score(state)
        }
        self.user_history.append(user_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.network_history) > 30:
            self.network_history = self.network_history[-30:]
        if len(self.user_history) > 30:
            self.user_history = self.user_history[-30:]
    
    def _calculate_engagement_score(self, state: Dict[str, Any]) -> float:
        """è®¡ç®—ç”¨æˆ·å‚ä¸åº¦åˆ†æ•°"""
        user_behavior = state.get("user_behavior", {})
        
        # è®¡ç®—å„ç§å‚ä¸åº¦æŒ‡æ ‡
        total_posts = user_behavior.get("posts_created", 0)
        total_likes = user_behavior.get("likes_given", 0)
        total_comments = user_behavior.get("comments_made", 0)
        total_shares = user_behavior.get("shares_made", 0)
        active_users = user_behavior.get("active_users", 0)
        
        # ç»¼åˆå‚ä¸åº¦åˆ†æ•°
        engagement_score = (
            total_posts * 0.3 +
            total_likes * 0.2 +
            total_comments * 0.3 +
            total_shares * 0.2
        ) * (active_users / 100.0)  # å½’ä¸€åŒ–
        
        return min(1.0, engagement_score)
    
    def _construct_decision_prompt(self, state: Dict[str, Any]) -> str:
        """æ„é€ å†³ç­–æç¤ºï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰"""
        network_state = state.get("network_state", {})
        user_behavior = state.get("user_behavior", {})
        content_metrics = state.get("content_metrics", {})
        
        # æ„å»ºç½‘ç»œçŠ¶æ€æ‘˜è¦
        network_summary = []
        for user_id, user_data in network_state.items():
            network_summary.append(
                f"User {user_id}: "
                f"Followers={user_data.get('followers', 0)}, "
                f"Following={user_data.get('following', 0)}, "
                f"Posts={user_data.get('posts', 0)}, "
                f"Engagement={user_data.get('engagement_rate', 0):.2f}%"
            )
        
        # æ„å»ºç”¨æˆ·è¡Œä¸ºæ‘˜è¦
        behavior_summary = f"""
Current User Behavior:
- Active Users: {user_behavior.get('active_users', 0)}
- Posts Created: {user_behavior.get('posts_created', 0)}
- Likes Given: {user_behavior.get('likes_given', 0)}
- Comments Made: {user_behavior.get('comments_made', 0)}
- Shares Made: {user_behavior.get('shares_made', 0)}
- Average Session Time: {user_behavior.get('avg_session_time', 0):.1f} minutes
"""
        
        # æ„å»ºå†…å®¹æŒ‡æ ‡æ‘˜è¦
        content_summary = f"""
Content Performance:
- Viral Posts: {content_metrics.get('viral_posts', 0)}
- Trending Topics: {content_metrics.get('trending_topics', 0)}
- Content Quality Score: {content_metrics.get('quality_score', 0):.2f}
- User Satisfaction: {content_metrics.get('satisfaction_score', 0):.2f}
"""
        
        # æ„å»ºå†å²å†³ç­–æ‘˜è¦
        history_summary = ""
        if self.decision_history:
            recent_decisions = self.decision_history[-5:]  # æœ€è¿‘5ä¸ªå†³ç­–
            history_summary = "\nRecent Decision History:\n"
            for record in recent_decisions:
                decision = record["decision"]
                history_summary += f"- Step {record['step']}: {decision['action']} - {decision.get('reasoning', 'No reasoning')}\n"
        
        # æ„å»ºå†å²è¡¨ç°æ‘˜è¦
        performance_summary = ""
        if self.user_history:
            recent_performance = self.user_history[-5:]  # æœ€è¿‘5ä¸ªè¡¨ç°è®°å½•
            performance_summary = "\nRecent Performance:\n"
            for record in recent_performance:
                performance_summary += f"- Step {record['step']}: Engagement Score = {record['engagement_score']:.3f}\n"
        
        # æ„å»ºå®Œæ•´æç¤º
        prompt = f"""
You are a social network strategy expert in a simulation game. This is NOT real social media management - it's a simulation game for testing AI strategies.

IMPORTANT: You MUST respond in ENGLISH only. Do NOT use Chinese or any other language.

Your goal is to maximize user engagement and network growth through strategic content and user interaction decisions.

Current Network State:
{chr(10).join(network_summary)}

{behavior_summary}

{content_summary}

{history_summary}

{performance_summary}

Based on the current state and history, you must make a specific decision to improve the social network. You can choose from these actions:

1. CREATE_POST - Create engaging content to increase user activity
2. ENCOURAGE_INTERACTION - Promote likes, comments, and shares
3. FEATURE_USER - Highlight active users to increase engagement
4. LAUNCH_CAMPAIGN - Start a viral marketing campaign
5. IMPROVE_ALGORITHM - Optimize content recommendation algorithm
6. ADD_FEATURE - Introduce new social features
7. MODERATE_CONTENT - Improve content quality and safety
8. EXPAND_NETWORK - Grow user base through targeted outreach

CRITICAL REQUIREMENTS:
- You MUST respond in ENGLISH only
- You MUST provide a specific action with clear reasoning
- No vague responses, no cautious strategies, no holding back
- This is a simulation game - be bold and specific!
- You MUST use the exact format below

REQUIRED FORMAT (copy and fill in):
ACTION: [specific action name from the list above]
TARGET: [specific target if applicable, or "N/A"]
REASONING: [clear explanation of why this action will improve the network]

Example response:
ACTION: CREATE_POST
TARGET: N/A
REASONING: The network has low content creation activity, creating engaging posts will increase user engagement and attract more active users.

What specific action will you take to improve this social network? Respond in the exact format above.
"""
        
        return prompt
    
    def _parse_decision(self, response: str, state: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æLLMå†³ç­–å“åº”"""
        response = response.strip().upper()
        
        # å®šä¹‰å¯èƒ½çš„åŠ¨ä½œ
        valid_actions = [
            "CREATE_POST", "ENCOURAGE_INTERACTION", "FEATURE_USER", 
            "LAUNCH_CAMPAIGN", "IMPROVE_ALGORITHM", "ADD_FEATURE", 
            "MODERATE_CONTENT", "EXPAND_NETWORK"
        ]
        
        # æ£€æŸ¥å“åº”ä¸­çš„åŠ¨ä½œ
        selected_action = None
        target = None
        reasoning = "No specific reasoning provided"
        
        # æå–åŠ¨ä½œ
        for action in valid_actions:
            if action in response:
                selected_action = action
                break
        
        # æå–ç›®æ ‡
        if "TARGET:" in response:
            target_start = response.find("TARGET:") + 7
            target_end = response.find("\n", target_start)
            if target_end == -1:
                target_end = len(response)
            target = response[target_start:target_end].strip()
        
        # æå–æ¨ç†
        if "REASONING:" in response:
            reasoning_start = response.find("REASONING:") + 10
            reasoning_end = response.find("\n", reasoning_start)
            if reasoning_end == -1:
                reasoning_end = len(response)
            reasoning = response[reasoning_start:reasoning_end].strip()
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨é»˜è®¤åŠ¨ä½œ
        if not selected_action:
            selected_action = "CREATE_POST"
            reasoning = "Fallback decision due to unclear response"
        
        return {
            "action": selected_action,
            "target": target,
            "reasoning": reasoning,
            "confidence": 0.8
        }
    
    def _fallback_decision(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """å¤‡ç”¨å†³ç­–é€»è¾‘"""
        user_behavior = state.get("user_behavior", {})
        active_users = user_behavior.get("active_users", 0)
        
        # åŸºäºå½“å‰çŠ¶æ€é€‰æ‹©å¤‡ç”¨åŠ¨ä½œ
        if active_users < 50:
            action = "EXPAND_NETWORK"
            reasoning = "Low user base - need to grow network"
        elif user_behavior.get("posts_created", 0) < 10:
            action = "CREATE_POST"
            reasoning = "Low content creation - need more posts"
        elif user_behavior.get("likes_given", 0) < 20:
            action = "ENCOURAGE_INTERACTION"
            reasoning = "Low interaction - need to encourage engagement"
        else:
            action = "IMPROVE_ALGORITHM"
            reasoning = "Stable state - optimize for better performance"
        
        return {
            "action": action,
            "target": None,
            "reasoning": reasoning,
            "confidence": 0.6
        }


def create_rl_social_workflow(llm_manager) -> tuple[SG_Workflow, RLTrainer, LLMDecisionMaker]:
    """åˆ›å»ºåŸºäºRLçš„LLMå†³ç­–ç¤¾äº¤ç½‘ç»œå·¥ä½œæµ"""
    
    # åˆ›å»ºRLé…ç½®
    rl_config = RLConfig(
        algorithm=RLAlgorithm.PPO,
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        batch_size=32,
        mini_batch_size=8,
        ppo_epochs=4,
        target_kl=0.01
    )
    
    # åˆ›å»ºRLè®­ç»ƒå™¨
    rl_trainer = RLTrainer(rl_config, llm_manager)
    
    # åˆ›å»ºLLMå†³ç­–å™¨
    decision_maker = LLMDecisionMaker(llm_manager)
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow = SG_Workflow("rl_social_workflow", WorkflowMode.TRADITIONAL, llm_manager)
    
    # åˆ›å»ºç¤¾äº¤ç½‘ç»œæ²™ç›’
    sandbox = SocialNetworkSandbox(
        initial_users=100,
        max_users=1000
    )
    
    # åˆ›å»ºç¤¾äº¤ç½‘ç»œç¯å¢ƒèŠ‚ç‚¹
    def social_env_func(inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ç¤¾äº¤ç½‘ç»œç¯å¢ƒèŠ‚ç‚¹å‡½æ•°"""
        # è·å–å½“å‰çŠ¶æ€
        case = sandbox.case_generator()
        current_state = case["state"]
        
        # ä½¿ç”¨LLMåšå‡ºå†³ç­–
        decision_result = decision_maker.make_decision(current_state)
        decision = decision_result["decision"]
        
        # æ‰§è¡Œç¤¾äº¤ç½‘ç»œå†³ç­–
        try:
            # éªŒè¯å’Œæ‰§è¡Œå†³ç­–
            score = sandbox.verify_score(
                f"{decision['action']} {decision.get('target', '')}",
                case
            )
            
            # è®¡ç®—å¥–åŠ±
            reward = score * 10  # å°†åˆ†æ•°è½¬æ¢ä¸ºå¥–åŠ±
            
            # æ„å»ºçŠ¶æ€ç‰¹å¾
            state_features = {
                "active_users": current_state["user_behavior"]["active_users"],
                "engagement_rate": _calculate_engagement_rate(current_state),
                "content_quality": current_state["content_metrics"]["quality_score"],
                "network_growth": _calculate_network_growth(current_state),
                "decision_type": _encode_decision_type(decision["action"])
            }
            
            # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
            rl_trainer.add_experience(
                state=state_features,
                action=json.dumps(decision),
                reward=reward,
                done=False
            )
            
            # æ›´æ–°ç­–ç•¥
            update_result = rl_trainer.update_policy()
            
            return {
                "state": current_state,
                "decision": decision,
                "llm_response": decision_result["llm_response"],
                "score": score,
                "reward": reward,
                "rl_update": update_result,
                "sandbox_id": sandbox.sandbox_id
            }
            
        except Exception as e:
            print(f"ç¤¾äº¤ç½‘ç»œæ‰§è¡Œé”™è¯¯: {e}")
            return {
                "state": current_state,
                "decision": {"action": "CREATE_POST", "reasoning": f"æ‰§è¡Œé”™è¯¯: {e}"},
                "score": 0.0,
                "reward": 0.0,
                "error": str(e)
            }
    
    # æ·»åŠ ç¤¾äº¤ç½‘ç»œç¯å¢ƒèŠ‚ç‚¹
    social_env_node = EnhancedWorkflowNode(
        "social_environment",
        NodeType.SANDBOX,
        sandbox=sandbox,
        condition=NodeCondition(),
        limits=NodeLimits(max_visits=10, resource_cost={"energy": 10, "tokens": 5})
    )
    workflow.add_node(social_env_node)
    
    return workflow, rl_trainer, decision_maker


def _calculate_engagement_rate(state: Dict[str, Any]) -> float:
    """è®¡ç®—ç”¨æˆ·å‚ä¸ç‡"""
    user_behavior = state.get("user_behavior", {})
    active_users = user_behavior.get("active_users", 0)
    total_users = len(state.get("network_state", {}))
    
    if total_users == 0:
        return 0.0
    
    return active_users / total_users


def _calculate_network_growth(state: Dict[str, Any]) -> float:
    """è®¡ç®—ç½‘ç»œå¢é•¿ç‡"""
    network_state = state.get("network_state", {})
    total_users = len(network_state)
    
    # åŸºäºç”¨æˆ·æ•°é‡å’Œè¿æ¥åº¦è®¡ç®—å¢é•¿ç‡
    total_connections = 0
    for user_data in network_state.values():
        total_connections += user_data.get("followers", 0) + user_data.get("following", 0)
    
    if total_users == 0:
        return 0.0
    
    avg_connections = total_connections / total_users
    return min(1.0, avg_connections / 100.0)  # å½’ä¸€åŒ–


def _encode_decision_type(action: str) -> int:
    """ç¼–ç å†³ç­–ç±»å‹"""
    action_map = {
        "CREATE_POST": 1,
        "ENCOURAGE_INTERACTION": 2,
        "FEATURE_USER": 3,
        "LAUNCH_CAMPAIGN": 4,
        "IMPROVE_ALGORITHM": 5,
        "ADD_FEATURE": 6,
        "MODERATE_CONTENT": 7,
        "EXPAND_NETWORK": 8
    }
    return action_map.get(action, 0)


def run_rl_social_demo(steps: int = 5, model_name: str = "mistralai/Mistral-7B-Instruct-v0.2"):
    """è¿è¡ŒåŸºäºRLçš„LLMå†³ç­–ç¤¾äº¤ç½‘ç»œæ¼”ç¤º"""
    
    print_section("RL-based LLM Decision Social Network Demo")
    
    # 1. åˆ›å»ºLLMç®¡ç†å™¨
    print(f"\n1. Creating LLM Manager with model: {model_name}")
    llm_manager = create_shared_llm_manager(
        model_name=model_name,
        backend="huggingface",
        temperature=0.7,
        max_length=512,
        device="auto",
        torch_dtype="float16"
    )
    
    # 2. åˆ›å»ºå·¥ä½œæµå’ŒRLè®­ç»ƒå™¨
    print("\n2. Creating RL Social Network Workflow")
    workflow, rl_trainer, decision_maker = create_rl_social_workflow(llm_manager)
    
    # 3. æ‰§è¡Œå¤šæ­¥ç¤¾äº¤ç½‘ç»œç®¡ç†
    print(f"\n3. Executing {steps} Social Network Management Steps")
    
    results = []
    for step in range(steps):
        print(f"\n--- ç¬¬ {step + 1} æ­¥ ---")
        
        try:
            # ç›´æ¥æ‰§è¡Œç¤¾äº¤ç½‘ç»œç¯å¢ƒèŠ‚ç‚¹
            node = workflow.nodes.get("social_environment")
            if node and node.sandbox:
                # è·å–å½“å‰çŠ¶æ€
                case = node.sandbox.case_generator()
                current_state = case["state"]
                
                # ä½¿ç”¨LLMåšå‡ºå†³ç­–
                decision_result = decision_maker.make_decision(current_state)
                decision = decision_result["decision"]
                
                # æ‰§è¡Œç¤¾äº¤ç½‘ç»œå†³ç­–
                try:
                    # éªŒè¯å’Œæ‰§è¡Œå†³ç­–
                    score = node.sandbox.verify_score(
                        f"{decision['action']} {decision.get('target', '')}",
                        case
                    )
                    
                    # è®¡ç®—å¥–åŠ±
                    reward = score * 10
                    
                    # æ„å»ºçŠ¶æ€ç‰¹å¾
                    state_features = {
                        "active_users": current_state["user_behavior"]["active_users"],
                        "engagement_rate": _calculate_engagement_rate(current_state),
                        "content_quality": current_state["content_metrics"]["quality_score"],
                        "network_growth": _calculate_network_growth(current_state),
                        "decision_type": _encode_decision_type(decision["action"])
                    }
                    
                    # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
                    rl_trainer.add_experience(
                        state=state_features,
                        action=json.dumps(decision),
                        reward=reward,
                        done=False
                    )
                    
                    # æ›´æ–°ç­–ç•¥
                    update_result = rl_trainer.update_policy()
                    
                    result = {
                        "step": step + 1,
                        "state": current_state,
                        "decision": decision,
                        "llm_response": decision_result["llm_response"],
                        "score": score,
                        "reward": reward,
                        "rl_update": update_result
                    }
                    
                    results.append(result)
                    
                    # æ‰“å°ç»“æœ
                    print(f"Decision: {decision['action']}")
                    print(f"Target: {decision.get('target', 'N/A')}")
                    print(f"Reasoning: {decision['reasoning']}")
                    print(f"Score: {score:.2f}")
                    print(f"Reward: {reward:.2f}")
                    print(f"Active Users: {current_state['user_behavior']['active_users']}")
                    print(f"Engagement Rate: {_calculate_engagement_rate(current_state):.3f}")
                    
                except Exception as e:
                    print(f"æ‰§è¡Œé”™è¯¯: {e}")
                    result = {
                        "step": step + 1,
                        "error": str(e),
                        "decision": {"action": "CREATE_POST", "reasoning": f"æ‰§è¡Œé”™è¯¯: {e}"}
                    }
                    results.append(result)
            
        except Exception as e:
            print(f"æ­¥éª¤ {step + 1} æ‰§è¡Œå¤±è´¥: {e}")
            result = {
                "step": step + 1,
                "error": str(e)
            }
            results.append(result)
    
    # 4. æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print_section("Final Results")
    
    successful_steps = [r for r in results if "error" not in r]
    if successful_steps:
        avg_score = sum(r["score"] for r in successful_steps) / len(successful_steps)
        avg_reward = sum(r["reward"] for r in successful_steps) / len(successful_steps)
        final_engagement = _calculate_engagement_rate(successful_steps[-1]["state"])
        
        print(f"Average Score: {avg_score:.2f}")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Final Engagement Rate: {final_engagement:.3f}")
        print(f"Successful Steps: {len(successful_steps)}/{steps}")
    
    # 5. æ˜¾ç¤ºRLè®­ç»ƒç»Ÿè®¡
    print_section("RL Training Statistics")
    training_stats = rl_trainer.get_training_stats()
    print(f"Training Steps: {training_stats['training_step']}")
    print(f"Algorithm: {training_stats['algorithm']}")
    print(f"Recent Updates: {len(training_stats['recent_updates'])}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SandGraph Social Network Demo")
    parser.add_argument("--steps", type=int, default=5, help="Number of steps to run")
    parser.add_argument("--model", type=str, default="mistralai/Mistral-7B-Instruct-v0.2", help="LLM model to use")
    
    args = parser.parse_args()
    
    print("ğŸ”¥ SandGraph Social Network Demo")
    print("=" * 60)
    print(f"Steps: {args.steps}")
    print(f"Model: {args.model}")
    
    # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    supported_models = [
        "Qwen/Qwen-7B-Chat",
        "Qwen/Qwen-1_8B-Chat", 
        "microsoft/Phi-2",
        "google/gemma-2b-it",
        "01-ai/Yi-6B-Chat",
        "mistralai/Mistral-7B-Instruct-v0.2",
        "THUDM/chatglm3-6b",
        "baichuan-inc/Baichuan2-7B-Chat"
    ]
    
    if args.model not in supported_models:
        print(f"\nâš ï¸  Warning: Model {args.model} not in supported list.")
        print(f"Supported models: {', '.join(supported_models)}")
        print("Continuing anyway...")
    
    try:
        results = run_rl_social_demo(args.steps, args.model)
        print("\nâœ… Demo completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 