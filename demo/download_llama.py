#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½Llama-2æ¨¡å‹åˆ°æœ¬åœ°çš„Pythonè„šæœ¬
"""

import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download, login
import argparse


def download_llama_model(model_name="meta-llama/Llama-2-7b-hf", 
                        local_dir="/cpfs04/shared/kilab/hf-hub/Llama-2-7b-hf",
                        token=None):
    """
    ä¸‹è½½Llama-2æ¨¡å‹åˆ°æœ¬åœ°
    
    Args:
        model_name: Hugging Faceæ¨¡å‹åç§°
        local_dir: æœ¬åœ°ä¿å­˜ç›®å½•
        token: Hugging Face tokenï¼ˆå¦‚æœéœ€è¦ï¼‰
    """
    
    print(f"ğŸš€ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ä¿å­˜åˆ°: {local_dir}")
    
    # åˆ›å»ºæœ¬åœ°ç›®å½•
    os.makedirs(local_dir, exist_ok=True)
    
    try:
        # å¦‚æœæä¾›äº†tokenï¼Œå…ˆç™»å½•
        if token:
            print("ğŸ” ä½¿ç”¨tokenç™»å½•Hugging Face...")
            login(token=token)
        
        # ä¸‹è½½æ¨¡å‹
        print("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        model_path = snapshot_download(
            repo_id=model_name,
            local_dir=local_dir,
            local_dir_use_symlinks=False,  # ä¸ä½¿ç”¨ç¬¦å·é“¾æ¥ï¼Œç›´æ¥å¤åˆ¶æ–‡ä»¶
            resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            max_workers=4  # å¹¶å‘ä¸‹è½½æ•°é‡
        )
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
        
        # æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶
        model_files = list(Path(model_path).glob("*"))
        print(f"ğŸ“Š ä¸‹è½½çš„æ–‡ä»¶æ•°é‡: {len(model_files)}")
        
        # æ˜¾ç¤ºä¸»è¦æ–‡ä»¶
        print("\nğŸ“‹ ä¸»è¦æ–‡ä»¶:")
        for file_path in model_files:
            if file_path.is_file():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"  - {file_path.name}: {size_mb:.1f} MB")
        
        return model_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ç¡®è®¤Hugging Faceè®¿é—®æƒé™")
        print("3. æ£€æŸ¥ç£ç›˜ç©ºé—´")
        print("4. å°è¯•ä½¿ç”¨tokenç™»å½•")
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸‹è½½Llama-2æ¨¡å‹åˆ°æœ¬åœ°")
    parser.add_argument("--model", default="meta-llama/Llama-2-7b-hf", 
                       help="æ¨¡å‹åç§° (é»˜è®¤: meta-llama/Llama-2-7b-hf)")
    parser.add_argument("--local-dir", default="/cpfs04/shared/kilab/hf-hub/Llama-2-7b-hf",
                       help="æœ¬åœ°ä¿å­˜ç›®å½•")
    parser.add_argument("--token", help="Hugging Face token")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¤– Llama-2æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    local_path = Path(args.local_dir)
    if local_path.exists():
        free_space = os.statvfs(local_path).f_frsize * os.statvfs(local_path).f_bavail
        free_space_gb = free_space / (1024**3)
        print(f"ğŸ’¾ å¯ç”¨ç£ç›˜ç©ºé—´: {free_space_gb:.1f} GB")
        
        if free_space_gb < 15:
            print("âš ï¸  è­¦å‘Š: ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³ï¼ŒLlama-2-7béœ€è¦çº¦13GBç©ºé—´")
    
    # ä¸‹è½½æ¨¡å‹
    model_path = download_llama_model(
        model_name=args.model,
        local_dir=args.local_dir,
        token=args.token
    )
    
    if model_path:
        print("\nğŸ‰ ä¸‹è½½æˆåŠŸ!")
        print(f"ğŸ“ åœ¨æ‚¨çš„ä»£ç ä¸­ä½¿ç”¨ä»¥ä¸‹è·¯å¾„:")
        print(f"   model_path = '{model_path}'")
        
        # åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
        example_code = f'''
# ä½¿ç”¨ç¤ºä¾‹
from vllm import LLM, SamplingParams

llm = LLM(
    model="{model_path}",
    enable_lora=True,
    max_lora_rank=64,
    max_loras=2,
    tensor_parallel_size=4
)

sampling_params = SamplingParams(
    temperature=0,
    max_tokens=256
)

outputs = llm.generate(["Hello, how are you?"], sampling_params)
print(outputs[0].outputs[0].text)
'''
        
        print("\nğŸ’» ä½¿ç”¨ç¤ºä¾‹:")
        print(example_code)
    else:
        print("\nâŒ ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
