#!/usr/bin/env python3
"""
ç®€åŒ–çš„åˆ†å¸ƒå¼æµ‹è¯•è„šæœ¬

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. çœŸå®æ¨¡å¼ï¼šè¿æ¥å®é™…çš„vLLMå®ä¾‹
2. æ¨¡æ‹Ÿæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿå“åº”ï¼ˆå½“vLLMå®ä¾‹ä¸å¯ç”¨æ—¶ï¼‰
"""

import asyncio
import time
import random
import logging
from typing import Dict, List
import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleDistributedTest:
    """ç®€åŒ–çš„åˆ†å¸ƒå¼æµ‹è¯•"""
    
    def __init__(self, mock_mode: bool = True):
        self.base_port = 8001
        self.num_gpus = 8
        self.mock_mode = mock_mode
        
        print(f"\nğŸš€ åˆå§‹åŒ–ç®€åŒ–åˆ†å¸ƒå¼æµ‹è¯•:")
        print(f"   - GPUå®ä¾‹: 8ä¸ª (ç«¯å£{self.base_port}-{self.base_port+7})")
        print(f"   - æ¨¡å¼: {'æ¨¡æ‹Ÿæ¨¡å¼' if mock_mode else 'çœŸå®æ¨¡å¼'}")
        
        # åˆå§‹åŒ–LoRAé…ç½®
        self.lora_configs = self._initialize_lora_configs()
        
        print("âœ… ç®€åŒ–åˆ†å¸ƒå¼æµ‹è¯•åˆå§‹åŒ–å®Œæˆ!")
    
    def _initialize_lora_configs(self) -> List[Dict]:
        """åˆå§‹åŒ–8ä¸ªLoRAé…ç½®"""
        print("\nğŸ”§ åˆå§‹åŒ–8ä¸ªLoRAé…ç½®:")
        
        lora_configs = []
        
        # LoRA 1-4: TRUMPç»„ (GPU 0-3, ç«¯å£8001-8004)
        for i in range(4):
            config = {
                'lora_id': i + 1,
                'gpu_id': i,
                'port': self.base_port + i,
                'url': f"http://localhost:{self.base_port + i}/v1",
                'group': "TRUMP",
                'total_reward': 0.0,
                'update_count': 0,
                'success_count': 0,
                'error_count': 0
            }
            lora_configs.append(config)
            print(f"   - LoRA {i+1}: TRUMPç»„, GPU{i}, ç«¯å£{self.base_port+i}")
        
        # LoRA 5-8: BIDENç»„ (GPU 4-7, ç«¯å£8005-8008)
        for i in range(4):
            config = {
                'lora_id': i + 5,
                'gpu_id': i + 4,
                'port': self.base_port + i + 4,
                'url': f"http://localhost:{self.base_port + i + 4}/v1",
                'group': "BIDEN",
                'total_reward': 0.0,
                'update_count': 0,
                'success_count': 0,
                'error_count': 0
            }
            lora_configs.append(config)
            print(f"   - LoRA {i+5}: BIDENç»„, GPU{i+4}, ç«¯å£{self.base_port+i+4}")
        
        print(f"âœ… åˆ›å»ºäº† {len(lora_configs)} ä¸ªLoRAé…ç½®")
        return lora_configs
    
    async def test_health_check(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        print("\nğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥...")
        
        for lora_config in self.lora_configs:
            port = lora_config['port']
            gpu_id = lora_config['gpu_id']
            
            if self.mock_mode:
                # æ¨¡æ‹Ÿæ¨¡å¼ï¼šéšæœºå¥åº·çŠ¶æ€
                is_healthy = random.choice([True, False])
                if is_healthy:
                    print(f"   âœ… GPU{gpu_id} (ç«¯å£{port}): æ¨¡æ‹Ÿå¥åº·")
                else:
                    print(f"   âš ï¸ GPU{gpu_id} (ç«¯å£{port}): æ¨¡æ‹Ÿä¸å¥åº·")
            else:
                # çœŸå®æ¨¡å¼ï¼šå®é™…æ£€æŸ¥
                try:
                    import aiohttp
                    
                    url = f"http://localhost:{port}/health"
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=5) as response:
                            if response.status == 200:
                                data = await response.json()
                                status = data.get("status", "unknown")
                                print(f"   âœ… GPU{gpu_id} (ç«¯å£{port}): {status}")
                            else:
                                print(f"   âŒ GPU{gpu_id} (ç«¯å£{port}): HTTP {response.status}")
                
                except Exception as e:
                    print(f"   âš ï¸ GPU{gpu_id} (ç«¯å£{port}): è¿æ¥å¤±è´¥ - {e}")
    
    async def test_single_request(self):
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        print("\nğŸ§ª æµ‹è¯•å•ä¸ªè¯·æ±‚...")
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªLoRAé…ç½®
        lora_config = random.choice(self.lora_configs)
        port = lora_config['port']
        gpu_id = lora_config['gpu_id']
        group = lora_config['group']
        
        print(f"   æµ‹è¯• GPU{gpu_id} (ç«¯å£{port}, {group}ç»„)...")
        
        if self.mock_mode:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿå“åº”
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            response = self._generate_mock_response(f"Hello from {group} group!", lora_config['lora_id'])
            print(f"   âœ… æ¨¡æ‹Ÿå“åº”: {response}")
            
            # æ›´æ–°å¥–åŠ±
            reward = self._calculate_reward(response, group)
            lora_config['total_reward'] += reward
            lora_config['update_count'] += 1
            lora_config['success_count'] += 1
            print(f"   ğŸ“ˆ å¥–åŠ±: {reward:.4f}")
            
        else:
            # çœŸå®æ¨¡å¼ï¼šå®é™…è¯·æ±‚
            try:
                import aiohttp
                
                url = f"http://localhost:{port}/v1/chat/completions"
                payload = {
                    "model": "qwen-2",
                    "messages": [{"role": "user", "content": f"Hello from {group} group!"}],
                    "max_tokens": 50,
                    "temperature": 0.7
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, json=payload, timeout=10) as response:
                        if response.status == 200:
                            data = await response.json()
                            result = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                            print(f"   âœ… å“åº”: {result[:100]}...")
                            
                            # æ›´æ–°å¥–åŠ±
                            reward = self._calculate_reward(result, group)
                            lora_config['total_reward'] += reward
                            lora_config['update_count'] += 1
                            lora_config['success_count'] += 1
                            print(f"   ğŸ“ˆ å¥–åŠ±: {reward:.4f}")
                            
                        else:
                            error_text = await response.text()
                            print(f"   âŒ è¯·æ±‚å¤±è´¥: HTTP {response.status}")
                            print(f"   ğŸ“„ é”™è¯¯ä¿¡æ¯: {error_text[:200]}...")
                            lora_config['error_count'] += 1
            
            except Exception as e:
                print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                lora_config['error_count'] += 1
    
    async def test_concurrent_requests(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print("\nğŸš€ æµ‹è¯•å¹¶å‘è¯·æ±‚...")
        
        tasks = []
        for lora_config in self.lora_configs:
            task = self._make_request(lora_config)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        # ç»Ÿè®¡ç»“æœ
        successful_requests = 0
        total_reward = 0.0
        
        for i, result in enumerate(results):
            lora_config = self.lora_configs[i]
            gpu_id = lora_config['gpu_id']
            port = lora_config['port']
            
            if isinstance(result, Exception):
                print(f"   âŒ GPU{gpu_id} (ç«¯å£{port}): å¤±è´¥ - {result}")
                lora_config['error_count'] += 1
            else:
                successful_requests += 1
                total_reward += result
                print(f"   âœ… GPU{gpu_id} (ç«¯å£{port}): æˆåŠŸ, å¥–åŠ± {result:.4f}")
                lora_config['success_count'] += 1
        
        print(f"\nğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"   - æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"   - æˆåŠŸè¯·æ±‚: {successful_requests}/{len(self.lora_configs)}")
        print(f"   - æ€»å¥–åŠ±: {total_reward:.4f}")
        print(f"   - å¹³å‡å“åº”æ—¶é—´: {(end_time - start_time) / len(self.lora_configs):.2f}ç§’")
    
    async def _make_request(self, lora_config: Dict) -> float:
        """å‘é€å•ä¸ªè¯·æ±‚"""
        port = lora_config['port']
        group = lora_config['group']
        
        if self.mock_mode:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿå“åº”
            await asyncio.sleep(random.uniform(0.05, 0.2))  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            response = self._generate_mock_response(f"Quick test from LoRA {lora_config['lora_id']}", lora_config['lora_id'])
            
            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(response, group)
            lora_config['total_reward'] += reward
            lora_config['update_count'] += 1
            
            return reward
            
        else:
            # çœŸå®æ¨¡å¼ï¼šå®é™…è¯·æ±‚
            try:
                import aiohttp
                
                url = f"http://localhost:{port}/v1/chat/completions"
                payload = {
                    "model": "qwen-2",
                    "messages": [{"role": "user", "content": f"Quick test from LoRA {lora_config['lora_id']}"}],
                    "max_tokens": 30,
                    "temperature": 0.7
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, json=payload, timeout=10) as response:
                        if response.status == 200:
                            data = await response.json()
                            result = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                            
                            # è®¡ç®—å¥–åŠ±
                            reward = self._calculate_reward(result, group)
                            lora_config['total_reward'] += reward
                            lora_config['update_count'] += 1
                            
                            return reward
                        else:
                            raise Exception(f"HTTP {response.status}")
            
            except Exception as e:
                raise e
    
    def _generate_mock_response(self, prompt: str, lora_id: int) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        if lora_id <= 4:  # TRUMPç»„
            responses = [
                f"[GPU{lora_id-1}] I support TRUMP and will post/forward TRUMP messages this round.",
                f"[GPU{lora_id-1}] Make America Great Again! TRUMP supporter here.",
                f"[GPU{lora_id-1}] Standing with TRUMP on this important issue.",
                f"[GPU{lora_id-1}] TRUMP's policies are the way forward for our country."
            ]
        else:  # BIDENç»„
            responses = [
                f"[GPU{lora_id-1}] I support BIDEN and will post/forward BIDEN messages this round.",
                f"[GPU{lora_id-1}] Build Back Better! BIDEN supporter here.",
                f"[GPU{lora_id-1}] Standing with BIDEN on this important issue.",
                f"[GPU{lora_id-1}] BIDEN's policies are the way forward for our country."
            ]
        
        return random.choice(responses)
    
    def _calculate_reward(self, response: str, group: str) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # åŸºç¡€å¥–åŠ±
        base_reward = random.uniform(0.1, 1.0)
        
        # å“åº”è´¨é‡å¥–åŠ±
        if len(response) > 20:
            quality_bonus = 0.2
        else:
            quality_bonus = 0.0
        
        # ä¿¡å¿µä¸€è‡´æ€§å¥–åŠ±
        if group in response.upper():
            belief_bonus = 0.3
        else:
            belief_bonus = 0.0
        
        return base_reward + quality_bonus + belief_bonus
    
    def print_final_statistics(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        print("\n" + "=" * 60)
        print("æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
        print("=" * 60)
        
        # LoRAæ€§èƒ½ç»Ÿè®¡
        print("LoRAæ€§èƒ½ç»Ÿè®¡:")
        for lora_config in self.lora_configs:
            print(f"  - LoRA {lora_config['lora_id']} (GPU{lora_config['gpu_id']}, {lora_config['group']}):")
            print(f"    æ€»å¥–åŠ±: {lora_config['total_reward']:.4f}")
            print(f"    æ›´æ–°æ¬¡æ•°: {lora_config['update_count']}")
            print(f"    æˆåŠŸæ¬¡æ•°: {lora_config['success_count']}")
            print(f"    é”™è¯¯æ¬¡æ•°: {lora_config['error_count']}")
        
        # ç»„åˆ«å¯¹æ¯”
        trump_loras = [config for config in self.lora_configs if config['group'] == "TRUMP"]
        biden_loras = [config for config in self.lora_configs if config['group'] == "BIDEN"]
        
        trump_total_reward = sum(lora['total_reward'] for lora in trump_loras)
        biden_total_reward = sum(lora['total_reward'] for lora in biden_loras)
        
        trump_success_rate = sum(lora['success_count'] for lora in trump_loras) / max(sum(lora['success_count'] + lora['error_count'] for lora in trump_loras), 1)
        biden_success_rate = sum(lora['success_count'] for lora in biden_loras) / max(sum(lora['success_count'] + lora['error_count'] for lora in biden_loras), 1)
        
        print(f"\nç»„åˆ«å¯¹æ¯”:")
        print(f"  - TRUMPç»„æ€»å¥–åŠ±: {trump_total_reward:.4f}, æˆåŠŸç‡: {trump_success_rate:.2%}")
        print(f"  - BIDENç»„æ€»å¥–åŠ±: {biden_total_reward:.4f}, æˆåŠŸç‡: {biden_success_rate:.2%}")
        print(f"  - èƒœå‡ºç»„: {'TRUMP' if trump_total_reward > biden_total_reward else 'BIDEN'}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_success = sum(config['success_count'] for config in self.lora_configs)
        total_errors = sum(config['error_count'] for config in self.lora_configs)
        total_requests = total_success + total_errors
        
        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print(f"  - æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"  - æˆåŠŸè¯·æ±‚: {total_success}")
        print(f"  - å¤±è´¥è¯·æ±‚: {total_errors}")
        print(f"  - æ€»ä½“æˆåŠŸç‡: {total_success / max(total_requests, 1):.2%}")
        
        print("=" * 60)
    
    def save_results(self, filename: str = "simple_distributed_test_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results = {
            'test_config': {
                'num_gpus': self.num_gpus,
                'base_port': self.base_port,
                'mock_mode': self.mock_mode
            },
            'lora_configs': self.lora_configs,
            'summary': {
                'total_requests': sum(config['success_count'] + config['error_count'] for config in self.lora_configs),
                'total_success': sum(config['success_count'] for config in self.lora_configs),
                'total_errors': sum(config['error_count'] for config in self.lora_configs),
                'total_reward': sum(config['total_reward'] for config in self.lora_configs)
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {filename}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ ç®€åŒ–åˆ†å¸ƒå¼æµ‹è¯•")
    print("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    mock_mode = True
    if len(sys.argv) > 1 and sys.argv[1] == "--real":
        mock_mode = False
        print("ä½¿ç”¨çœŸå®æ¨¡å¼ï¼ˆéœ€è¦vLLMå®ä¾‹è¿è¡Œï¼‰")
    else:
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€vLLMå®ä¾‹ï¼‰")
    
    try:
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        test = SimpleDistributedTest(mock_mode=mock_mode)
        
        # å¥åº·æ£€æŸ¥
        await test.test_health_check()
        
        # å•ä¸ªè¯·æ±‚æµ‹è¯•
        await test.test_single_request()
        
        # å¹¶å‘è¯·æ±‚æµ‹è¯•
        await test.test_concurrent_requests()
        
        # æ‰“å°ç»Ÿè®¡
        test.print_final_statistics()
        
        # ä¿å­˜ç»“æœ
        test.save_results()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ ç®€åŒ–åˆ†å¸ƒå¼æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
