#!/usr/bin/env python3
"""
ä½¿ç”¨Camelå’ŒOasis VLLMæ¥å£çš„ç¤ºä¾‹
"""

import asyncio
import os
import sys
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

async def test_camel_oasis_vllm():
    """æµ‹è¯•Camelå’ŒOasis VLLMæ¥å£"""
    print("ğŸ§ª æµ‹è¯•Camelå’ŒOasis VLLMæ¥å£...")
    print("=" * 50)
    
    try:
        # å¯¼å…¥Camelå’ŒOasis
        from camel.models import ModelFactory
        from camel.types import ModelPlatformType
        import oasis
        from oasis import ActionType, LLMAction, ManualAction, generate_reddit_agent_graph
        
        print("âœ… æˆåŠŸå¯¼å…¥Camelå’ŒOasis")
        
        # åˆ›å»ºVLLMæ¨¡å‹
        print("\nğŸ”§ åˆ›å»ºVLLMæ¨¡å‹...")
        vllm_model_1 = ModelFactory.create(
            model_platform=ModelPlatformType.VLLM,
            model_type="qwen-2",
            url="http://localhost:8001/v1",
        )
        vllm_model_2 = ModelFactory.create(
            model_platform=ModelPlatformType.VLLM,
            model_type="qwen-2",
            url="http://localhost:8001/v1",
        )
        models = [vllm_model_1, vllm_model_2]
        
        print(f"âœ… åˆ›å»ºäº† {len(models)} ä¸ªVLLMæ¨¡å‹")
        
        # å®šä¹‰å¯ç”¨åŠ¨ä½œ
        available_actions = [
            ActionType.CREATE_POST,
            ActionType.LIKE_POST,
            ActionType.REPOST,
            ActionType.FOLLOW,
            ActionType.DO_NOTHING,
            ActionType.QUOTE_POST,
        ]
        
        print(f"âœ… å®šä¹‰äº† {len(available_actions)} ä¸ªå¯ç”¨åŠ¨ä½œ")
        
        # ç”Ÿæˆagentå›¾
        print("\nğŸ”§ ç”Ÿæˆagentå›¾...")
        agent_graph = await generate_reddit_agent_graph(
            profile_path="user_data_36.json",
            model=models,
            available_actions=available_actions,
        )
        
        print(f"âœ… ç”Ÿæˆäº†agentå›¾ï¼ŒåŒ…å« {len(list(agent_graph.get_agents()))} ä¸ªagents")
        
        # ç»™æ¯ä¸ªagentåˆ†é…ç»„
        import random
        trump_ratio = 0.5  # 50% Trump, 50% Biden
        agent_ids = [id for id, _ in agent_graph.get_agents()]
        trump_agents = set(random.sample(agent_ids, int(len(agent_ids) * trump_ratio)))
        
        for id, agent in agent_graph.get_agents():
            agent.group = "TRUMP" if id in trump_agents else "BIDEN"
        
        print(f"âœ… åˆ†é…äº†agentç»„: TRUMP={len(trump_agents)}, BIDEN={len(agent_ids)-len(trump_agents)}")
        
        # åˆ›å»ºç¯å¢ƒ
        print("\nğŸ”§ åˆ›å»ºOasisç¯å¢ƒ...")
        db_path = "camel_oasis_test.db"
        
        # åˆ é™¤æ—§æ•°æ®åº“
        if os.path.exists(db_path):
            os.remove(db_path)
        
        env = oasis.make(
            agent_graph=agent_graph,
            platform=oasis.DefaultPlatformType.TWITTER,
            database_path=db_path,
        )
        
        print("âœ… åˆ›å»ºäº†Oasisç¯å¢ƒ")
        
        # è¿è¡Œç¯å¢ƒ
        print("\nğŸ”„ è¿è¡Œç¯å¢ƒ...")
        await env.reset()
        
        # æ‰§è¡Œä¸€äº›åŠ¨ä½œ
        print("ğŸ“ æ‰§è¡Œæ‰‹åŠ¨åŠ¨ä½œ...")
        actions_1 = {}
        actions_1[env.agent_graph.get_agent(0)] = ManualAction(
            action_type=ActionType.CREATE_POST,
            action_args={"content": "Multi-model training is amazing!"}
        )
        await env.step(actions_1)
        
        print("ğŸ¤– æ‰§è¡ŒLLMåŠ¨ä½œ...")
        actions_2 = {
            agent: LLMAction()
            for _, agent in env.agent_graph.get_agents([1, 3, 5, 7, 9])
        }
        await env.step(actions_2)
        
        # æ¨¡æ‹Ÿå¤šæ¨¡å‹è®­ç»ƒåœºæ™¯
        print("\nğŸ¯ æ¨¡æ‹Ÿå¤šæ¨¡å‹è®­ç»ƒåœºæ™¯...")
        for step in range(5):
            print(f"   Step {step + 1}:")
            
            # ä½¿ç”¨VLLMæ¨¡å‹ç”Ÿæˆå“åº”
            for id, agent in agent_graph.get_agents():
                neighbors = agent.get_neighbors()
                neighbor_groups = [n.group for n in neighbors]
                
                prompt = (
                    f"You are a {agent.group} supporter. "
                    f"Your neighbors' groups: {neighbor_groups}. "
                    "Will you post/forward TRUMP or BIDEN message this round?"
                )
                
                # ä½¿ç”¨VLLMæ¨¡å‹ç”Ÿæˆå“åº”
                try:
                    # ä½¿ç”¨æ­£ç¡®çš„Camel VLLM APIæ ¼å¼
                    messages = [{"role": "user", "content": prompt}]
                    resp = await vllm_model_1.arun(messages)
                    print(f"     Agent {id} ({agent.group}): {resp[:50]}...")
                except Exception as e:
                    print(f"     Agent {id} ({agent.group}): VLLMè°ƒç”¨å¤±è´¥ - {e}")
            
            # ç»Ÿè®¡ç»„åˆ†å¸ƒ
            trump_count = sum(1 for _, agent in agent_graph.get_agents() if agent.group == "TRUMP")
            biden_count = sum(1 for _, agent in agent_graph.get_agents() if agent.group == "BIDEN")
            print(f"     TRUMP={trump_count}, BIDEN={biden_count}")
        
        # å…³é—­ç¯å¢ƒ
        await env.close()
        
        print("\nâœ… Camelå’ŒOasis VLLMæ¥å£æµ‹è¯•å®Œæˆ")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…Camelå’ŒOasis: pip install camel-ai oasis")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_multi_model_integration():
    """æµ‹è¯•å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿé›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿé›†æˆ...")
    print("=" * 50)
    
    try:
        # å¯¼å…¥å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿ
        from demo.multi_model_single_env_simple import (
            MultiModelEnvironment, ModelConfig, ModelRole, TrainingMode, VLLMClient
        )
        
        print("âœ… æˆåŠŸå¯¼å…¥å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
        
        # æµ‹è¯•VLLMå®¢æˆ·ç«¯
        print("\nğŸ”§ æµ‹è¯•VLLMå®¢æˆ·ç«¯...")
        vllm_client = VLLMClient("http://localhost:8001/v1", "qwen-2")
        
        test_prompt = "è¯·ç®€è¦è¯´æ˜å¤šæ¨¡å‹è®­ç»ƒçš„ä¼˜åŠ¿ã€‚"
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {test_prompt}")
        
        response = await vllm_client.generate(test_prompt, max_tokens=50)
        print(f"ğŸ¤– VLLMå“åº”: {response}")
        
        # æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–
        print("\nğŸ”§ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–...")
        env = MultiModelEnvironment(
            vllm_url="http://localhost:8001/v1",
            training_mode=TrainingMode.COOPERATIVE,
            max_models=3
        )
        
        print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸï¼ŒVLLMå¯ç”¨æ€§: {env.vllm_available}")
        
        # æµ‹è¯•æ¨¡å‹æ·»åŠ 
        print("\nğŸ”§ æµ‹è¯•æ¨¡å‹æ·»åŠ ...")
        config = ModelConfig(
            model_id="camel_test_model_001",
            model_name="qwen-2",
            role=ModelRole.LEADER,
            lora_rank=8,
            team_id="camel_test_team"
        )
        
        success = env.add_model(config)
        print(f"âœ… æ¨¡å‹æ·»åŠ : {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        print("\nâœ… å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿé›†æˆæµ‹è¯•å®Œæˆ")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Camelå’ŒOasis VLLMæ¥å£æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•Camelå’ŒOasis VLLMæ¥å£
    await test_camel_oasis_vllm()
    
    # æµ‹è¯•å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿé›†æˆ
    await test_multi_model_integration()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
