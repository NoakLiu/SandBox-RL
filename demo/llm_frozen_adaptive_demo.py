#!/usr/bin/env python3
"""
LLMs Frozen & Adaptive Update Demo
==================================

æ¼”ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„å†»ç»“å’Œè‡ªé€‚åº”æ›´æ–°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- ä¸åŒæ›´æ–°ç­–ç•¥çš„å¯¹æ¯”
- å‚æ•°å†»ç»“/è§£å†»æ“ä½œ
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
- æ€§èƒ½ç›‘æ§å’Œå›æ»š
- å‚æ•°é‡è¦æ€§åˆ†æ
"""

import sys
import os
import time
import json
import argparse
import logging
from typing import Dict, List, Any, Optional
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sandgraph.core.llm_interface import (
    create_llm_config, create_llm, MockLLM, LLMConfig, LLMBackend
)
from sandgraph.core.llm_frozen_adaptive import (
    FrozenAdaptiveLLM, FrozenAdaptiveManager, FrozenConfig,
    UpdateStrategy, ParameterImportance, create_frozen_config,
    create_frozen_adaptive_llm, create_frozen_adaptive_manager,
    get_preset_configs
)


def setup_logging(level: str = "INFO") -> None:
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('llm_frozen_adaptive_demo.log', encoding='utf-8')
        ]
    )


def generate_mock_gradients(parameters: Dict[str, Any], 
                           importance_weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ‹Ÿæ¢¯åº¦"""
    gradients = {}
    
    for name, param in parameters.items():
        if importance_weights and name in importance_weights:
            weight = importance_weights[name]
        else:
            weight = 1.0
        
        if isinstance(param, list):
            gradients[name] = [np.random.normal(0, 0.1 * weight) for _ in param]
        elif isinstance(param, np.ndarray):
            gradients[name] = np.random.normal(0, 0.1 * weight, param.shape)
        else:
            gradients[name] = np.random.normal(0, 0.1 * weight)
    
    return gradients


def evaluate_performance(model: FrozenAdaptiveLLM, 
                        test_prompts: List[str]) -> float:
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    total_confidence = 0.0
    count = 0
    
    for prompt in test_prompts:
        try:
            response = model.generate(prompt)
            total_confidence += response.confidence
            count += 1
        except Exception as e:
            logging.warning(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    return total_confidence / count if count > 0 else 0.0


def demo_basic_functionality():
    """åŸºç¡€åŠŸèƒ½æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ”§ åŸºç¡€åŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºåŸºç¡€LLM
    config = create_llm_config(backend="mock", model_name="demo_model")
    base_llm = create_llm(config)
    
    # åˆ›å»ºå†»ç»“è‡ªé€‚åº”LLM
    frozen_config = create_frozen_config(
        strategy="adaptive",
        frozen_layers=["embedding"],
        adaptive_learning_rate=True
    )
    
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    print(f"âœ… åˆ›å»ºå†»ç»“è‡ªé€‚åº”LLMæˆåŠŸ")
    print(f"   ç­–ç•¥: {frozen_config.strategy.value}")
    print(f"   å†»ç»“å±‚: {frozen_config.frozen_layers}")
    print(f"   è‡ªé€‚åº”å­¦ä¹ ç‡: {frozen_config.adaptive_learning_rate}")
    
    # æµ‹è¯•ç”Ÿæˆ
    test_prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    response = frozen_llm.generate(test_prompt)
    print(f"\nğŸ“ æµ‹è¯•ç”Ÿæˆ:")
    print(f"   æç¤º: {test_prompt}")
    print(f"   å“åº”: {response.text}")
    print(f"   ç½®ä¿¡åº¦: {response.confidence:.3f}")
    
    # è·å–å‚æ•°ä¿¡æ¯
    param_info = frozen_llm.get_parameter_info()
    print(f"\nğŸ“Š å‚æ•°ä¿¡æ¯:")
    for name, info in param_info.items():
        print(f"   {name}: é‡è¦æ€§={info.importance.value}, å†»ç»“={info.frozen}")


def demo_update_strategies():
    """æ›´æ–°ç­–ç•¥æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ”„ æ›´æ–°ç­–ç•¥æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = create_frozen_adaptive_manager()
    
    # è·å–é¢„è®¾é…ç½®
    preset_configs = get_preset_configs()
    
    # ä¸ºæ¯ä¸ªç­–ç•¥åˆ›å»ºæ¨¡å‹
    for strategy_name, config in preset_configs.items():
        print(f"\nğŸ“‹ ç­–ç•¥: {strategy_name}")
        print(f"   æ›´æ–°ç­–ç•¥: {config.strategy.value}")
        print(f"   å†»ç»“å±‚: {config.frozen_layers}")
        print(f"   å­¦ä¹ ç‡èŒƒå›´: {config.min_learning_rate:.2e} - {config.max_learning_rate:.2e}")
        
        # åˆ›å»ºåŸºç¡€LLM
        base_config = create_llm_config(backend="mock", model_name=f"model_{strategy_name}")
        base_llm = create_llm(base_config)
        
        # æ³¨å†Œæ¨¡å‹
        frozen_llm = manager.register_model(f"model_{strategy_name}", base_llm, config)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ¢¯åº¦
        parameters = base_llm.get_parameters()
        gradients = generate_mock_gradients(parameters)
        
        # æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡
        performance = 0.7 + np.random.normal(0, 0.1)
        
        # æ›´æ–°å‚æ•°
        updated_params = frozen_llm.update_parameters(gradients, performance)
        
        print(f"   æ›´æ–°å‚æ•°æ•°é‡: {len(updated_params)}")
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = frozen_llm.get_performance_stats()
        if stats:
            print(f"   å½“å‰æ€§èƒ½: {stats.get('current_performance', 0):.3f}")
            print(f"   å­¦ä¹ ç‡: {stats.get('current_learning_rate', 0):.2e}")


def demo_parameter_management():
    """å‚æ•°ç®¡ç†æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ”’ å‚æ•°ç®¡ç†æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    base_config = create_llm_config(backend="mock", model_name="param_demo")
    base_llm = create_llm(base_config)
    
    frozen_config = create_frozen_config(
        strategy="selective",
        frozen_layers=[],
        adaptive_learning_rate=True
    )
    
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    # è·å–åˆå§‹å‚æ•°ä¿¡æ¯
    initial_params = frozen_llm.get_parameter_info()
    print(f"ğŸ“Š åˆå§‹å‚æ•°çŠ¶æ€:")
    for name, info in initial_params.items():
        print(f"   {name}: å†»ç»“={info.frozen}, é‡è¦æ€§={info.importance.value}")
    
    # å†»ç»“ç‰¹å®šå‚æ•°
    param_names = list(initial_params.keys())
    if len(param_names) >= 2:
        freeze_params = param_names[:2]
        frozen_llm.freeze_parameters(freeze_params)
        print(f"\nğŸ”’ å†»ç»“å‚æ•°: {freeze_params}")
    
    # å†»ç»“ç‰¹å®šå±‚
    frozen_llm.freeze_layers(["embedding"])
    print(f"ğŸ”’ å†»ç»“å±‚: embedding")
    
    # ç”Ÿæˆæ¢¯åº¦å¹¶æ›´æ–°
    parameters = base_llm.get_parameters()
    gradients = generate_mock_gradients(parameters)
    performance = 0.75
    
    updated_params = frozen_llm.update_parameters(gradients, performance)
    
    # æ£€æŸ¥æ›´æ–°ç»“æœ
    updated_info = frozen_llm.get_parameter_info()
    print(f"\nğŸ“Š æ›´æ–°åå‚æ•°çŠ¶æ€:")
    for name, info in updated_info.items():
        update_count = info.update_count
        frozen_status = "ğŸ”’" if info.frozen else "ğŸ”“"
        print(f"   {frozen_status} {name}: æ›´æ–°æ¬¡æ•°={update_count}, é‡è¦æ€§={info.importance.value}")
    
    # è§£å†»å‚æ•°
    if len(param_names) >= 2:
        unfreeze_params = param_names[:1]
        frozen_llm.unfreeze_parameters(unfreeze_params)
        print(f"\nğŸ”“ è§£å†»å‚æ•°: {unfreeze_params}")


def demo_adaptive_learning_rate():
    """è‡ªé€‚åº”å­¦ä¹ ç‡æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“ˆ è‡ªé€‚åº”å­¦ä¹ ç‡æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    base_config = create_llm_config(backend="mock", model_name="adaptive_demo")
    base_llm = create_llm(base_config)
    
    frozen_config = create_frozen_config(
        strategy="adaptive",
        adaptive_learning_rate=True,
        min_learning_rate=1e-6,
        max_learning_rate=1e-3
    )
    
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    parameters = base_llm.get_parameters()
    learning_rates = []
    performances = []
    
    print("ğŸ”„ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹:")
    for epoch in range(10):
        # ç”Ÿæˆæ¢¯åº¦
        gradients = generate_mock_gradients(parameters)
        
        # æ¨¡æ‹Ÿæ€§èƒ½å˜åŒ–
        if epoch < 5:
            performance = 0.6 + epoch * 0.05 + np.random.normal(0, 0.02)  # æ€§èƒ½æå‡
        else:
            performance = 0.8 - (epoch - 5) * 0.03 + np.random.normal(0, 0.02)  # æ€§èƒ½ä¸‹é™
        
        # æ›´æ–°å‚æ•°
        updated_params = frozen_llm.update_parameters(gradients, performance)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = frozen_llm.get_performance_stats()
        current_lr = stats.get('current_learning_rate', 0)
        
        learning_rates.append(current_lr)
        performances.append(performance)
        
        print(f"   Epoch {epoch+1:2d}: æ€§èƒ½={performance:.3f}, å­¦ä¹ ç‡={current_lr:.2e}")
    
    # åˆ†æå­¦ä¹ ç‡å˜åŒ–
    print(f"\nğŸ“Š å­¦ä¹ ç‡å˜åŒ–åˆ†æ:")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {learning_rates[0]:.2e}")
    print(f"   æœ€ç»ˆå­¦ä¹ ç‡: {learning_rates[-1]:.2e}")
    print(f"   å­¦ä¹ ç‡å˜åŒ–: {((learning_rates[-1] - learning_rates[0]) / learning_rates[0] * 100):+.1f}%")


def demo_performance_monitoring():
    """æ€§èƒ½ç›‘æ§æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    base_config = create_llm_config(backend="mock", model_name="monitor_demo")
    base_llm = create_llm(base_config)
    
    frozen_config = create_frozen_config(
        strategy="gradual",
        performance_window=20,
        rollback_threshold=0.05
    )
    
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«",
        "ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†",
        "å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨åœºæ™¯"
    ]
    
    print("ğŸ”„ æ€§èƒ½ç›‘æ§è®­ç»ƒ:")
    for step in range(15):
        # ç”Ÿæˆæ¢¯åº¦
        parameters = base_llm.get_parameters()
        gradients = generate_mock_gradients(parameters)
        
        # è¯„ä¼°æ€§èƒ½
        performance = evaluate_performance(frozen_llm, test_prompts)
        
        # æ›´æ–°å‚æ•°
        updated_params = frozen_llm.update_parameters(gradients, performance)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = frozen_llm.get_performance_stats()
        
        print(f"   Step {step+1:2d}: æ€§èƒ½={performance:.3f}, "
              f"å¹³å‡æ€§èƒ½={stats.get('average_performance', 0):.3f}, "
              f"è¶‹åŠ¿={stats.get('performance_trend', 0):+.3f}")
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = "demo_checkpoint.pkl"
    if frozen_llm.save_checkpoint(checkpoint_path):
        print(f"\nğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    # å¯¼å‡ºé…ç½®
    config_path = "demo_config.json"
    if frozen_llm.export_config(config_path):
        print(f"ğŸ“„ é…ç½®å·²å¯¼å‡º: {config_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    final_stats = frozen_llm.get_performance_stats()
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    for key, value in final_stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")


def demo_checkpoint_and_rollback():
    """æ£€æŸ¥ç‚¹å’Œå›æ»šæ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ’¾ æ£€æŸ¥ç‚¹å’Œå›æ»šæ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    base_config = create_llm_config(backend="mock", model_name="checkpoint_demo")
    base_llm = create_llm(base_config)
    
    frozen_config = create_frozen_config(strategy="adaptive")
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    # åˆå§‹çŠ¶æ€
    initial_params = frozen_llm.get_parameters()
    print(f"ğŸ“Š åˆå§‹å‚æ•°æ•°é‡: {len(initial_params)}")
    
    # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
    parameters = base_llm.get_parameters()
    for step in range(5):
        gradients = generate_mock_gradients(parameters)
        performance = 0.7 + step * 0.02
        frozen_llm.update_parameters(gradients, performance)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = "rollback_checkpoint.pkl"
    frozen_llm.save_checkpoint(checkpoint_path)
    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ç»§ç»­è®­ç»ƒï¼ˆæ¨¡æ‹Ÿæ€§èƒ½ä¸‹é™ï¼‰
    for step in range(3):
        gradients = generate_mock_gradients(parameters)
        performance = 0.6 - step * 0.05  # æ€§èƒ½ä¸‹é™
        frozen_llm.update_parameters(gradients, performance)
    
    stats_before_rollback = frozen_llm.get_performance_stats()
    print(f"ğŸ“Š å›æ»šå‰æ€§èƒ½: {stats_before_rollback.get('current_performance', 0):.3f}")
    
    # å›æ»šåˆ°æ£€æŸ¥ç‚¹
    success = frozen_llm.rollback_to_checkpoint(checkpoint_path)
    if success:
        print(f"ğŸ”„ æˆåŠŸå›æ»šåˆ°æ£€æŸ¥ç‚¹")
        
        stats_after_rollback = frozen_llm.get_performance_stats()
        print(f"ğŸ“Š å›æ»šåæ€§èƒ½: {stats_after_rollback.get('current_performance', 0):.3f}")
    
    # æ¸…ç†æ–‡ä»¶
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)


def demo_importance_analysis():
    """é‡è¦æ€§åˆ†ææ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ¯ å‚æ•°é‡è¦æ€§åˆ†ææ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    base_config = create_llm_config(backend="mock", model_name="importance_demo")
    base_llm = create_llm(base_config)
    
    frozen_config = create_frozen_config(strategy="selective")
    frozen_llm = FrozenAdaptiveLLM(base_llm, frozen_config)
    
    # ç”Ÿæˆä¸åŒé‡è¦æ€§çš„æ¢¯åº¦
    parameters = base_llm.get_parameters()
    param_names = list(parameters.keys())
    
    # è®¾ç½®é‡è¦æ€§æƒé‡
    importance_weights = {}
    for i, name in enumerate(param_names):
        if i < len(param_names) // 4:
            importance_weights[name] = 2.0  # é«˜é‡è¦æ€§
        elif i < len(param_names) // 2:
            importance_weights[name] = 1.0  # ä¸­ç­‰é‡è¦æ€§
        else:
            importance_weights[name] = 0.5  # ä½é‡è¦æ€§
    
    # ç”Ÿæˆæ¢¯åº¦
    gradients = generate_mock_gradients(parameters, importance_weights)
    
    # æ›´æ–°å‚æ•°
    performance = 0.75
    updated_params = frozen_llm.update_parameters(gradients, performance)
    
    # åˆ†æé‡è¦æ€§
    param_info = frozen_llm.get_parameter_info()
    
    print("ğŸ“Š å‚æ•°é‡è¦æ€§åˆ†æ:")
    importance_counts = {}
    for name, info in param_info.items():
        importance = info.importance.value
        if importance not in importance_counts:
            importance_counts[importance] = 0
        importance_counts[importance] += 1
        
        print(f"   {name}: é‡è¦æ€§={importance}, æ•æ„Ÿæ€§={info.sensitivity:.3f}, "
              f"æ¢¯åº¦èŒƒæ•°={info.gradient_norm:.3f}")
    
    print(f"\nğŸ“ˆ é‡è¦æ€§åˆ†å¸ƒ:")
    for importance, count in importance_counts.items():
        print(f"   {importance}: {count} ä¸ªå‚æ•°")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LLMs Frozen & Adaptive Update Demo")
    parser.add_argument("--demo", choices=[
        "basic", "strategies", "parameters", "adaptive", 
        "monitoring", "checkpoint", "importance", "all"
    ], default="all", help="é€‰æ‹©æ¼”ç¤ºç±»å‹")
    parser.add_argument("--log-level", default="INFO", 
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    print("ğŸš€ LLMs Frozen & Adaptive Update Demo")
    print("=" * 60)
    
    try:
        if args.demo == "all" or args.demo == "basic":
            demo_basic_functionality()
        
        if args.demo == "all" or args.demo == "strategies":
            demo_update_strategies()
        
        if args.demo == "all" or args.demo == "parameters":
            demo_parameter_management()
        
        if args.demo == "all" or args.demo == "adaptive":
            demo_adaptive_learning_rate()
        
        if args.demo == "all" or args.demo == "monitoring":
            demo_performance_monitoring()
        
        if args.demo == "all" or args.demo == "checkpoint":
            demo_checkpoint_and_rollback()
        
        if args.demo == "all" or args.demo == "importance":
            demo_importance_analysis()
        
        print("\n" + "="*60)
        print("âœ… æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = ["demo_checkpoint.pkl", "demo_config.json", "rollback_checkpoint.pkl"]
        for file in temp_files:
            if os.path.exists(file):
                os.remove(file)
                print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
        logging.exception("æ¼”ç¤ºå¼‚å¸¸")


if __name__ == "__main__":
    main() 