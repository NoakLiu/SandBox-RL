#!/usr/bin/env python3
"""
8GPUåˆ†å¸ƒå¼è°ƒåº¦å™¨æ¼”ç¤º

åŸºäº8ä¸ªvLLMå®ä¾‹çš„åˆ†å¸ƒå¼éƒ¨ç½²æ–¹æ¡ˆæ¼”ç¤ºï¼š
- 8ä¸ªGPUå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹å ç”¨1å¼ GPU
- ç«¯å£æ˜ å°„ï¼š8001-8008
- æ”¯æŒLoRAè·¯ç”±åˆ°ä¸åŒGPUå®ä¾‹
- å¹¶å‘è¯·æ±‚ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨8å¡èµ„æº
"""

import asyncio
import time
import random
import logging
from typing import Dict, List
import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥SandGraphæ ¸å¿ƒç»„ä»¶
from sandgraph.core import (
    TaskDefinition, ModelRole, InteractionType
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Distributed8GPUSimulation:
    """8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿå™¨"""
    
    def __init__(self, num_agents: int = 50, num_steps: int = 30):
        self.num_agents = num_agents
        self.num_steps = num_steps
        self.current_step = 0
        
        print(f"\nğŸš€ åˆå§‹åŒ–8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿå™¨:")
        print(f"   - Agentæ•°é‡: {num_agents}")
        print(f"   - æ—¶é—´æ­¥æ•°: {num_steps}")
        print(f"   - GPUå®ä¾‹: 8ä¸ª (ç«¯å£8001-8008)")
        
        # åˆå§‹åŒ–8ä¸ªLoRAé…ç½®
        self.lora_configs = self._initialize_lora_configs()
        
        # åˆå§‹åŒ–agents
        self.agents = self._initialize_agents()
        
        # ç»Ÿè®¡æ•°æ®
        self.statistics = {
            'step_rewards': [],
            'lora_rewards': [],
            'vllm_calls': [],
            'gpu_utilization': []
        }
        
        print("âœ… 8GPUåˆ†å¸ƒå¼ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
    
    def _initialize_lora_configs(self) -> List[Dict]:
        """åˆå§‹åŒ–8ä¸ªLoRAé…ç½®"""
        print("\nğŸ”§ åˆå§‹åŒ–8ä¸ªLoRAé…ç½®:")
        
        lora_configs = []
        
        # LoRA 1-4: TRUMPç»„ (GPU 0-3, ç«¯å£8001-8004)
        for i in range(4):
            config = {
                'lora_id': i + 1,
                'gpu_id': i,
                'port': 8001 + i,
                'url': f"http://localhost:{8001 + i}/v1",
                'group': "TRUMP",
                'total_reward': 0.0,
                'update_count': 0
            }
            lora_configs.append(config)
            print(f"   - LoRA {i+1}: TRUMPç»„, GPU{i}, ç«¯å£{8001+i}")
        
        # LoRA 5-8: BIDENç»„ (GPU 4-7, ç«¯å£8005-8008)
        for i in range(4):
            config = {
                'lora_id': i + 5,
                'gpu_id': i + 4,
                'port': 8005 + i,
                'url': f"http://localhost:{8005 + i}/v1",
                'group': "BIDEN",
                'total_reward': 0.0,
                'update_count': 0
            }
            lora_configs.append(config)
            print(f"   - LoRA {i+5}: BIDENç»„, GPU{i+4}, ç«¯å£{8005+i}")
        
        print(f"âœ… åˆ›å»ºäº† {len(lora_configs)} ä¸ªLoRAé…ç½®")
        return lora_configs
    
    def _initialize_agents(self) -> List[Dict]:
        """åˆå§‹åŒ–agents"""
        print(f"\nğŸ”§ åˆå§‹åŒ– {self.num_agents} ä¸ªagents:")
        
        agents = []
        
        # åˆ†é…agentsåˆ°LoRAæ¨¡å‹
        agents_per_lora = self.num_agents // 8
        remaining_agents = self.num_agents % 8
        
        agent_id = 0
        for lora_config in self.lora_configs:
            current_agents = agents_per_lora
            if remaining_agents > 0:
                current_agents += 1
                remaining_agents -= 1
            
            for _ in range(current_agents):
                agent = {
                    'agent_id': agent_id,
                    'group': lora_config['group'],
                    'lora_id': lora_config['lora_id'],
                    'gpu_id': lora_config['gpu_id'],
                    'belief_strength': random.uniform(0.6, 1.0),
                    'total_reward': 0.0
                }
                agents.append(agent)
                agent_id += 1
        
        # ç»Ÿè®¡
        trump_agents = [a for a in agents if a['group'] == "TRUMP"]
        biden_agents = [a for a in agents if a['group'] == "BIDEN"]
        
        print(f"   - TRUMPç»„: {len(trump_agents)} agents")
        print(f"   - BIDENç»„: {len(biden_agents)} agents")
        
        for lora_config in self.lora_configs:
            lora_agents = [a for a in agents if a['lora_id'] == lora_config['lora_id']]
            print(f"   - LoRA {lora_config['lora_id']} (GPU{lora_config['gpu_id']}, {lora_config['group']}): {len(lora_agents)} agents")
        
        print(f"âœ… åˆ›å»ºäº† {len(agents)} ä¸ªagents")
        return agents
    
    async def _call_vllm_api(self, prompt: str, lora_id: int) -> str:
        """è°ƒç”¨VLLM API"""
        lora_config = next((config for config in self.lora_configs if config['lora_id'] == lora_id), None)
        if not lora_config:
            return f"Error: Invalid LoRA ID {lora_id}"
        
        try:
            import aiohttp
            
            url = f"{lora_config['url']}/chat/completions"
            payload = {
                "model": "qwen-2",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 100,
                "temperature": 0.7
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        result = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                        return result
                    else:
                        error_text = await response.text()
                        logger.warning(f"GPU{lora_config['gpu_id']} (LoRA{lora_id}) è¯·æ±‚å¤±è´¥: {response.status}")
                        return self._generate_mock_response(prompt, lora_id)
        
        except Exception as e:
            logger.warning(f"GPU{lora_config['gpu_id']} (LoRA{lora_id}) è¯·æ±‚å¼‚å¸¸: {e}")
            return self._generate_mock_response(prompt, lora_id)
    
    def _generate_mock_response(self, prompt: str, lora_id: int) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        lora_config = next((config for config in self.lora_configs if config['lora_id'] == lora_id), None)
        if lora_config:
            if lora_config['group'] == "TRUMP":
                return f"[GPU{lora_config['gpu_id']}] I support TRUMP and will post/forward TRUMP messages this round."
            else:
                return f"[GPU{lora_config['gpu_id']}] I support BIDEN and will post/forward BIDEN messages this round."
        return f"[Mock] LoRA {lora_id} response"
    
    def _update_lora_weights(self, lora_id: int, reward: float):
        """æ›´æ–°LoRAæƒé‡"""
        lora_config = next((config for config in self.lora_configs if config['lora_id'] == lora_id), None)
        if lora_config:
            lora_config['total_reward'] += reward
            lora_config['update_count'] += 1
            
            logger.info(f"LoRA {lora_id} (GPU{lora_config['gpu_id']}, {lora_config['group']}) æ›´æ–°: reward={reward:.4f}, æ€»reward={lora_config['total_reward']:.4f}")
    
    async def simulate_step(self):
        """å¹¶å‘ç‰ˆï¼šåŒä¸€æ—¶é—´æ­¥å†…æ”¶é›†ä»»åŠ¡åä¸€èµ·å‘ï¼Œå……åˆ†åˆ©ç”¨8å¡"""
        self.current_step += 1
        logger.info(f"=== æ—¶é—´æ­¥ {self.current_step} ===")
        
        vllm_calls = 0
        lora_rewards = [0.0] * 8
        
        tasks = []
        owners = []   # è®°å½• (lora_id, agent) ä¾›å›å¡«å¥–åŠ±
        
        # æ”¶é›†è¦å‘çš„è¯·æ±‚
        for agent in self.agents:
            if random.random() < 0.2:  # 20%æ¦‚ç‡å‘å¸–
                prompt = f"You are a {agent['group']} supporter using LoRA {agent['lora_id']} on GPU {agent['gpu_id']}."
                tasks.append(self._call_vllm_api(prompt, agent['lora_id']))
                owners.append((agent['lora_id'], agent))
                vllm_calls += 1
        
        # å¹¶å‘æ‰§è¡Œ
        results = []
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ ¹æ®ç»“æœå›å¡«å¥–åŠ±
        for (lora_id, agent), resp in zip(owners, results):
            if isinstance(resp, Exception):
                logger.warning(f"Agent {agent['agent_id']} è¯·æ±‚å¤±è´¥: {resp}")
                reward = 0.1  # å¤±è´¥æ—¶ç»™å°‘é‡å¥–åŠ±
            else:
                # åŸºäºå“åº”è´¨é‡è®¡ç®—å¥–åŠ±
                reward = self._calculate_reward(resp, agent)
            
            lora_rewards[lora_id - 1] += reward
            agent['total_reward'] += reward
        
        # æ›´æ–°LoRAæƒé‡
        for i, reward in enumerate(lora_rewards):
            if reward > 0:
                self._update_lora_weights(i + 1, reward)
        
        # ç»Ÿè®¡
        step_total_reward = sum(lora_rewards)
        self.statistics['step_rewards'].append(step_total_reward)
        self.statistics['lora_rewards'].append(lora_rewards)
        self.statistics['vllm_calls'].append(vllm_calls)
        
        # è®¡ç®—GPUåˆ©ç”¨ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        gpu_utilization = [0.0] * 8
        for agent in self.agents:
            if random.random() < 0.2:  # æ´»è·ƒçš„agent
                gpu_id = agent['gpu_id']
                gpu_utilization[gpu_id] += 0.1
        self.statistics['gpu_utilization'].append(gpu_utilization)
        
        # è¾“å‡ºç»Ÿè®¡
        logger.info(f"ğŸ“Š æ­¥éª¤ {self.current_step} ç»Ÿè®¡:")
        logger.info(f"   - æ€»å¥–åŠ±: {step_total_reward:.4f}")
        logger.info(f"   - VLLMè°ƒç”¨: {vllm_calls}")
        
        # LoRAå¥–åŠ±è¯¦æƒ…
        logger.info(f"   - LoRAå¥–åŠ±è¯¦æƒ…:")
        for i, reward in enumerate(lora_rewards):
            lora_config = self.lora_configs[i]
            logger.info(f"     * LoRA {lora_config['lora_id']} (GPU{lora_config['gpu_id']}, {lora_config['group']}): {reward:.4f}")
        
        # ç»„åˆ«è¡¨ç°
        trump_reward = sum(lora_rewards[i] for i, config in enumerate(self.lora_configs) if config['group'] == "TRUMP")
        biden_reward = sum(lora_rewards[i] for i, config in enumerate(self.lora_configs) if config['group'] == "BIDEN")
        
        logger.info(f"   - ç»„åˆ«è¡¨ç°:")
        logger.info(f"     * TRUMPç»„: {trump_reward:.4f}")
        logger.info(f"     * BIDENç»„: {biden_reward:.4f}")
        
        # GPUåˆ©ç”¨ç‡
        logger.info(f"   - GPUåˆ©ç”¨ç‡:")
        for i, utilization in enumerate(gpu_utilization):
            logger.info(f"     * GPU {i}: {utilization:.2f}")
    
    def _calculate_reward(self, response: str, agent: Dict) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # åŸºäºå“åº”è´¨é‡å’Œagentä¿¡å¿µè®¡ç®—å¥–åŠ±
        base_reward = random.uniform(0.1, 1.0)
        
        # å“åº”è´¨é‡å¥–åŠ±
        if len(response) > 20:
            quality_bonus = 0.2
        else:
            quality_bonus = 0.0
        
        # ä¿¡å¿µä¸€è‡´æ€§å¥–åŠ±
        if agent['group'] in response.upper():
            belief_bonus = 0.3
        else:
            belief_bonus = 0.0
        
        return base_reward + quality_bonus + belief_bonus
    
    async def run_simulation(self):
        """è¿è¡Œæ¨¡æ‹Ÿ"""
        logger.info("å¼€å§‹8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿ...")
        
        for step in range(self.num_steps):
            await self.simulate_step()
            
            if (step + 1) % 10 == 0:
                self._print_detailed_statistics()
        
        logger.info("æ¨¡æ‹Ÿå®Œæˆ!")
        self._print_final_statistics()
    
    def _print_detailed_statistics(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡"""
        logger.info("=" * 60)
        logger.info("è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        
        for lora_config in self.lora_configs:
            logger.info(f"  - LoRA {lora_config['lora_id']} (GPU{lora_config['gpu_id']}, {lora_config['group']}):")
            logger.info(f"    æ€»å¥–åŠ±: {lora_config['total_reward']:.4f}")
            logger.info(f"    æ›´æ–°æ¬¡æ•°: {lora_config['update_count']}")
        
        logger.info("=" * 60)
    
    def _print_final_statistics(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        logger.info("=" * 80)
        logger.info("æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
        logger.info("=" * 80)
        
        total_vllm_calls = sum(self.statistics['vllm_calls'])
        total_reward = sum(self.statistics['step_rewards'])
        
        logger.info(f"æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"  - æ€»VLLMè°ƒç”¨: {total_vllm_calls}")
        logger.info(f"  - æ€»å¥–åŠ±: {total_reward:.4f}")
        
        # LoRAæœ€ç»ˆæ€§èƒ½
        logger.info(f"LoRAæœ€ç»ˆæ€§èƒ½:")
        for lora_config in self.lora_configs:
            logger.info(f"  - LoRA {lora_config['lora_id']} (GPU{lora_config['gpu_id']}, {lora_config['group']}):")
            logger.info(f"    æ€»å¥–åŠ±: {lora_config['total_reward']:.4f}")
            logger.info(f"    æ›´æ–°æ¬¡æ•°: {lora_config['update_count']}")
        
        # ç»„åˆ«å¯¹æ¯”
        trump_loras = [config for config in self.lora_configs if config['group'] == "TRUMP"]
        biden_loras = [config for config in self.lora_configs if config['group'] == "BIDEN"]
        
        trump_total_reward = sum(lora['total_reward'] for lora in trump_loras)
        biden_total_reward = sum(lora['total_reward'] for lora in biden_loras)
        
        logger.info(f"ç»„åˆ«å¯¹æ¯”:")
        logger.info(f"  - TRUMPç»„æ€»å¥–åŠ±: {trump_total_reward:.4f}")
        logger.info(f"  - BIDENç»„æ€»å¥–åŠ±: {biden_total_reward:.4f}")
        logger.info(f"  - èƒœå‡ºç»„: {'TRUMP' if trump_total_reward > biden_total_reward else 'BIDEN'}")
        
        # GPUåˆ©ç”¨ç‡ç»Ÿè®¡
        avg_gpu_utilization = []
        for gpu_id in range(8):
            gpu_utils = [step_utils[gpu_id] for step_utils in self.statistics['gpu_utilization']]
            avg_util = sum(gpu_utils) / len(gpu_utils) if gpu_utils else 0.0
            avg_gpu_utilization.append(avg_util)
        
        logger.info(f"GPUåˆ©ç”¨ç‡ç»Ÿè®¡:")
        for gpu_id, avg_util in enumerate(avg_gpu_utilization):
            logger.info(f"  - GPU {gpu_id}: å¹³å‡åˆ©ç”¨ç‡ {avg_util:.3f}")
        
        logger.info("=" * 80)
    
    def save_results(self, filename: str = "distributed_8gpu_results.json"):
        """ä¿å­˜æ¨¡æ‹Ÿç»“æœ"""
        results = {
            'simulation_config': {
                'num_agents': self.num_agents,
                'num_steps': self.num_steps,
                'num_gpus': 8
            },
            'lora_configs': self.lora_configs,
            'statistics': self.statistics
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ 8GPUåˆ†å¸ƒå¼è°ƒåº¦å™¨æ¼”ç¤º")
    print("=" * 80)
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿå™¨
        print("\nğŸ“‹ åˆ›å»º8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿå™¨...")
        simulation = Distributed8GPUSimulation(num_agents=50, num_steps=30)
        
        # è¿è¡Œæ¨¡æ‹Ÿ
        print("\nğŸ¬ å¼€å§‹8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿ...")
        await simulation.run_simulation()
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜æ¨¡æ‹Ÿç»“æœ...")
        simulation.save_results()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ 8GPUåˆ†å¸ƒå¼æ¨¡æ‹Ÿå®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
