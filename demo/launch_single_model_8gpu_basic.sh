#!/usr/bin/env bash

# åŸºç¡€ç‰ˆæœ¬ï¼šä¸åŠ è½½LoRAï¼Œç¡®ä¿vLLMèƒ½æ­£å¸¸å¯åŠ¨
echo "ğŸš€ å¯åŠ¨å•æ¨¡å‹+8GPU vLLM (åŸºç¡€ç‰ˆæœ¬ï¼Œæ— LoRA)..."

# åœæ­¢ç°æœ‰è¿›ç¨‹
pkill -f "vllm serve" || true
sleep 3

# å¯åŠ¨vLLMï¼Œä¸åŠ è½½LoRA
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 vllm serve /cpfs04/shared/kilab/hf-hub/Qwen2.5-7B-Instruct \
  --port 8001 \
  --served-model-name qwen-2 \
  --tensor-parallel-size 4 \
  --dtype bfloat16 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.4 \
  --max-num-seqs 128

echo "âœ… vLLMå¯åŠ¨å®Œæˆ"
