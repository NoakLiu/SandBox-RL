#!/usr/bin/env bash

# æ ¹æ®vLLMå®˜æ–¹æ–‡æ¡£çš„æ­£ç¡®LoRAå¯åŠ¨æ–¹å¼
echo "ğŸš€ å¯åŠ¨å•æ¨¡å‹+8GPU vLLM (æ­£ç¡®LoRAé…ç½®)..."

# åœæ­¢ç°æœ‰è¿›ç¨‹
pkill -f "vllm serve" || true
sleep 3

# åˆ›å»ºLoRAæ¨¡å—é…ç½®
# ä½¿ç”¨--lora-moduleså‚æ•°é¢„åŠ è½½LoRAé€‚é…å™¨
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 vllm serve /cpfs04/shared/kilab/hf-hub/Qwen2.5-7B-Instruct \
  --port 8001 \
  --served-model-name qwen-2 \
  --tensor-parallel-size 4 \
  --enable-lora \
  --max-lora-rank 64 \
  --max-loras 16 \
  --lora-modules \
    lora1=/cpfs04/shared/kilab/liudong/lora1 \
    lora2=/cpfs04/shared/kilab/liudong/lora2 \
    lora3=/cpfs04/shared/kilab/liudong/lora3 \
    lora4=/cpfs04/shared/kilab/liudong/lora4 \
    lora5=/cpfs04/shared/kilab/liudong/lora5 \
    lora6=/cpfs04/shared/kilab/liudong/lora6 \
    lora7=/cpfs04/shared/kilab/liudong/lora7 \
    lora8=/cpfs04/shared/kilab/liudong/lora8

echo "âœ… vLLMå¯åŠ¨å®Œæˆ"
