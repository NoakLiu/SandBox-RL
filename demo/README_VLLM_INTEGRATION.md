# VLLMé›†æˆè¯´æ˜

## æ¦‚è¿°

æœ¬å¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿå·²é›†æˆVLLMæ¥å£ï¼Œæ”¯æŒä½¿ç”¨Camelå’ŒOasisæ¡†æ¶çš„VLLMæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚

## é›†æˆæ–¹å¼

### 1. Camelå’ŒOasis VLLMæ¥å£

ç³»ç»Ÿä¼˜å…ˆä½¿ç”¨Camelå’ŒOasisæä¾›çš„VLLMæ¥å£ï¼š

```python
from camel.models import ModelFactory
from camel.types import ModelPlatformType

# åˆ›å»ºVLLMæ¨¡å‹
vllm_model = ModelFactory.create(
    model_platform=ModelPlatformType.VLLM,
    model_type="qwen-2",
    url="http://localhost:8001/v1",
)
```

### 2. å¤‡ç”¨HTTPå®¢æˆ·ç«¯

å¦‚æœCamelå’ŒOasisä¸å¯ç”¨ï¼Œç³»ç»Ÿä¼šå›é€€åˆ°HTTPå®¢æˆ·ç«¯ï¼š

```python
import aiohttp

async with aiohttp.ClientSession() as session:
    payload = {
        "model": "qwen-2",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": 0.7
    }
    
    async with session.post(f"{vllm_url}/chat/completions", json=payload) as response:
        result = await response.json()
        content = result["choices"][0]["message"]["content"]
```

### 3. æ¨¡æ‹Ÿæ¨¡å¼

å¦‚æœæ‰€æœ‰VLLMæ¥å£éƒ½ä¸å¯ç”¨ï¼Œç³»ç»Ÿä¼šä½¿ç”¨æ¨¡æ‹Ÿå“åº”ã€‚

## ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨VLLMæœåŠ¡å™¨

```bash
# å¯åŠ¨VLLMæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model qwen/Qwen2-7B-Instruct \
    --host 0.0.0.0 \
    --port 8001
```

### 2. è¿è¡Œå¤šæ¨¡å‹è®­ç»ƒ

```bash
# è¿è¡Œè®­ç»ƒç³»ç»Ÿ
python demo/multi_model_single_env_simple.py
```

### 3. æµ‹è¯•VLLMé›†æˆ

```bash
# æµ‹è¯•Camelå’ŒOasis VLLMæ¥å£
python demo/camel_oasis_vllm_example.py
```

## é…ç½®è¯´æ˜

### VLLMå®¢æˆ·ç«¯é…ç½®

```python
class VLLMClient:
    def __init__(self, url: str = "http://localhost:8001/v1", model_name: str = "qwen-2"):
        self.url = url
        self.model_name = model_name
        self.camel_model = None
        self.connection_available = False
        self._initialize_camel_model()
```

### å¤šæ¨¡å‹ç¯å¢ƒé…ç½®

```python
env = MultiModelEnvironment(
    vllm_url="http://localhost:8001/v1",
    training_mode=TrainingMode.COOPERATIVE,
    max_models=10
)
```

## åŠŸèƒ½ç‰¹æ€§

### 1. è‡ªåŠ¨æ£€æµ‹

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹VLLMæœåŠ¡å™¨çš„å¯ç”¨æ€§ï¼š

- Camelå’ŒOasis VLLMæ¥å£
- HTTPå®¢æˆ·ç«¯
- æ¨¡æ‹Ÿæ¨¡å¼

### 2. æ™ºèƒ½å›é€€

å¦‚æœä¸»è¦æ¥å£å¤±è´¥ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å›é€€åˆ°å¤‡ç”¨æ–¹æ¡ˆï¼š

1. Camelå’ŒOasis VLLMæ¥å£
2. HTTPå®¢æˆ·ç«¯
3. æ¨¡æ‹Ÿå“åº”

### 3. ä»»åŠ¡æç¤ºè¯

ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºä¸“é—¨çš„æç¤ºè¯ï¼š

```python
def _build_task_prompt(self, task: TrainingTask, strategy: Dict[str, Any], 
                      other_models: List['LoRAModel']) -> str:
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªAIæ¨¡å‹ï¼Œæ­£åœ¨å‚ä¸å¤šæ¨¡å‹è®­ç»ƒä»»åŠ¡ã€‚

ä»»åŠ¡ä¿¡æ¯:
- ä»»åŠ¡ID: {task.task_id}
- ä»»åŠ¡ç±»å‹: {task.task_type}
- éš¾åº¦: {task.difficulty:.2f}
- å¥–åŠ±æ± : {task.reward_pool:.2f}
- åˆä½œçº§åˆ«: {task.cooperation_level:.2f}

ä½ çš„è§’è‰²: {self.config.role.value}
ä½ çš„å›¢é˜Ÿ: {self.config.team_id or 'æ— '}
ä½ çš„ä¸“é•¿: {self.config.specialization}

å½“å‰ç­–ç•¥:
- æ¨¡å¼: {strategy.get('mode', 'unknown')}
- å›¢é˜Ÿåˆä½œçº§åˆ«: {strategy.get('teamwork_level', 0):.2f}
- é€šä¿¡: {strategy.get('communication', False)}
- èµ„æºå…±äº«: {strategy.get('resource_sharing', False)}

å…¶ä»–æ¨¡å‹æ•°é‡: {len(other_models)}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºè¿™ä¸ªä»»åŠ¡æä¾›æ‰§è¡Œç­–ç•¥å»ºè®®ã€‚è€ƒè™‘ä½ çš„è§’è‰²ã€ä»»åŠ¡ç±»å‹å’Œåˆä½œçº§åˆ«ã€‚
"""
    return prompt
```

### 4. æ€§èƒ½ä¼˜åŒ–

VLLMå“åº”ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼š

```python
# VLLMå“åº”è´¨é‡åŠ æˆ
vllm_bonus = 0.1 if "ååŒ" in vllm_response or "åˆä½œ" in vllm_response else 0.05
if "ç«äº‰" in vllm_response or "ä¼˜åŒ–" in vllm_response:
    vllm_bonus += 0.05

# åº”ç”¨åˆ°æ€§èƒ½è®¡ç®—
accuracy = min(1.0, base_accuracy + cooperation_bonus + vllm_bonus)
efficiency = min(1.0, base_efficiency + cooperation_bonus + vllm_bonus)
```

## è®­ç»ƒæ¨¡å¼

### 1. ååŒè®­ç»ƒ

æ¨¡å‹ä¹‹é—´ç›¸äº’åˆä½œï¼Œå…±äº«VLLMç”Ÿæˆçš„ç­–ç•¥å»ºè®®ã€‚

### 2. ç«äº‰è®­ç»ƒ

æ¨¡å‹ç‹¬ç«‹ä½¿ç”¨VLLMç”Ÿæˆç«äº‰ç­–ç•¥ã€‚

### 3. ç»„é˜Ÿåšå¼ˆ

å›¢é˜Ÿå†…éƒ¨å…±äº«VLLMç­–ç•¥ï¼Œå›¢é˜Ÿé—´ç«äº‰ã€‚

### 4. æ··åˆè®­ç»ƒ

ç»“åˆå¤šç§è®­ç»ƒæ–¹å¼ï¼ŒåŠ¨æ€è°ƒæ•´VLLMä½¿ç”¨ç­–ç•¥ã€‚

## ç›‘æ§å’Œæ—¥å¿—

### VLLMè°ƒç”¨æ—¥å¿—

```
ğŸ¤– Camel VLLMç”Ÿæˆ: åŸºäºå½“å‰ä»»åŠ¡åˆ†æï¼Œæˆ‘å»ºè®®é‡‡ç”¨ååŒç­–ç•¥...
ğŸ¤– HTTP VLLMç”Ÿæˆ: é€šè¿‡ç«äº‰æœºåˆ¶å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½è¡¨ç°...
ğŸ¤– æ¨¡æ‹ŸVLLMç”Ÿæˆ: å›¢é˜Ÿåˆä½œæ˜¯è§£å†³å¤æ‚é—®é¢˜çš„å…³é”®...
```

### æ€§èƒ½æŒ‡æ ‡

- VLLMå“åº”è´¨é‡
- è°ƒç”¨æˆåŠŸç‡
- å“åº”æ—¶é—´
- ç­–ç•¥æ•ˆæœ

## æ•…éšœæ’é™¤

### 1. VLLMæœåŠ¡å™¨è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥VLLMæœåŠ¡å™¨çŠ¶æ€
curl http://localhost:8001/v1/models

# é‡å¯VLLMæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model qwen/Qwen2-7B-Instruct \
    --host 0.0.0.0 \
    --port 8001
```

### 2. Camelå’ŒOasiså¯¼å…¥å¤±è´¥

```bash
# å®‰è£…ä¾èµ–
pip install camel-ai oasis

# æ£€æŸ¥å®‰è£…
python -c "import camel; import oasis; print('OK')"
```

### 3. æ€§èƒ½é—®é¢˜

- æ£€æŸ¥VLLMæœåŠ¡å™¨èµ„æºä½¿ç”¨æƒ…å†µ
- è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆmax_tokens, temperatureï¼‰
- ä¼˜åŒ–æç¤ºè¯é•¿åº¦

## æ‰©å±•åŠŸèƒ½

### 1. è‡ªå®šä¹‰VLLMæ¨¡å‹

```python
# æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹
custom_model = ModelFactory.create(
    model_platform=ModelPlatformType.VLLM,
    model_type="custom-model",
    url="http://localhost:8002/v1",
)
```

### 2. å¤šVLLMæœåŠ¡å™¨

```python
# æ”¯æŒå¤šä¸ªVLLMæœåŠ¡å™¨
vllm_servers = [
    "http://localhost:8001/v1",
    "http://localhost:8002/v1",
    "http://localhost:8003/v1"
]
```

### 3. è´Ÿè½½å‡è¡¡

```python
# å®ç°è´Ÿè½½å‡è¡¡
import random
vllm_url = random.choice(vllm_servers)
```

## æ€»ç»“

VLLMé›†æˆä¸ºå¤šæ¨¡å‹è®­ç»ƒç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼š

- âœ… æ”¯æŒCamelå’ŒOasis VLLMæ¥å£
- âœ… è‡ªåŠ¨å›é€€æœºåˆ¶
- âœ… æ™ºèƒ½æç¤ºè¯æ„å»º
- âœ… æ€§èƒ½ä¼˜åŒ–
- âœ… å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—
- âœ… çµæ´»çš„é…ç½®é€‰é¡¹

é€šè¿‡VLLMé›†æˆï¼Œç³»ç»Ÿèƒ½å¤Ÿï¼š

1. ç”Ÿæˆæ›´æ™ºèƒ½çš„è®­ç»ƒç­–ç•¥
2. æé«˜æ¨¡å‹åä½œæ•ˆç‡
3. ä¼˜åŒ–èµ„æºåˆ†é…
4. å¢å¼ºè®­ç»ƒæ•ˆæœ
5. æ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹äº¤äº’
