#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¤šæ¨¡å‹å•ç¯å¢ƒè®­ç»ƒç³»ç»Ÿ
æ”¯æŒååŒã€ç«äº‰ã€ç»„é˜Ÿåšå¼ˆç­‰ä¸åŒè®­ç»ƒæ¨¡å¼
ä½¿ç”¨VLLM/AReaLä¸­é›†æˆçš„ä¸åŒLoRAå®ç°
"""

import asyncio
import time
import logging
import random
import json
import os
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import threading
from concurrent.futures import ThreadPoolExecutor

# VLLMé›†æˆ - ä½¿ç”¨Camelå’ŒOasisæ¥å£
try:
    from camel.models import ModelFactory
    from camel.types import ModelPlatformType
    import oasis
    CAMEL_OASIS_AVAILABLE = True
    print("âœ… Camelå’ŒOasis VLLMæ¥å£å¯ç”¨")
except ImportError:
    CAMEL_OASIS_AVAILABLE = False
    print("âš ï¸ Camelå’ŒOasis VLLMæ¥å£ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# å¤‡ç”¨HTTPå®¢æˆ·ç«¯
try:
    import requests
    import aiohttp
    VLLM_AVAILABLE = True
    print("âœ… VLLM HTTPå®¢æˆ·ç«¯å¯ç”¨")
except ImportError:
    VLLM_AVAILABLE = False
    print("âš ï¸ VLLM HTTPå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# ç®€åŒ–çš„numpyæ›¿ä»£
def simple_randn(*shape):
    """ç®€åŒ–çš„éšæœºæ•°ç”Ÿæˆå‡½æ•°"""
    if len(shape) == 1:
        return [random.uniform(-1, 1) for _ in range(shape[0])]
    elif len(shape) == 2:
        return [[random.uniform(-1, 1) for _ in range(shape[1])] 
               for _ in range(shape[0])]
    else:
        return [random.uniform(-1, 1) for _ in range(shape[0])]

class SimpleNumpy:
    @staticmethod
    def mean(data):
        if not data:
            return 0.0
        return sum(data) / len(data)
    
    @staticmethod
    def array(data):
        return data
    
    @staticmethod
    def zeros(shape):
        if len(shape) == 1:
            return [0.0] * shape[0]
        elif len(shape) == 2:
            return [[0.0] * shape[1] for _ in range(shape[0])]
        return [0.0] * shape[0]

np = SimpleNumpy()

logger = logging.getLogger(__name__)

class VLLMClient:
    """VLLMå®¢æˆ·ç«¯ - ä½¿ç”¨Camelå’ŒOasisæ¥å£"""
    
    def __init__(self, url: str = "http://localhost:8001/v1", model_name: str = "qwen-2"):
        self.url = url
        self.model_name = model_name
        self.camel_model = None
        self.connection_available = False
        self._initialize_camel_model()
    
    def _initialize_camel_model(self):
        """åˆå§‹åŒ–Camelæ¨¡å‹"""
        if CAMEL_OASIS_AVAILABLE:
            try:
                self.camel_model = ModelFactory.create(
                    model_platform=ModelPlatformType.VLLM,
                    model_type=self.model_name,
                    url=self.url,
                )
                self.connection_available = True
                print(f"âœ… Camel VLLMæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.url}")
            except Exception as e:
                print(f"âš ï¸ Camel VLLMæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self.connection_available = False
        else:
            print("âš ï¸ Camelå’ŒOasisä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.connection_available = False
    
    async def generate(self, prompt: str, max_tokens: int = 100) -> str:
        """ç”Ÿæˆæ–‡æœ¬å“åº”"""
        if self.camel_model and CAMEL_OASIS_AVAILABLE and self.connection_available:
            try:
                # ä½¿ç”¨Camelæ¨¡å‹ç”Ÿæˆå“åº”
                response = await self.camel_model.generate(prompt, max_tokens=max_tokens)
                print(f"ğŸ¤– Camel VLLMç”Ÿæˆ: {response[:50]}...")
                return response
            except Exception as e:
                print(f"âŒ Camel VLLMè°ƒç”¨å¤±è´¥: {e}")
                print("å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
        
        # å¤‡ç”¨HTTPå®¢æˆ·ç«¯
        if VLLM_AVAILABLE:
            try:
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "model": self.model_name,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": 0.7
                    }
                    
                    async with session.post(f"{self.url}/chat/completions", json=payload) as response:
                        if response.status == 200:
                            result = await response.json()
                            content = result["choices"][0]["message"]["content"]
                            print(f"ğŸ¤– HTTP VLLMç”Ÿæˆ: {content[:50]}...")
                            return content
                        else:
                            print(f"âŒ HTTP VLLMå“åº”é”™è¯¯: {response.status}")
                            return self._generate_mock_response(prompt)
            except Exception as e:
                print(f"âŒ HTTP VLLMè°ƒç”¨å¤±è´¥: {e}")
                return self._generate_mock_response(prompt)
        
        return self._generate_mock_response(prompt)
    
    def _generate_mock_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        mock_responses = [
            "åŸºäºå½“å‰ä»»åŠ¡åˆ†æï¼Œæˆ‘å»ºè®®é‡‡ç”¨ååŒç­–ç•¥ã€‚",
            "é€šè¿‡ç«äº‰æœºåˆ¶å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚",
            "å›¢é˜Ÿåˆä½œæ˜¯è§£å†³å¤æ‚é—®é¢˜çš„å…³é”®ã€‚",
            "éœ€è¦å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å…³ç³»ã€‚",
            "LoRAé€‚åº”å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚"
        ]
        return random.choice(mock_responses)

class TrainingMode(Enum):
    """è®­ç»ƒæ¨¡å¼"""
    COOPERATIVE = "cooperative"      # ååŒè®­ç»ƒ
    COMPETITIVE = "competitive"      # ç«äº‰è®­ç»ƒ
    TEAM_BATTLE = "team_battle"      # ç»„é˜Ÿåšå¼ˆ
    MIXED = "mixed"                  # æ··åˆæ¨¡å¼

class ModelRole(Enum):
    """æ¨¡å‹è§’è‰²"""
    LEADER = "leader"                # é¢†å¯¼è€…
    FOLLOWER = "follower"            # è·Ÿéšè€…
    COMPETITOR = "competitor"        # ç«äº‰è€…
    TEAMMATE = "teammate"            # é˜Ÿå‹
    NEUTRAL = "neutral"              # ä¸­ç«‹è€…

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_id: str
    model_name: str
    role: ModelRole
    lora_rank: int = 8
    lora_alpha: float = 16.0
    learning_rate: float = 1e-4
    team_id: Optional[str] = None
    specialization: str = "general"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_id": self.model_id,
            "model_name": self.model_name,
            "role": self.role.value,
            "lora_rank": self.lora_rank,
            "lora_alpha": self.lora_alpha,
            "learning_rate": self.learning_rate,
            "team_id": self.team_id,
            "specialization": self.specialization
        }

@dataclass
class TrainingTask:
    """è®­ç»ƒä»»åŠ¡"""
    task_id: str
    task_type: str
    difficulty: float  # 0-1
    reward_pool: float
    max_steps: int
    required_models: int
    cooperation_level: float  # 0-1, 0=å®Œå…¨ç«äº‰, 1=å®Œå…¨ååŒ
    created_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "task_type": self.task_type,
            "difficulty": self.difficulty,
            "reward_pool": self.reward_pool,
            "max_steps": self.max_steps,
            "required_models": self.required_models,
            "cooperation_level": self.cooperation_level,
            "created_at": self.created_at.isoformat()
        }

@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    model_id: str
    task_id: str
    completion_time: float
    accuracy: float
    efficiency: float
    cooperation_score: float
    reward_earned: float
    weight_updates: int
    lora_adaptations: int
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_id": self.model_id,
            "task_id": self.task_id,
            "completion_time": self.completion_time,
            "accuracy": self.accuracy,
            "efficiency": self.efficiency,
            "cooperation_score": self.cooperation_score,
            "reward_earned": self.reward_earned,
            "weight_updates": self.weight_updates,
            "lora_adaptations": self.lora_adaptations,
            "timestamp": self.timestamp.isoformat()
        }

class MockLoRAManager:
    """æ¨¡æ‹ŸLoRAç®¡ç†å™¨"""
    
    def __init__(self, rank: int = 8, alpha: float = 16.0):
        self.rank = rank
        self.alpha = alpha
        self.adaptation_count = 0
        self.adaptations = []
    
    def adapt(self, task_type: str, performance: float):
        """æ¨¡æ‹ŸLoRAé€‚åº”"""
        self.adaptation_count += 1
        self.adaptations.append({
            "task_type": task_type,
            "performance": performance,
            "timestamp": datetime.now().isoformat()
        })
        return True
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            "rank": self.rank,
            "alpha": self.alpha,
            "adaptation_count": self.adaptation_count,
            "recent_adaptations": self.adaptations[-5:] if self.adaptations else []
        }

class MockFrozenLLM:
    """æ¨¡æ‹ŸFrozen Adaptive LLM"""
    
    def __init__(self, learning_rate: float = 1e-4):
        self.learning_rate = learning_rate
        self.weight_update_count = 0
        self.parameters = {
            "embedding": simple_randn(100, 768),
            "layers.0": simple_randn(768, 768),
            "layers.1": simple_randn(768, 768),
            "output": simple_randn(768, 100)
        }
    
    def update_parameters(self, gradients: Dict[str, Any], lr: float):
        """æ›´æ–°å‚æ•°"""
        self.weight_update_count += 1
        # æ¨¡æ‹Ÿå‚æ•°æ›´æ–°
        for key in self.parameters:
            if key in gradients:
                # ç®€åŒ–çš„å‚æ•°æ›´æ–°
                pass
        return True
    
    def generate(self, prompt: str) -> str:
        """ç”Ÿæˆå“åº”"""
        return f"Mock response for {prompt[:20]}..."
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            "learning_rate": self.learning_rate,
            "weight_update_count": self.weight_update_count,
            "parameter_count": len(self.parameters)
        }

class LoRAModel:
    """LoRAæ¨¡å‹å°è£…"""
    
    def __init__(self, config: ModelConfig, vllm_url: str = "http://localhost:8001/v1"):
        self.config = config
        self.vllm_url = vllm_url
        self.lora_manager = MockLoRAManager(config.lora_rank, config.lora_alpha)
        self.frozen_llm = MockFrozenLLM(config.learning_rate)
        self.performance_history: List[ModelPerformance] = []
        self.current_task = None
        
        # åˆå§‹åŒ–VLLMå®¢æˆ·ç«¯
        self.vllm_client = VLLMClient(vllm_url, f"{config.model_name}-lora-{config.lora_rank}")
        
        print(f"âœ… Model {self.config.model_id} initialized with LoRA rank={self.config.lora_rank}")
        print(f"ğŸ¤– VLLMå®¢æˆ·ç«¯: {self.vllm_client.connection_available}")
    
    async def process_task(self, task: TrainingTask, other_models: List['LoRAModel']) -> ModelPerformance:
        """å¤„ç†è®­ç»ƒä»»åŠ¡"""
        start_time = time.time()
        self.current_task = task
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œåˆä½œçº§åˆ«å†³å®šç­–ç•¥
        strategy = self._determine_strategy(task, other_models)
        
        # æ‰§è¡Œä»»åŠ¡
        result = await self._execute_task(task, strategy, other_models)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        completion_time = time.time() - start_time
        performance = ModelPerformance(
            model_id=self.config.model_id,
            task_id=task.task_id,
            completion_time=completion_time,
            accuracy=result.get("accuracy", 0.0),
            efficiency=result.get("efficiency", 0.0),
            cooperation_score=result.get("cooperation_score", 0.0),
            reward_earned=result.get("reward_earned", 0.0),
            weight_updates=self.frozen_llm.weight_update_count,
            lora_adaptations=self.lora_manager.adaptation_count
        )
        
        self.performance_history.append(performance)
        
        # æ›´æ–°æ¨¡å‹æƒé‡
        await self._update_weights(result)
        
        return performance
    
    def _determine_strategy(self, task: TrainingTask, other_models: List['LoRAModel']) -> Dict[str, Any]:
        """æ ¹æ®ä»»åŠ¡å’Œåˆä½œçº§åˆ«ç¡®å®šç­–ç•¥"""
        strategy = {
            "mode": "cooperative" if task.cooperation_level > 0.5 else "competitive",
            "teamwork_level": task.cooperation_level,
            "communication": task.cooperation_level > 0.3,
            "resource_sharing": task.cooperation_level > 0.7
        }
        
        # æ ¹æ®è§’è‰²è°ƒæ•´ç­–ç•¥
        if self.config.role == ModelRole.LEADER:
            strategy["leadership"] = True
            strategy["decision_making"] = "centralized"
        elif self.config.role == ModelRole.FOLLOWER:
            strategy["leadership"] = False
            strategy["decision_making"] = "delegated"
        elif self.config.role == ModelRole.COMPETITOR:
            strategy["mode"] = "competitive"
            strategy["aggression"] = 0.8
        
        return strategy
    
    async def _execute_task(self, task: TrainingTask, strategy: Dict[str, Any], 
                          other_models: List['LoRAModel']) -> Dict[str, Any]:
        """æ‰§è¡Œä»»åŠ¡"""
        # æ„å»ºVLLMæç¤ºè¯
        prompt = self._build_task_prompt(task, strategy, other_models)
        
        # ä½¿ç”¨VLLMç”Ÿæˆå“åº”
        try:
            vllm_response = await self.vllm_client.generate(prompt, max_tokens=150)
            print(f"ğŸ¤– Model {self.config.model_id} VLLMå“åº”: {vllm_response[:100]}...")
        except Exception as e:
            print(f"âŒ VLLMè°ƒç”¨å¤±è´¥: {e}")
            vllm_response = "ä½¿ç”¨é»˜è®¤ç­–ç•¥æ‰§è¡Œä»»åŠ¡"
        
        # æ ¹æ®VLLMå“åº”å’Œç­–ç•¥è®¡ç®—æ€§èƒ½
        base_accuracy = random.uniform(0.6, 0.9)
        base_efficiency = random.uniform(0.5, 0.8)
        
        # VLLMå“åº”è´¨é‡åŠ æˆ
        vllm_bonus = 0.1 if "ååŒ" in vllm_response or "åˆä½œ" in vllm_response else 0.05
        if "ç«äº‰" in vllm_response or "ä¼˜åŒ–" in vllm_response:
            vllm_bonus += 0.05
        
        # åˆä½œåŠ æˆ
        cooperation_bonus = strategy["teamwork_level"] * 0.2
        accuracy = min(1.0, base_accuracy + cooperation_bonus + vllm_bonus)
        efficiency = min(1.0, base_efficiency + cooperation_bonus + vllm_bonus)
        
        # è®¡ç®—å¥–åŠ±
        reward_earned = task.reward_pool * accuracy * efficiency / len(other_models + [self])
        
        # æ¨¡æ‹ŸLoRAé€‚åº”
        if random.random() < 0.3:
            self.lora_manager.adapt(task.task_type, accuracy)
        
        return {
            "accuracy": accuracy,
            "efficiency": efficiency,
            "cooperation_score": strategy["teamwork_level"],
            "reward_earned": reward_earned,
            "vllm_response": vllm_response,
            "strategy_used": strategy
        }
    
    def _build_task_prompt(self, task: TrainingTask, strategy: Dict[str, Any], 
                          other_models: List['LoRAModel']) -> str:
        """æ„å»ºVLLMä»»åŠ¡æç¤ºè¯"""
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªAIæ¨¡å‹ï¼Œæ­£åœ¨å‚ä¸å¤šæ¨¡å‹è®­ç»ƒä»»åŠ¡ã€‚

ä»»åŠ¡ä¿¡æ¯:
- ä»»åŠ¡ID: {task.task_id}
- ä»»åŠ¡ç±»å‹: {task.task_type}
- éš¾åº¦: {task.difficulty:.2f}
- å¥–åŠ±æ± : {task.reward_pool:.2f}
- åˆä½œçº§åˆ«: {task.cooperation_level:.2f}

ä½ çš„è§’è‰²: {self.config.role.value}
ä½ çš„å›¢é˜Ÿ: {self.config.team_id or 'æ— '}
ä½ çš„ä¸“é•¿: {self.config.specialization}

å½“å‰ç­–ç•¥:
- æ¨¡å¼: {strategy.get('mode', 'unknown')}
- å›¢é˜Ÿåˆä½œçº§åˆ«: {strategy.get('teamwork_level', 0):.2f}
- é€šä¿¡: {strategy.get('communication', False)}
- èµ„æºå…±äº«: {strategy.get('resource_sharing', False)}

å…¶ä»–æ¨¡å‹æ•°é‡: {len(other_models)}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºè¿™ä¸ªä»»åŠ¡æä¾›æ‰§è¡Œç­–ç•¥å»ºè®®ã€‚è€ƒè™‘ä½ çš„è§’è‰²ã€ä»»åŠ¡ç±»å‹å’Œåˆä½œçº§åˆ«ã€‚
"""
        return prompt
    
    async def _update_weights(self, result: Dict[str, Any]):
        """æ›´æ–°æ¨¡å‹æƒé‡"""
        if result.get("accuracy", 0) > 0.7:
            # ç”Ÿæˆæ¨¡æ‹Ÿæ¢¯åº¦
            gradients = {
                "embedding": simple_randn(100, 768),
                "layers.0": simple_randn(768, 768),
                "layers.1": simple_randn(768, 768),
                "output": simple_randn(768, 100)
            }
            
            try:
                self.frozen_llm.update_parameters(gradients, self.config.learning_rate)
                print(f"ğŸ”„ Model {self.config.model_id} weights updated")
            except Exception as e:
                print(f"âš ï¸ Failed to update weights for model {self.config.model_id}: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        if not self.performance_history:
            return {"model_id": self.config.model_id, "no_data": True}
        
        recent_performances = self.performance_history[-10:]  # æœ€è¿‘10æ¬¡
        
        return {
            "model_id": self.config.model_id,
            "role": self.config.role.value,
            "team_id": self.config.team_id,
            "total_tasks": len(self.performance_history),
            "avg_accuracy": np.mean([p.accuracy for p in recent_performances]),
            "avg_efficiency": np.mean([p.efficiency for p in recent_performances]),
            "avg_cooperation": np.mean([p.cooperation_score for p in recent_performances]),
            "total_reward": sum([p.reward_earned for p in self.performance_history]),
            "weight_updates": self.frozen_llm.weight_update_count,
            "lora_adaptations": self.lora_manager.adaptation_count,
            "last_activity": self.performance_history[-1].timestamp.isoformat() if self.performance_history else None,
            "lora_stats": self.lora_manager.get_stats(),
            "llm_stats": self.frozen_llm.get_stats()
        }

class MultiModelEnvironment:
    """å¤šæ¨¡å‹å•ç¯å¢ƒè®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, 
                 vllm_url: str = "http://localhost:8001/v1",
                 training_mode: TrainingMode = TrainingMode.MIXED,
                 max_models: int = 10):
        self.vllm_url = vllm_url
        self.training_mode = training_mode
        self.max_models = max_models
        
        self.models: Dict[str, LoRAModel] = {}
        self.tasks: List[TrainingTask] = []
        self.task_queue: List[TrainingTask] = []
        self.completed_tasks: List[TrainingTask] = []
        
        # æµ‹è¯•VLLMè¿æ¥
        self.vllm_available = self._test_vllm_connection()
        
        self._generate_initial_tasks()
        print(f"âœ… Multi-model environment initialized with mode: {training_mode.value}")
        print(f"ğŸ¤– VLLMå¯ç”¨æ€§: {self.vllm_available}")
    
    def _test_vllm_connection(self) -> bool:
        """æµ‹è¯•VLLMè¿æ¥"""
        try:
            response = requests.get(f"{self.vllm_url}/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _generate_initial_tasks(self):
        """ç”Ÿæˆåˆå§‹è®­ç»ƒä»»åŠ¡"""
        task_types = ["classification", "generation", "reasoning", "optimization", "collaboration"]
        
        for i in range(20):
            task = TrainingTask(
                task_id=f"task_{i:03d}",
                task_type=random.choice(task_types),
                difficulty=random.uniform(0.3, 0.9),
                reward_pool=random.uniform(10.0, 100.0),
                max_steps=random.randint(5, 20),
                required_models=random.randint(2, 5),
                cooperation_level=random.uniform(0.0, 1.0)
            )
            self.task_queue.append(task)
    
    def add_model(self, config: ModelConfig) -> bool:
        """æ·»åŠ æ¨¡å‹åˆ°ç¯å¢ƒ"""
        if len(self.models) >= self.max_models:
            print(f"âŒ Maximum models ({self.max_models}) reached")
            return False
        
        if config.model_id in self.models:
            print(f"âŒ Model {config.model_id} already exists")
            return False
        
        model = LoRAModel(config, self.vllm_url)
        self.models[config.model_id] = model
        
        print(f"âœ… Model {config.model_id} added to environment")
        return True
    
    def remove_model(self, model_id: str) -> bool:
        """ä»ç¯å¢ƒä¸­ç§»é™¤æ¨¡å‹"""
        if model_id not in self.models:
            return False
        
        del self.models[model_id]
        print(f"âœ… Model {model_id} removed from environment")
        return True
    
    async def run_training_cycle(self, cycles: int = 5) -> List[Dict[str, Any]]:
        """è¿è¡Œè®­ç»ƒå‘¨æœŸ"""
        results = []
        
        for cycle in range(cycles):
            print(f"\nğŸ”„ Training Cycle {cycle + 1}/{cycles}")
            
            # åˆ†é…ä»»åŠ¡ç»™æ¨¡å‹
            cycle_results = await self._execute_training_cycle()
            results.extend(cycle_results)
            
            # çŸ­æš‚ä¼‘æ¯
            await asyncio.sleep(1.0)
        
        return results
    
    async def _execute_training_cycle(self) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå•ä¸ªè®­ç»ƒå‘¨æœŸ"""
        results = []
        
        # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
        if not self.task_queue:
            self._generate_initial_tasks()
        
        # é€‰æ‹©é€‚åˆçš„ä»»åŠ¡
        available_models = list(self.models.values())
        if len(available_models) < 2:
            print("âš ï¸ Need at least 2 models for training")
            return results
        
        # éšæœºé€‰æ‹©ä»»åŠ¡
        task = random.choice(self.task_queue)
        self.task_queue.remove(task)
        
        print(f"ğŸ“‹ Executing task: {task.task_id} ({task.task_type})")
        print(f"   Cooperation level: {task.cooperation_level:.2f}")
        print(f"   Required models: {task.required_models}")
        print(f"   VLLM available: {self.vllm_available}")
        
        # é€‰æ‹©å‚ä¸æ¨¡å‹
        participating_models = random.sample(available_models, 
                                           min(task.required_models, len(available_models)))
        
        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
        tasks = []
        for model in participating_models:
            other_models = [m for m in participating_models if m != model]
            task_coro = model.process_task(task, other_models)
            tasks.append(task_coro)
        
        performances = await asyncio.gather(*tasks)
        
        # è®°å½•ç»“æœ
        for performance in performances:
            results.append(performance.to_dict())
            print(f"   Model {performance.model_id}: accuracy={performance.accuracy:.3f}, "
                  f"efficiency={performance.efficiency:.3f}, reward={performance.reward_earned:.2f}")
        
        self.completed_tasks.append(task)
        return results
    
    def get_environment_stats(self) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒç»Ÿè®¡ä¿¡æ¯"""
        model_stats = {model_id: model.get_stats() for model_id, model in self.models.items()}
        
        return {
            "environment_info": {
                "training_mode": self.training_mode.value,
                "total_models": len(self.models),
                "max_models": self.max_models,
                "total_tasks": len(self.completed_tasks),
                "pending_tasks": len(self.task_queue)
            },
            "model_stats": model_stats,
            "task_distribution": {
                "completed": len(self.completed_tasks),
                "pending": len(self.task_queue),
                "total_generated": len(self.completed_tasks) + len(self.task_queue)
            },
            "performance_summary": {
                "avg_accuracy": np.mean([stats.get("avg_accuracy", 0.0) for stats in model_stats.values()]),
                "avg_efficiency": np.mean([stats.get("avg_efficiency", 0.0) for stats in model_stats.values()]),
                "total_reward": sum([stats.get("total_reward", 0.0) for stats in model_stats.values()]),
                "total_weight_updates": sum([stats.get("weight_updates", 0) for stats in model_stats.values()]),
                "total_lora_adaptations": sum([stats.get("lora_adaptations", 0) for stats in model_stats.values()])
            }
        }

def create_cooperative_team() -> List[ModelConfig]:
    """åˆ›å»ºååŒå›¢é˜Ÿ"""
    return [
        ModelConfig(
            model_id="leader_001",
            model_name="qwen-2-leader",
            role=ModelRole.LEADER,
            lora_rank=16,
            team_id="team_alpha",
            specialization="strategy"
        ),
        ModelConfig(
            model_id="follower_001",
            model_name="qwen-2-follower",
            role=ModelRole.FOLLOWER,
            lora_rank=8,
            team_id="team_alpha",
            specialization="execution"
        ),
        ModelConfig(
            model_id="follower_002",
            model_name="qwen-2-follower",
            role=ModelRole.FOLLOWER,
            lora_rank=8,
            team_id="team_alpha",
            specialization="analysis"
        )
    ]

def create_competitive_models() -> List[ModelConfig]:
    """åˆ›å»ºç«äº‰æ¨¡å‹"""
    return [
        ModelConfig(
            model_id="competitor_001",
            model_name="qwen-2-competitor",
            role=ModelRole.COMPETITOR,
            lora_rank=12,
            specialization="aggressive"
        ),
        ModelConfig(
            model_id="competitor_002",
            model_name="qwen-2-competitor",
            role=ModelRole.COMPETITOR,
            lora_rank=12,
            specialization="defensive"
        ),
        ModelConfig(
            model_id="neutral_001",
            model_name="qwen-2-neutral",
            role=ModelRole.NEUTRAL,
            lora_rank=8,
            specialization="balanced"
        )
    ]

def create_team_battle_models() -> List[ModelConfig]:
    """åˆ›å»ºç»„é˜Ÿåšå¼ˆæ¨¡å‹"""
    return [
        # Team Alpha
        ModelConfig(
            model_id="alpha_leader",
            model_name="qwen-2-alpha",
            role=ModelRole.LEADER,
            lora_rank=16,
            team_id="team_alpha",
            specialization="offensive"
        ),
        ModelConfig(
            model_id="alpha_support",
            model_name="qwen-2-alpha",
            role=ModelRole.TEAMMATE,
            lora_rank=8,
            team_id="team_alpha",
            specialization="support"
        ),
        # Team Beta
        ModelConfig(
            model_id="beta_leader",
            model_name="qwen-2-beta",
            role=ModelRole.LEADER,
            lora_rank=16,
            team_id="team_beta",
            specialization="defensive"
        ),
        ModelConfig(
            model_id="beta_support",
            model_name="qwen-2-beta",
            role=ModelRole.TEAMMATE,
            lora_rank=8,
            team_id="team_beta",
            specialization="support"
        )
    ]

async def demo_cooperative_training():
    """æ¼”ç¤ºååŒè®­ç»ƒ"""
    print("\nğŸ¤ Cooperative Training Demo")
    print("=" * 50)
    
    env = MultiModelEnvironment(
        training_mode=TrainingMode.COOPERATIVE,
        max_models=5
    )
    
    # æ·»åŠ ååŒå›¢é˜Ÿ
    team_configs = create_cooperative_team()
    for config in team_configs:
        env.add_model(config)
    
    # è¿è¡Œè®­ç»ƒ
    results = await env.run_training_cycle(cycles=3)
    
    # æ˜¾ç¤ºç»“æœ
    stats = env.get_environment_stats()
    print(f"\nğŸ“Š Cooperative Training Results:")
    print(f"   Total models: {stats['environment_info']['total_models']}")
    print(f"   Completed tasks: {stats['environment_info']['total_tasks']}")
    print(f"   Average accuracy: {stats['performance_summary']['avg_accuracy']:.3f}")
    print(f"   Average efficiency: {stats['performance_summary']['avg_efficiency']:.3f}")
    print(f"   Total reward: {stats['performance_summary']['total_reward']:.2f}")
    
    return results

async def demo_competitive_training():
    """æ¼”ç¤ºç«äº‰è®­ç»ƒ"""
    print("\nâš”ï¸ Competitive Training Demo")
    print("=" * 50)
    
    env = MultiModelEnvironment(
        training_mode=TrainingMode.COMPETITIVE,
        max_models=5
    )
    
    # æ·»åŠ ç«äº‰æ¨¡å‹
    competitor_configs = create_competitive_models()
    for config in competitor_configs:
        env.add_model(config)
    
    # è¿è¡Œè®­ç»ƒ
    results = await env.run_training_cycle(cycles=3)
    
    # æ˜¾ç¤ºç»“æœ
    stats = env.get_environment_stats()
    print(f"\nğŸ“Š Competitive Training Results:")
    print(f"   Total models: {stats['environment_info']['total_models']}")
    print(f"   Completed tasks: {stats['environment_info']['total_tasks']}")
    print(f"   Average accuracy: {stats['performance_summary']['avg_accuracy']:.3f}")
    print(f"   Average efficiency: {stats['performance_summary']['avg_efficiency']:.3f}")
    print(f"   Total reward: {stats['performance_summary']['total_reward']:.2f}")
    
    return results

async def demo_team_battle():
    """æ¼”ç¤ºç»„é˜Ÿåšå¼ˆ"""
    print("\nğŸ† Team Battle Demo")
    print("=" * 50)
    
    env = MultiModelEnvironment(
        training_mode=TrainingMode.TEAM_BATTLE,
        max_models=6
    )
    
    # æ·»åŠ ç»„é˜Ÿæ¨¡å‹
    team_configs = create_team_battle_models()
    for config in team_configs:
        env.add_model(config)
    
    # è¿è¡Œè®­ç»ƒ
    results = await env.run_training_cycle(cycles=4)
    
    # æ˜¾ç¤ºç»“æœ
    stats = env.get_environment_stats()
    print(f"\nğŸ“Š Team Battle Results:")
    print(f"   Total models: {stats['environment_info']['total_models']}")
    print(f"   Completed tasks: {stats['environment_info']['total_tasks']}")
    print(f"   Average accuracy: {stats['performance_summary']['avg_accuracy']:.3f}")
    print(f"   Average efficiency: {stats['performance_summary']['avg_efficiency']:.3f}")
    print(f"   Total reward: {stats['performance_summary']['total_reward']:.2f}")
    
    # æ˜¾ç¤ºå›¢é˜Ÿè¡¨ç°
    team_performance = {}
    for model_id, model_stats in stats['model_stats'].items():
        team_id = model_stats.get('team_id', 'no_team')
        if team_id not in team_performance:
            team_performance[team_id] = []
        team_performance[team_id].append(model_stats.get('total_reward', 0.0))
    
    print(f"\nğŸ… Team Performance:")
    for team_id, rewards in team_performance.items():
        total_reward = sum(rewards)
        avg_reward = total_reward / len(rewards)
        print(f"   {team_id}: total={total_reward:.2f}, avg={avg_reward:.2f}")
    
    return results

async def demo_mixed_training():
    """æ¼”ç¤ºæ··åˆè®­ç»ƒæ¨¡å¼"""
    print("\nğŸ­ Mixed Training Mode Demo")
    print("=" * 50)
    
    env = MultiModelEnvironment(
        training_mode=TrainingMode.MIXED,
        max_models=8
    )
    
    # æ·»åŠ å„ç§ç±»å‹çš„æ¨¡å‹
    all_configs = (
        create_cooperative_team() + 
        create_competitive_models() + 
        create_team_battle_models()
    )
    
    for config in all_configs[:8]:  # é™åˆ¶æ•°é‡
        env.add_model(config)
    
    # è¿è¡Œè®­ç»ƒ
    results = await env.run_training_cycle(cycles=5)
    
    # æ˜¾ç¤ºç»“æœ
    stats = env.get_environment_stats()
    print(f"\nğŸ“Š Mixed Training Results:")
    print(f"   Total models: {stats['environment_info']['total_models']}")
    print(f"   Completed tasks: {stats['environment_info']['total_tasks']}")
    print(f"   Average accuracy: {stats['performance_summary']['avg_accuracy']:.3f}")
    print(f"   Average efficiency: {stats['performance_summary']['avg_efficiency']:.3f}")
    print(f"   Total reward: {stats['performance_summary']['total_reward']:.2f}")
    print(f"   Total weight updates: {stats['performance_summary']['total_weight_updates']}")
    print(f"   Total LoRA adaptations: {stats['performance_summary']['total_lora_adaptations']}")
    
    return results

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Multi-Model Single Environment Training System (Simplified)")
    print("=" * 70)
    print("ğŸ¤– VLLM Integration: Enabled")
    print(f"ğŸ“¡ VLLM HTTP Client: {'Available' if VLLM_AVAILABLE else 'Not Available'}")
    print(f"ğŸ”— Camel & Oasis VLLM Client: {'Available' if CAMEL_OASIS_AVAILABLE else 'Not Available'}")
    print("=" * 70)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    try:
        # æ¼”ç¤ºä¸åŒçš„è®­ç»ƒæ¨¡å¼
        results = []
        
        # 1. ååŒè®­ç»ƒ
        cooperative_results = await demo_cooperative_training()
        results.extend(cooperative_results)
        
        # 2. ç«äº‰è®­ç»ƒ
        competitive_results = await demo_competitive_training()
        results.extend(competitive_results)
        
        # 3. ç»„é˜Ÿåšå¼ˆ
        team_battle_results = await demo_team_battle()
        results.extend(team_battle_results)
        
        # 4. æ··åˆè®­ç»ƒ
        mixed_results = await demo_mixed_training()
        results.extend(mixed_results)
        
        # ä¿å­˜ç»“æœ
        with open("multi_model_training_simple_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ‰ Training completed! Total results: {len(results)}")
        print("ğŸ“ Results saved to: multi_model_training_simple_results.json")
        
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
