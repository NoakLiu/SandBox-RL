#!/usr/bin/env python3
"""
SandGraph äº¤æ˜“ç¯å¢ƒæ¼”ç¤º - åŸºäºRLçš„LLMå†³ç­–æ¶æ„

æ–°çš„æ¶æ„è®¾è®¡ï¼š
1. Sandboxä½œä¸ºç¯å¢ƒèŠ‚ç‚¹
2. LLMä½œä¸ºå†³ç­–å™¨ï¼ˆä¸æ˜¯èŠ‚ç‚¹ï¼‰
3. RLç®—æ³•æ›´æ–°LLMæƒé‡
4. çŠ¶æ€è½¬ç§»ç”±LLMå†³ç­–é©±åŠ¨
"""

import sys
import os
import time
import json
import argparse
import random
from typing import Dict, Any, List, Union, Optional
from datetime import datetime, timedelta
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sandgraph.core.llm_interface import create_shared_llm_manager
from sandgraph.core.sg_workflow import (
    SG_Workflow, WorkflowMode, EnhancedWorkflowNode,
    NodeType, NodeCondition, NodeLimits, GameState
)
from sandgraph.core.rl_algorithms import RLTrainer, RLConfig, RLAlgorithm
from sandgraph.sandbox_implementations import TradingSandbox


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")


class LLMDecisionMaker:
    """LLMå†³ç­–å™¨ - è€Œæ˜¯å†³ç­–å¼•æ“"""
    
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        self.decision_count = 0
        
        # å†å²æ•°æ®ç®¡ç†
        self.decision_history = []  # å†³ç­–å†å²
        self.market_history = []    # å¸‚åœºæ•°æ®å†å²
        self.portfolio_history = [] # æŠ•èµ„ç»„åˆå†å²
        self.performance_history = [] # è¡¨ç°å†å²
        
        # æ³¨å†Œå†³ç­–èŠ‚ç‚¹
        self.llm_manager.register_node("trading_decision", {
            "role": "äº¤æ˜“å†³ç­–ä¸“å®¶",
            "reasoning_type": "strategic",
            "temperature": 0.7,
            "max_length": 512
        })
    
    def make_decision(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºå½“å‰çŠ¶æ€åšå‡ºäº¤æ˜“å†³ç­–"""
        self.decision_count += 1
        
        # æ„é€ å†³ç­–æç¤ºï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰
        prompt = self._construct_decision_prompt(state)
        print(f"\n{'='*80}")
        print(f"Decision {self.decision_count} - Complete Prompt Content:")
        print(f"{'='*80}")
        print(prompt)
        print(f"{'='*80}")
        
        # ä½¿ç”¨LLMç”Ÿæˆå†³ç­–
        try:
            response = self.llm_manager.generate_for_node(
                "trading_decision", 
                prompt,
                temperature=0.7,
                max_new_tokens=128,  # ä½¿ç”¨max_new_tokensè€Œä¸æ˜¯max_length
                do_sample=True,
                pad_token_id=self.llm_manager.tokenizer.eos_token_id if hasattr(self.llm_manager, 'tokenizer') else None
            )
            print(f"\nLLM Response Status: {response.status if hasattr(response, 'status') else 'unknown'}")
            print(f"LLM Complete Response: {response.text}")
        except Exception as e:
            print(f"LLM Call Error: {e}")
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™å†³ç­–
            return self._fallback_decision(state)
        
        # è§£æå†³ç­–
        decision = self._parse_decision(response.text, state)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£æ
        if decision is None:
            decision = self._parse_decision_fallback(response.text, state)
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å†³ç­–
        if decision is None:
            decision = {
                "action": "BUY",
                "symbol": "AAPL",
                "amount": 100,
                "reasoning": "Fallback decision due to parsing failure"
            }
        
        # æ›´æ–°å†å²æ•°æ®
        self._update_history(state, decision, response.text)
        
        return {
            "decision": decision,
            "llm_response": response.text,
            "prompt": prompt,
            "decision_count": self.decision_count
        }
    
    def _update_history(self, state: Dict[str, Any], decision: Dict[str, Any], llm_response: str):
        """æ›´æ–°å†å²æ•°æ®"""
        # è®°å½•å†³ç­–å†å²
        decision_record = {
            "step": self.decision_count,
            "timestamp": datetime.now().isoformat(),
            "decision": decision,
            "llm_response": llm_response,
            "market_data": state.get("market_data", {}),
            "portfolio": state.get("portfolio", {}),
            "technical_indicators": state.get("technical_indicators", {}),
            "detailed_history": state.get("detailed_history", {})
        }
        self.decision_history.append(decision_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.decision_history) > 50:
            self.decision_history = self.decision_history[-50:]
        
        # è®°å½•å¸‚åœºæ•°æ®å†å²
        market_record = {
            "step": self.decision_count,
            "market_data": state.get("market_data", {}),
            "detailed_history": state.get("detailed_history", {})
        }
        self.market_history.append(market_record)
        
        # è®°å½•æŠ•èµ„ç»„åˆå†å²
        portfolio_record = {
            "step": self.decision_count,
            "portfolio": state.get("portfolio", {}),
            "total_value": self._calculate_total_portfolio_value(state)
        }
        self.portfolio_history.append(portfolio_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.market_history) > 30:
            self.market_history = self.market_history[-30:]
        if len(self.portfolio_history) > 30:
            self.portfolio_history = self.portfolio_history[-30:]
    
    def _calculate_total_portfolio_value(self, state: Dict[str, Any]) -> float:
        """è®¡ç®—æŠ•èµ„ç»„åˆæ€»ä»·å€¼"""
        portfolio = state.get("portfolio", {})
        market_data = state.get("market_data", {})
        
        cash = portfolio.get("cash", 0)
        positions = portfolio.get("positions", {})
        
        position_value = 0
        for symbol, amount in positions.items():
            if symbol in market_data and "close" in market_data[symbol]:
                position_value += amount * market_data[symbol]["close"]
        
        return cash + position_value
    
    def _construct_decision_prompt(self, state: Dict[str, Any]) -> str:
        """æ„é€ å†³ç­–æç¤ºï¼ˆåŒ…å«å†å²æ•°æ®ï¼‰"""
        market_data = state.get("market_data", {})
        portfolio = state.get("portfolio", {})
        technical_indicators = state.get("technical_indicators", {})
        
        # æ„å»ºå¸‚åœºæ•°æ®æ‘˜è¦
        market_summary = []
        for symbol, data in market_data.items():
            market_summary.append(
                f"{symbol}: ä»·æ ¼={data.get('close', 0):.2f}, "
                f"æˆäº¤é‡={data.get('volume', 0)}"
            )
        
        # æ„å»ºæŠ€æœ¯æŒ‡æ ‡æ‘˜è¦
        technical_summary = []
        for symbol, indicators in technical_indicators.items():
            technical_summary.append(
                f"{symbol}: MA5={indicators.get('ma5', 0):.2f}, "
                f"RSI={indicators.get('rsi', 0):.1f}, "
                f"è¶‹åŠ¿={indicators.get('price_trend', 'unknown')}"
            )
        
        # æ„å»ºå†³ç­–å†å²æ‘˜è¦
        history_summary = ""
        if self.decision_history:
            recent_decisions = self.decision_history[-3:]  # æœ€è¿‘3ä¸ªå†³ç­–
            history_summary = "\nRecent Decisions:\n"
            for record in recent_decisions:
                decision = record["decision"]
                history_summary += f"- Step {record['step']}: {decision.get('action', '')} {decision.get('symbol', '')} - {decision.get('reasoning', '')[:30]}...\n"
        
        # æ„å»ºæŠ•èµ„ç»„åˆæ‘˜è¦
        cash = portfolio.get("cash", 0)
        positions = portfolio.get("positions", {})
        position_summary = []
        for symbol, amount in positions.items():
            position_summary.append(f"{symbol}: {amount} è‚¡")
        
        # é‡æ„åçš„ç®€æ´æç¤ºï¼Œæ ¼å¼æ›´æ¸…æ™°
        prompt = f"""You are a trading expert in a simulation game.

REQUIRED RESPONSE FORMAT:
ACTION: [BUY|SELL] [SYMBOL] [AMOUNT] shares
REASONING: [brief explanation]

Available Actions:
1. BUY - Purchase stocks
2. SELL - Sell stocks

Available Symbols: {', '.join(market_data.keys())}

Current Market:
{chr(10).join(market_summary[:5])}

Technical Indicators:
{chr(10).join(technical_summary[:5])}

Current Portfolio:
Cash: {cash:.2f}
Positions: {chr(10).join(position_summary) if position_summary else 'None'}
{history_summary.strip()}

Choose the best trading action to maximize returns. Respond ONLY in the required format above."""
        
        return prompt

    def _parse_decision(self, response: str, state: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è§£æLLMçš„å†³ç­–å“åº”"""
        print(f"ğŸ” è§£æå“åº”: {response[:200]}...")  # æ‰“å°å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        response = response.strip()
        
        # è·å–å¯ç”¨è‚¡ç¥¨åˆ—è¡¨
        symbols = state.get("symbols", ["AAPL", "GOOGL", "MSFT", "AMZN"])
        
        # å°è¯•è§£ææ ‡å‡†æ ¼å¼
        try:
            # æŸ¥æ‰¾ACTIONè¡Œ - ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
            action_patterns = [
                # æ–°æ ¼å¼: ACTION: BUY AAPL shares 5000
                r'ACTION:\s*([A-Z]+)\s+([A-Z]+)\s+shares\s+(\d+(?:\.\d+)?)',
                # æ ‡å‡†æ ¼å¼: ACTION: BUY GOOGL 500 shares
                r'ACTION:\s*([A-Z]+)\s+([A-Z]+)\s+(\d+(?:\.\d+)?)\s+shares',
                # å¸¦è¿å­—ç¬¦æ ¼å¼: ACTION: BUY GOOGL - 500 shares
                r'ACTION:\s*([A-Z]+)\s+([A-Z]+)\s*-\s*(\d+(?:\.\d+)?)\s*shares',
                # å°å†™æ ¼å¼: action: buy googl 500 shares
                r'action:\s*([A-Z]+)\s+([A-Z]+)\s+(\d+(?:\.\d+)?)\s+shares',
                # é¦–å­—æ¯å¤§å†™æ ¼å¼: Action: Buy Googl 500 shares
                r'Action:\s*([A-Z]+)\s+([A-Z]+)\s+(\d+(?:\.\d+)?)\s+shares',
                # æ— å†’å·ç©ºæ ¼æ ¼å¼: ACTION BUY GOOGL 500 shares
                r'ACTION\s*:\s*([A-Z]+)\s+([A-Z]+)\s+(\d+(?:\.\d+)?)\s+shares',
                # ç­‰å·æ ¼å¼: ACTION=BUY GOOGL 500 shares
                r'ACTION\s*=\s*([A-Z]+)\s+([A-Z]+)\s+(\d+(?:\.\d+)?)\s+shares',
                # ç¼ºå°‘æ•°é‡æ ¼å¼: ACTION: BUY AAPL shares
                r'ACTION:\s*([A-Z]+)\s+([A-Z]+)\s+shares',
                # ç¼ºå°‘æ•°é‡æ ¼å¼: ACTION: BUY AAPL
                r'ACTION:\s*([A-Z]+)\s+([A-Z]+)',
            ]
            
            action = None
            symbol = None
            amount = None
            
            for i, pattern in enumerate(action_patterns):
                action_match = re.search(pattern, response, re.IGNORECASE)
                if action_match:
                    action = action_match.group(1).upper()
                    symbol = action_match.group(2).upper()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°é‡å‚æ•°
                    if len(action_match.groups()) >= 3:
                        amount = float(action_match.group(3))
                    else:
                        # å¦‚æœæ²¡æœ‰æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤æ•°é‡
                        amount = 100.0
                    
                    if i == 0:  # æ–°æ ¼å¼
                        print(f"âœ… æ‰¾åˆ°æ–°æ ¼å¼ACTION: {action} {symbol} shares {amount}")
                    elif i == 2:  # å¸¦è¿å­—ç¬¦æ ¼å¼
                        print(f"âœ… æ‰¾åˆ°è¿å­—ç¬¦æ ¼å¼ACTION: {action} {symbol} - {amount}")
                    elif i >= 7:  # ç¼ºå°‘æ•°é‡æ ¼å¼
                        print(f"âœ… æ‰¾åˆ°ç¼ºå°‘æ•°é‡æ ¼å¼ACTION: {action} {symbol} (ä½¿ç”¨é»˜è®¤æ•°é‡: {amount})")
                    else:  # æ ‡å‡†æ ¼å¼
                        print(f"âœ… æ‰¾åˆ°æ ‡å‡†æ ¼å¼ACTION: {action} {symbol} {amount}")
                    break
            
            if not action or not symbol:
                print("âŒ æœªæ‰¾åˆ°å®Œæ•´çš„ACTIONå­—æ®µ")
                return None
            
            # éªŒè¯åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ
            if action not in ["BUY", "SELL"]:
                print(f"âŒ æ— æ•ˆçš„ACTION: {action}")
                return None
            
            # éªŒè¯è‚¡ç¥¨ä»£ç æ˜¯å¦æœ‰æ•ˆ
            if symbol not in symbols:
                print(f"âŒ æ— æ•ˆçš„SYMBOL: {symbol}")
                return None
            
            # æŸ¥æ‰¾REASONINGè¡Œ - ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
            reasoning_patterns = [
                r'REASONING:\s*(.+?)(?:\n|$)',  # æ ‡å‡†æ ¼å¼
                r'reasoning:\s*(.+?)(?:\n|$)',  # å°å†™
                r'Reasoning:\s*(.+?)(?:\n|$)',  # é¦–å­—æ¯å¤§å†™
                r'REASONING\s*:\s*(.+?)(?:\n|$)',  # æ— å†’å·ç©ºæ ¼
                r'REASONING\s*=\s*(.+?)(?:\n|$)',  # ç­‰å·æ ¼å¼
            ]
            
            reasoning = "No reasoning provided"
            for pattern in reasoning_patterns:
                reasoning_match = re.search(pattern, response, re.IGNORECASE)
                if reasoning_match:
                    reasoning = reasoning_match.group(1).strip()
                    print(f"âœ… æ‰¾åˆ°REASONING: {reasoning[:50]}...")
                    break
            
            print(f"âœ… è§£ææˆåŠŸ: {action} {symbol} {amount} | {reasoning[:30]}...")
            
            return {
                "action": action,
                "symbol": symbol,
                "amount": amount,
                "reasoning": reasoning
            }
            
        except Exception as e:
            print(f"âŒ Decision parsing failed: {e}")
            return None
    
    def _parse_decision_fallback(self, response: str, state: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤‡ç”¨å†³ç­–è§£æé€»è¾‘"""
        # å°è¯•ä»å“åº”ä¸­æå–ä»»ä½•å¯èƒ½çš„åŠ¨ä½œ
        response_upper = response.upper()
        symbols = state.get("symbols", ["AAPL", "GOOGL", "MSFT", "AMZN"])
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æœ‰æ•ˆåŠ¨ä½œå’Œè‚¡ç¥¨
        for action in ["BUY", "SELL"]:
            if action in response_upper:
                for symbol in symbols:
                    if symbol in response_upper:
                        # å°è¯•æå–åˆç†çš„æ•°é‡ï¼ˆé¿å…æå–RSIç­‰æŒ‡æ ‡å€¼ï¼‰
                        import re
                        
                        # æŸ¥æ‰¾åˆç†çš„æ•°é‡æ¨¡å¼
                        amount_patterns = [
                            r'(\d{3,4})\s*shares',  # 500 shares, 1000 shares
                            r'(\d{3,4})\s*è‚¡',      # 500 è‚¡
                            r'(\d{3,4})\s*',        # 500 (ç©ºæ ¼å)
                            r'(\d{2,3})\s*shares',  # 50 shares, 100 shares
                        ]
                        
                        amount = 100.0  # é»˜è®¤æ•°é‡
                        for pattern in amount_patterns:
                            amount_match = re.search(pattern, response, re.IGNORECASE)
                            if amount_match:
                                potential_amount = float(amount_match.group(1))
                                # ç¡®ä¿æ•°é‡åˆç†ï¼ˆä¸æ˜¯RSIå€¼ç­‰ï¼‰
                                if 10 <= potential_amount <= 10000:
                                    amount = potential_amount
                                    break
                        
                        return {
                            "action": action,
                            "symbol": symbol,
                            "amount": amount,
                            "reasoning": f"Extracted action '{action} {symbol}' from response (using default amount: {amount})"
                        }
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåŠ¨ä½œï¼Œè¿”å›None
        return None

    def _fallback_decision(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """LLMè°ƒç”¨å¤±è´¥æ—¶çš„å¤‡ç”¨å†³ç­–"""
        market_data = state.get("market_data", {})
        if not market_data:
            # å¦‚æœæ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œå¼ºåˆ¶ä¹°å…¥ç¬¬ä¸€ä¸ªè‚¡ç¥¨
            symbols = state.get("symbols", ["AAPL", "GOOGL", "MSFT", "AMZN"])
            return {
                "action": "BUY",
                "symbol": symbols[0],
                "amount": 100,
                "reasoning": "å¤‡ç”¨å†³ç­–ï¼šæ— å¸‚åœºæ•°æ®ï¼Œä¹°å…¥é»˜è®¤è‚¡ç¥¨"
            }
        
        # åˆ†æå¸‚åœºæ•°æ®ï¼Œé€‰æ‹©æœ€ä½³ä¹°å…¥æˆ–å–å‡º
        symbols = list(market_data.keys())
        if symbols:
            # è®¡ç®—æ¯ä¸ªè‚¡ç¥¨çš„è¯„åˆ†
            best_buy_symbol = None
            best_buy_score = -1
            best_sell_symbol = None
            best_sell_score = -1
            
            for symbol in symbols:
                symbol_data = market_data[symbol]
                technical_indicators = state.get("technical_indicators", {}).get(symbol, {})
                
                # è®¡ç®—ä¹°å…¥è¯„åˆ†
                rsi = technical_indicators.get("rsi", 50)
                ma5 = technical_indicators.get("ma5", 0)
                ma20 = technical_indicators.get("ma20", 0)
                
                buy_score = 0
                if rsi < 70:  # éè¶…ä¹°
                    buy_score += 0.3
                if ma5 > ma20:  # ä¸Šæ¶¨è¶‹åŠ¿
                    buy_score += 0.4
                if ma5 > 0 and ma20 > 0:
                    trend_strength = (ma5 - ma20) / ma20
                    buy_score += min(0.3, trend_strength * 10)
                
                # è®¡ç®—å–å‡ºè¯„åˆ†
                sell_score = 0
                if rsi > 30:  # éè¶…å–
                    sell_score += 0.3
                if ma5 < ma20:  # ä¸‹è·Œè¶‹åŠ¿
                    sell_score += 0.4
                if ma5 > 0 and ma20 > 0:
                    trend_strength = (ma20 - ma5) / ma20
                    sell_score += min(0.3, trend_strength * 10)
                
                # æ›´æ–°æœ€ä½³é€‰æ‹©
                if buy_score > best_buy_score:
                    best_buy_score = buy_score
                    best_buy_symbol = symbol
                
                if sell_score > best_sell_score:
                    best_sell_score = sell_score
                    best_sell_symbol = symbol
            
            # é€‰æ‹©è¯„åˆ†æ›´é«˜çš„æ“ä½œ
            if best_buy_score > best_sell_score and best_buy_score > 0.3:
                return {
                    "action": "BUY",
                    "symbol": best_buy_symbol,
                    "amount": 100,
                    "reasoning": f"å¤‡ç”¨å†³ç­–ï¼šåŸºäºæŠ€æœ¯åˆ†æä¹°å…¥{best_buy_symbol}"
                }
            elif best_sell_score > 0.3:
                return {
                    "action": "SELL",
                    "symbol": best_sell_symbol,
                    "amount": 100,
                    "reasoning": f"å¤‡ç”¨å†³ç­–ï¼šåŸºäºæŠ€æœ¯åˆ†æå–å‡º{best_sell_symbol}"
                }
            else:
                # å¦‚æœæŠ€æœ¯åˆ†æä¸æ˜ç¡®ï¼Œé€‰æ‹©ä¹°å…¥æ¶¨å¹…æœ€å¤§çš„è‚¡ç¥¨
                best_symbol = max(symbols, 
                                key=lambda s: market_data[s].get("close", 0) - market_data[s].get("open", 0))
                return {
                    "action": "BUY",
                    "symbol": best_symbol,
                    "amount": 100,
                    "reasoning": f"å¤‡ç”¨å†³ç­–ï¼šä¹°å…¥{best_symbol}ï¼ˆæ¶¨å¹…æœ€å¤§ï¼‰"
                }
        
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        return {
            "action": "BUY",
            "symbol": "AAPL",
            "amount": 100,
            "reasoning": "å¤‡ç”¨å†³ç­–ï¼šä¹°å…¥AAPLï¼ˆé»˜è®¤é€‰æ‹©ï¼‰"
        }


def create_rl_trading_workflow(llm_manager, strategy_type: str = "simulated") -> tuple[SG_Workflow, RLTrainer, LLMDecisionMaker, Any]:
    """åˆ›å»ºåŸºäºRLçš„LLMå†³ç­–äº¤æ˜“å·¥ä½œæµ - ä½¿ç”¨çº¯æ¨¡æ‹Ÿæ•°æ®"""
    
    # åˆ›å»ºRLé…ç½®
    rl_config = RLConfig(
        algorithm=RLAlgorithm.PPO,
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        batch_size=4,  # ä»32å‡å°åˆ°4
        mini_batch_size=2,  # ä»8å‡å°åˆ°2
        ppo_epochs=2,  # ä»4å‡å°åˆ°2
        target_kl=0.01
    )
    
    # åˆ›å»ºRLè®­ç»ƒå™¨
    rl_trainer = RLTrainer(rl_config, llm_manager)
    
    # åˆ›å»ºLLMå†³ç­–å™¨
    decision_maker = LLMDecisionMaker(llm_manager)
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow = SG_Workflow("rl_trading_workflow", WorkflowMode.TRADITIONAL, llm_manager)
    
    # åˆ›å»ºäº¤æ˜“æ²™ç›’
    sandbox = TradingSandbox(
        initial_balance=100000.0,
        symbols=["AAPL", "GOOGL", "MSFT", "AMZN"]
    )
    
    # åˆ›å»ºäº¤æ˜“ç¯å¢ƒèŠ‚ç‚¹
    def trading_env_func(inputs: Dict[str, Any]) -> Dict[str, Any]:
        """äº¤æ˜“ç¯å¢ƒèŠ‚ç‚¹å‡½æ•°"""
        # è·å–å½“å‰çŠ¶æ€
        case = sandbox.case_generator()
        current_state = case["state"]
        
        # ä½¿ç”¨LLMåšå‡ºå†³ç­–
        decision_result = decision_maker.make_decision(current_state)
        decision = decision_result["decision"]
        
        # æ‰§è¡Œäº¤æ˜“å†³ç­–
        try:
            # å°†LLMå†³ç­–è½¬æ¢ä¸ºTradingSandboxæœŸæœ›çš„æ ¼å¼
            # TradingSandboxæœŸæœ›: "BUY AAPL 500" è€Œä¸æ˜¯ "ACTION: BUY AAPL 500 shares"
            sandbox_response = f"{decision['action']} {decision['symbol']} {decision['amount']}"
            
            # éªŒè¯å’Œæ‰§è¡Œäº¤æ˜“
            score = sandbox.verify_score(sandbox_response, case)
            
            # è®¡ç®—å¥–åŠ±
            reward = score * 10  # å°†åˆ†æ•°è½¬æ¢ä¸ºå¥–åŠ±
            
            # æ„å»ºçŠ¶æ€ç‰¹å¾
            state_features = {
                "market_volatility": _calculate_volatility(current_state),
                "portfolio_value": _calculate_portfolio_value(current_state),
                "cash_ratio": current_state["portfolio"]["cash"] / 100000.0,
                "position_count": len(current_state["portfolio"]["positions"]),
                "decision_type": 1 if decision["action"] == "BUY" else (2 if decision["action"] == "SELL" else 0)
            }
            
            # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
            rl_trainer.add_experience(
                state=state_features,
                action=json.dumps(decision),
                reward=reward,
                done=False
            )
            
            # æ›´æ–°ç­–ç•¥
            update_result = rl_trainer.update_policy()
            
            # æ˜¾ç¤ºRLæ›´æ–°çŠ¶æ€
            print(f"RL Update Status: {update_result.get('status', 'unknown')}")
            if update_result.get('status') == 'insufficient_data':
                print(f"  Trajectory Count: {update_result.get('trajectory_count', 0)}")
                print(f"  Required Batch Size: {update_result.get('required_batch_size', 0)}")
            elif update_result.get('status') == 'updated':
                print(f"  Training Step: {update_result.get('training_step', 0)}")
                print(f"  Algorithm: {update_result.get('algorithm', 'unknown')}")
            
            result = {
                "state": current_state,
                "decision": decision,
                "llm_response": decision_result["llm_response"],
                "sandbox_response": sandbox_response,  # æ·»åŠ è½¬æ¢åçš„å“åº”
                "score": score,
                "reward": reward,
                "rl_update": update_result,
                "sandbox_id": sandbox.sandbox_id
            }
            
            print(f"LLM Decision: {decision['action']} {decision.get('symbol', '')} {decision.get('amount', '')}")
            print(f"Decision Reason: {decision.get('reasoning', '')}")
            print(f"Sandbox Response: {sandbox_response}")
            print(f"Trading Score: {score:.3f}")
            print(f"RL Reward: {reward:.3f}")
            
            # æ˜¾ç¤ºå½“å‰æŠ•èµ„ç»„åˆçŠ¶æ€
            portfolio = current_state.get("portfolio", {})
            cash = portfolio.get("cash", 0)
            positions = portfolio.get("positions", {})
            print(f"Current Cash: {cash:.2f}")
            print(f"Current Positions: {positions}")
            
            return result
            
        except Exception as e:
            print(f"äº¤æ˜“æ‰§è¡Œé”™è¯¯: {e}")
            return {
                "state": current_state,
                "decision": {"action": "HOLD", "reasoning": f"æ‰§è¡Œé”™è¯¯: {e}"},
                "score": 0.0,
                "reward": 0.0,
                "error": str(e)
            }
    
    # æ·»åŠ äº¤æ˜“ç¯å¢ƒèŠ‚ç‚¹
    trading_env_node = EnhancedWorkflowNode(
        "trading_environment",
        NodeType.SANDBOX,
        sandbox=sandbox,
        condition=NodeCondition(),
        limits=NodeLimits(max_visits=10, resource_cost={"energy": 10, "tokens": 5})
    )
    workflow.add_node(trading_env_node)
    
    return workflow, rl_trainer, decision_maker, trading_env_func


def _calculate_volatility(state: Dict[str, Any]) -> float:
    """è®¡ç®—å¸‚åœºæ³¢åŠ¨æ€§"""
    prices = []
    for symbol_data in state.get("market_data", {}).values():
        if "close" in symbol_data:
            prices.append(symbol_data["close"])
    
    if len(prices) < 2:
        return 0.5
    
    # è®¡ç®—ä»·æ ¼å˜åŒ–ç‡çš„æ ‡å‡†å·®
    returns = []
    for i in range(1, len(prices)):
        if prices[i-1] > 0:
            returns.append(abs(prices[i] - prices[i-1]) / prices[i-1])
    
    if not returns:
        return 0.5
    
    return min(1.0, sum(returns) / len(returns) * 10)


def _calculate_portfolio_value(state: Dict[str, Any]) -> float:
    """è®¡ç®—æŠ•èµ„ç»„åˆæ€»ä»·å€¼"""
    cash = state["portfolio"]["cash"]
    positions = state["portfolio"]["positions"]
    market_data = state["market_data"]
    
    position_value = 0
    for symbol, amount in positions.items():
        if symbol in market_data and "close" in market_data[symbol]:
            position_value += amount * market_data[symbol]["close"]
    
    return cash + position_value


def run_rl_trading_demo(strategy_type: str = "simulated", steps: int = 5):
    """è¿è¡ŒåŸºäºRLçš„LLMå†³ç­–äº¤æ˜“æ¼”ç¤º"""
    
    print_section(f"RL-based LLM Decision Trading Demo") #- {strategy_type.upper()}
    
    # 1. åˆ›å»ºLLMç®¡ç†å™¨
    print("\n1. Creating LLM Manager")
    llm_manager = create_shared_llm_manager(
        model_name="mistralai/Mistral-7B-Instruct-v0.2",#"Qwen/Qwen-7B-Chat",
        backend="huggingface",
        temperature=0.7,
        max_length=512,
        device="auto",
        torch_dtype="float16"
    )
    
    # 2. åˆ›å»ºå·¥ä½œæµå’ŒRLè®­ç»ƒå™¨
    print("\n2. Creating RL Trading Workflow")
    workflow, rl_trainer, decision_maker, trading_env_func = create_rl_trading_workflow(llm_manager, strategy_type)
    
    # 3. æ‰§è¡Œå¤šæ­¥äº¤æ˜“
    print(f"\n3. Executing {steps} Trading Steps")
    
    results = []
    for step in range(steps):
        print(f"\n--- ç¬¬ {step + 1} æ­¥ ---")
        
        try:
            # ä½¿ç”¨trading_env_funcæ‰§è¡Œäº¤æ˜“
            result = trading_env_func({})
            results.append(result)
            
        except Exception as e:
            print(f"âŒ Step {step + 1} Execution Error: {e}")
            result = {
                "error": str(e),
                "step": step + 1
            }
            results.append(result)
    
    # 4. è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n4. Final Results")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_reward = sum(r.get("reward", 0) for r in results)
    avg_score = sum(r.get("score", 0) for r in results) / len(results) if results else 0
    decision_count = decision_maker.decision_count
    
    print(f"Total Decisions: {decision_count}")
    print(f"Total Reward: {total_reward:.3f}")
    print(f"Average Score: {avg_score:.3f}")
    
    # æ˜¾ç¤ºRLè®­ç»ƒç»Ÿè®¡
    rl_stats = rl_trainer.get_training_stats()
    print(f"RL Training Steps: {rl_stats['training_step']}")
    print(f"RL Algorithm: {rl_stats['algorithm']}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäºRLçš„LLMå†³ç­–äº¤æ˜“æ¼”ç¤º")
    parser.add_argument(
        "--strategy",
        type=str,
        choices=["simulated"],
        default="simulated",
        help="é€‰æ‹©äº¤æ˜“ç­–ç•¥ç±»å‹"
    )
    parser.add_argument(
        "--steps",
        type=int,
        default=5,
        help="äº¤æ˜“æ­¥æ•°"
    )
    args = parser.parse_args()
    
    run_rl_trading_demo(args.strategy, args.steps)


if __name__ == "__main__":
    main() 