#!/usr/bin/env python3
"""
SandGraph è™šå‡ä¿¡æ¯ä¼ æ’­æ¼”ç¤º - åŸºäºRLçš„LLMå†³ç­–æ¶æ„

æ¨¡æ‹Ÿè™šå‡ä¿¡æ¯åœ¨ç¤¾äº¤ç½‘ç»œä¸­çš„ä¼ æ’­ï¼š
1. ä¿¡æ¯ä¼ æ’­æœºåˆ¶
2. è™šå‡ä¿¡æ¯æ£€æµ‹
3. å¹²é¢„ç­–ç•¥
4. ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡
"""

import sys
import os
import time
import json
import argparse
import random
import re
from typing import Dict, Any, List, Union, Optional
from datetime import datetime, timedelta
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sandgraph.core.llm_interface import create_shared_llm_manager
from sandgraph.core.sg_workflow import (
    SG_Workflow, WorkflowMode, EnhancedWorkflowNode,
    NodeType, NodeCondition, NodeLimits, GameState
)
from sandgraph.core.rl_algorithms import RLTrainer, RLConfig, RLAlgorithm


class InformationType(Enum):
    """ä¿¡æ¯ç±»å‹"""
    TRUE = "true"
    FALSE = "false"
    MISLEADING = "misleading"
    UNVERIFIED = "unverified"


class UserBelief(Enum):
    """ç”¨æˆ·ä¿¡å¿µçŠ¶æ€"""
    BELIEVER = "believer"      # ç›¸ä¿¡
    SKEPTIC = "skeptic"        # æ€€ç–‘
    NEUTRAL = "neutral"        # ä¸­ç«‹
    DISBELIEVER = "disbeliever" # ä¸ç›¸ä¿¡


class InterventionType(Enum):
    """å¹²é¢„ç±»å‹"""
    FACT_CHECK = "fact_check"           # äº‹å®æ ¸æŸ¥
    WARNING_LABEL = "warning_label"     # è­¦å‘Šæ ‡ç­¾
    DOWNRANK = "downrank"               # é™æƒ
    REMOVE = "remove"                   # åˆ é™¤
    EDUCATE = "educate"                 # æ•™è‚²ç”¨æˆ·
    PROMOTE_TRUTH = "promote_truth"     # æ¨å¹¿çœŸå®ä¿¡æ¯


class MisinformationSandbox:
    """è™šå‡ä¿¡æ¯ä¼ æ’­æ²™ç›’"""
    
    def __init__(self, 
                 initial_users: int = 100,
                 max_users: int = 1000,
                 initial_misinfo_count: int = 5,
                 spread_probability: float = 0.3,
                 belief_change_prob: float = 0.2):
        
        self.sandbox_id = f"misinfo_spread_{int(time.time())}"
        self.initial_users = initial_users
        self.max_users = max_users
        self.initial_misinfo_count = initial_misinfo_count
        self.spread_probability = spread_probability
        self.belief_change_prob = belief_change_prob
        
        # ç½‘ç»œçŠ¶æ€
        self.users = {}
        self.connections = []
        self.information_pieces = []
        self.spread_history = []
        self.interventions = []
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self._initialize_network()
        self._initialize_misinformation()
    
    def _initialize_network(self):
        """åˆå§‹åŒ–ç¤¾äº¤ç½‘ç»œ"""
        # åˆ›å»ºç”¨æˆ·
        for i in range(self.initial_users):
            user_id = f"user_{i}"
            self.users[user_id] = {
                "id": user_id,
                "followers": [],
                "following": [],
                "posts": [],
                "belief_state": random.choice(list(UserBelief)),
                "credibility": random.uniform(0.1, 1.0),
                "susceptibility": random.uniform(0.1, 1.0),  # å¯¹è™šå‡ä¿¡æ¯çš„æ˜“æ„Ÿæ€§
                "fact_checking_habit": random.uniform(0.0, 1.0),  # äº‹å®æ ¸æŸ¥ä¹ æƒ¯
                "created_at": datetime.now()
            }
        
        # åˆ›å»ºè¿æ¥
        for user_id in self.users:
            # æ¯ä¸ªç”¨æˆ·éšæœºå…³æ³¨5-15ä¸ªå…¶ä»–ç”¨æˆ·
            following_count = random.randint(5, 15)
            potential_follows = [uid for uid in self.users if uid != user_id]
            follows = random.sample(potential_follows, min(following_count, len(potential_follows)))
            
            for follow_id in follows:
                self.users[user_id]["following"].append(follow_id)
                self.users[follow_id]["followers"].append(user_id)
                self.connections.append((user_id, follow_id))
    
    def _initialize_misinformation(self):
        """åˆå§‹åŒ–è™šå‡ä¿¡æ¯"""
        misinfo_templates = [
            {
                "title": "éœ‡æƒŠï¼ç§‘å­¦å®¶å‘ç°æ–°çš„å¥åº·é£é™©",
                "content": "æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œæ—¥å¸¸ç”¨å“ä¸­å­˜åœ¨æœªçŸ¥çš„å¥åº·é£é™©...",
                "type": InformationType.FALSE,
                "virality_factor": 0.8,
                "credibility_impact": -0.3
            },
            {
                "title": "æ”¿åºœéšç’é‡è¦ä¿¡æ¯",
                "content": "æ®å†…éƒ¨æ¶ˆæ¯ï¼Œæ”¿åºœæ­£åœ¨éšç’ä¸€é¡¹é‡è¦å‘ç°...",
                "type": InformationType.MISLEADING,
                "virality_factor": 0.9,
                "credibility_impact": -0.4
            },
            {
                "title": "ç‰¹æ•ˆè¯æ²»æ„ˆç‡100%",
                "content": "æŸå…¬å¸å£°ç§°å…¶æ–°è¯æ²»æ„ˆç‡è¾¾åˆ°100%...",
                "type": InformationType.FALSE,
                "virality_factor": 0.7,
                "credibility_impact": -0.5
            },
            {
                "title": "å¤–æ˜Ÿäººæ¥è§¦è¯æ®æ›å…‰",
                "content": "åŒ¿åäººå£«æä¾›çš„ç…§ç‰‡æ˜¾ç¤ºå¤–æ˜Ÿäººæ¥è§¦è¯æ®...",
                "type": InformationType.UNVERIFIED,
                "virality_factor": 0.6,
                "credibility_impact": -0.2
            },
            {
                "title": "ç»æµå´©æºƒé¢„è­¦",
                "content": "ä¸“å®¶é¢„æµ‹ç»æµå³å°†å´©æºƒï¼Œå»ºè®®ç«‹å³è¡ŒåŠ¨...",
                "type": InformationType.MISLEADING,
                "virality_factor": 0.85,
                "credibility_impact": -0.35
            }
        ]
        
        for i in range(self.initial_misinfo_count):
            template = random.choice(misinfo_templates)
            info_id = f"misinfo_{i}"
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªç”¨æˆ·ä½œä¸ºä¿¡æ¯æº
            source_user = random.choice(list(self.users.keys()))
            
            self.information_pieces.append({
                "id": info_id,
                "title": template["title"],
                "content": template["content"],
                "type": template["type"],
                "source_user": source_user,
                "virality_factor": template["virality_factor"],
                "credibility_impact": template["credibility_impact"],
                "spread_count": 0,
                "believers": [],
                "skeptics": [],
                "created_at": datetime.now(),
                "is_verified": False,
                "verification_score": 0.0
            })
            
            # åˆå§‹ä¼ æ’­
            self._spread_information(info_id, source_user)
    
    def _spread_information(self, info_id: str, source_user: str):
        """ä¼ æ’­ä¿¡æ¯"""
        info = next((i for i in self.information_pieces if i["id"] == info_id), None)
        if not info:
            return
        
        # è·å–æºç”¨æˆ·çš„å…³æ³¨è€…
        followers = self.users[source_user]["followers"]
        
        for follower_id in followers:
            # è®¡ç®—ä¼ æ’­æ¦‚ç‡
            base_prob = self.spread_probability
            user_susceptibility = self.users[follower_id]["susceptibility"]
            virality = info["virality_factor"]
            
            spread_prob = base_prob * user_susceptibility * virality
            
            if random.random() < spread_prob:
                # ä¿¡æ¯ä¼ æ’­æˆåŠŸ
                info["spread_count"] += 1
                
                # æ›´æ–°ç”¨æˆ·ä¿¡å¿µ
                self._update_user_belief(follower_id, info)
                
                # è®°å½•ä¼ æ’­å†å²
                self.spread_history.append({
                    "info_id": info_id,
                    "from_user": source_user,
                    "to_user": follower_id,
                    "timestamp": datetime.now(),
                    "belief_state": self.users[follower_id]["belief_state"]
                })
    
    def _update_user_belief(self, user_id: str, info: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·ä¿¡å¿µçŠ¶æ€"""
        user = self.users[user_id]
        current_belief = user["belief_state"]
        
        # åŸºäºä¿¡æ¯ç±»å‹å’Œç”¨æˆ·ç‰¹å¾è®¡ç®—ä¿¡å¿µå˜åŒ–æ¦‚ç‡
        if info["type"] == InformationType.FALSE:
            change_prob = self.belief_change_prob * user["susceptibility"]
        elif info["type"] == InformationType.MISLEADING:
            change_prob = self.belief_change_prob * user["susceptibility"] * 0.8
        else:  # UNVERIFIED
            change_prob = self.belief_change_prob * user["susceptibility"] * 0.5
        
        if random.random() < change_prob:
            # ä¿¡å¿µå‘ç”Ÿå˜åŒ–
            if current_belief == UserBelief.NEUTRAL:
                new_belief = UserBelief.BELIEVER if random.random() < 0.6 else UserBelief.SKEPTIC
            elif current_belief == UserBelief.SKEPTIC:
                new_belief = UserBelief.BELIEVER if random.random() < 0.3 else UserBelief.NEUTRAL
            elif current_belief == UserBelief.BELIEVER:
                new_belief = UserBelief.SKEPTIC if random.random() < 0.2 else UserBelief.NEUTRAL
            else:  # DISBELIEVER
                new_belief = UserBelief.SKEPTIC if random.random() < 0.4 else UserBelief.NEUTRAL
            
            user["belief_state"] = new_belief
            
            # æ›´æ–°ä¿¡æ¯ç»Ÿè®¡
            if new_belief == UserBelief.BELIEVER:
                info["believers"].append(user_id)
            elif new_belief == UserBelief.SKEPTIC:
                info["skeptics"].append(user_id)
    
    def case_generator(self) -> Dict[str, Any]:
        """ç”Ÿæˆå½“å‰çŠ¶æ€"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_users = len(self.users)
        total_connections = len(self.connections)
        total_misinfo = len(self.information_pieces)
        
        # è®¡ç®—ä¼ æ’­ç»Ÿè®¡
        total_spreads = sum(info["spread_count"] for info in self.information_pieces)
        total_believers = sum(len(info["believers"]) for info in self.information_pieces)
        total_skeptics = sum(len(info["skeptics"]) for info in self.information_pieces)
        
        # è®¡ç®—ä¿¡å¿µåˆ†å¸ƒ
        belief_distribution = {}
        for belief in UserBelief:
            count = sum(1 for user in self.users.values() if user["belief_state"] == belief)
            belief_distribution[belief.value] = count
        
        # è®¡ç®—å¹²é¢„æ•ˆæœ
        intervention_stats = {
            "total_interventions": len(self.interventions),
            "successful_interventions": sum(1 for i in self.interventions if i.get("success", False)),
            "recent_interventions": len([i for i in self.interventions if (datetime.now() - i["timestamp"]).seconds < 3600])
        }
        
        return {
            "state": {
                "network_state": {
                    "total_users": total_users,
                    "total_connections": total_connections,
                    "avg_followers": total_connections / total_users if total_users > 0 else 0,
                    "network_density": total_connections / (total_users * (total_users - 1)) if total_users > 1 else 0
                },
                "misinformation_state": {
                    "total_pieces": total_misinfo,
                    "total_spreads": total_spreads,
                    "avg_spreads_per_piece": total_spreads / total_misinfo if total_misinfo > 0 else 0,
                    "believers_count": total_believers,
                    "skeptics_count": total_skeptics,
                    "belief_ratio": total_believers / (total_believers + total_skeptics) if (total_believers + total_skeptics) > 0 else 0
                },
                "user_beliefs": belief_distribution,
                "intervention_stats": intervention_stats,
                "recent_spreads": self.spread_history[-10:] if self.spread_history else [],
                "active_misinfo": [info for info in self.information_pieces if info["spread_count"] > 0]
            },
            "metadata": {
                "sandbox_id": self.sandbox_id,
                "timestamp": datetime.now().isoformat()
            }
        }
    
    def verify_score(self, action: str, case: Dict[str, Any]) -> float:
        """éªŒè¯å¹²é¢„è¡ŒåŠ¨çš„æ•ˆæœ"""
        try:
            # è§£æè¡ŒåŠ¨
            action_parts = action.split()
            if len(action_parts) < 2:
                return 0.0
            
            intervention_type = action_parts[0].upper()
            target = action_parts[1] if len(action_parts) > 1 else "general"
            
            # åŸºç¡€åˆ†æ•°
            base_score = 0.0
            
            # æ ¹æ®å¹²é¢„ç±»å‹è¯„åˆ†
            if intervention_type == "FACT_CHECK":
                base_score = 0.7
            elif intervention_type == "WARNING_LABEL":
                base_score = 0.6
            elif intervention_type == "DOWNRANK":
                base_score = 0.5
            elif intervention_type == "REMOVE":
                base_score = 0.8
            elif intervention_type == "EDUCATE":
                base_score = 0.4
            elif intervention_type == "PROMOTE_TRUTH":
                base_score = 0.6
            else:
                base_score = 0.3
            
            # æ ¹æ®ç›®æ ‡è°ƒæ•´åˆ†æ•°
            if target == "high_spread":
                base_score *= 1.2
            elif target == "high_belief":
                base_score *= 1.1
            elif target == "general":
                base_score *= 1.0
            
            # è€ƒè™‘å½“å‰çŠ¶æ€
            current_state = case["state"]
            belief_ratio = current_state["misinformation_state"]["belief_ratio"]
            
            # å¦‚æœä¿¡å¿µæ¯”ä¾‹é«˜ï¼Œå¹²é¢„æ•ˆæœæ›´å¥½
            if belief_ratio > 0.5:
                base_score *= 1.3
            
            # é™åˆ¶åˆ†æ•°èŒƒå›´
            return min(1.0, max(0.0, base_score))
            
        except Exception as e:
            print(f"è¯„åˆ†é”™è¯¯: {e}")
            return 0.0
    
    def execute_intervention(self, intervention_type: str, target: str = "general") -> Dict[str, Any]:
        """æ‰§è¡Œå¹²é¢„è¡ŒåŠ¨"""
        intervention = {
            "type": intervention_type,
            "target": target,
            "timestamp": datetime.now(),
            "success": False,
            "impact": {}
        }
        
        # æ¨¡æ‹Ÿå¹²é¢„æ•ˆæœ
        if intervention_type == "FACT_CHECK":
            # äº‹å®æ ¸æŸ¥ï¼šé™ä½è™šå‡ä¿¡æ¯çš„ä¼ æ’­
            for info in self.information_pieces:
                if info["type"] in [InformationType.FALSE, InformationType.MISLEADING]:
                    info["verification_score"] += 0.3
                    info["virality_factor"] *= 0.8
            
            intervention["success"] = True
            intervention["impact"] = {"verification_improved": True, "virality_reduced": True}
            
        elif intervention_type == "WARNING_LABEL":
            # è­¦å‘Šæ ‡ç­¾ï¼šå¢åŠ ç”¨æˆ·æ€€ç–‘
            for user_id in self.users:
                if self.users[user_id]["belief_state"] == UserBelief.BELIEVER:
                    if random.random() < 0.3:
                        self.users[user_id]["belief_state"] = UserBelief.SKEPTIC
            
            intervention["success"] = True
            intervention["impact"] = {"skepticism_increased": True}
            
        elif intervention_type == "DOWNRANK":
            # é™æƒï¼šå‡å°‘ä¿¡æ¯å¯è§æ€§
            for info in self.information_pieces:
                info["virality_factor"] *= 0.6
            
            intervention["success"] = True
            intervention["impact"] = {"visibility_reduced": True}
            
        elif intervention_type == "REMOVE":
            # åˆ é™¤ï¼šç§»é™¤è™šå‡ä¿¡æ¯
            self.information_pieces = [info for info in self.information_pieces 
                                     if info["type"] == InformationType.TRUE]
            
            intervention["success"] = True
            intervention["impact"] = {"misinfo_removed": True}
            
        elif intervention_type == "EDUCATE":
            # æ•™è‚²ï¼šæé«˜ç”¨æˆ·äº‹å®æ ¸æŸ¥ä¹ æƒ¯
            for user_id in self.users:
                self.users[user_id]["fact_checking_habit"] = min(1.0, 
                    self.users[user_id]["fact_checking_habit"] + 0.1)
            
            intervention["success"] = True
            intervention["impact"] = {"education_improved": True}
            
        elif intervention_type == "PROMOTE_TRUTH":
            # æ¨å¹¿çœŸå®ä¿¡æ¯
            # åˆ›å»ºæ–°çš„çœŸå®ä¿¡æ¯
            true_info = {
                "id": f"truth_{len(self.information_pieces)}",
                "title": "äº‹å®æ ¸æŸ¥ï¼šæ¾„æ¸…è™šå‡ä¿¡æ¯",
                "content": "ç»è¿‡ä¸“ä¸šæ ¸æŸ¥ï¼Œä¹‹å‰ä¼ æ’­çš„ä¿¡æ¯å­˜åœ¨è¯¯å¯¼æ€§...",
                "type": InformationType.TRUE,
                "source_user": "platform",
                "virality_factor": 0.5,
                "credibility_impact": 0.3,
                "spread_count": 0,
                "believers": [],
                "skeptics": [],
                "created_at": datetime.now(),
                "is_verified": True,
                "verification_score": 1.0
            }
            self.information_pieces.append(true_info)
            
            intervention["success"] = True
            intervention["impact"] = {"truth_promoted": True}
        
        # è®°å½•å¹²é¢„
        self.interventions.append(intervention)
        
        return intervention


class LLMDecisionMaker:
    """LLMå†³ç­–å™¨ - è™šå‡ä¿¡æ¯å¹²é¢„ä¸“å®¶"""
    
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        self.decision_count = 0
        
        # å†å²æ•°æ®ç®¡ç†
        self.decision_history = []
        self.intervention_history = []
        self.spread_history = []
        
        # æ³¨å†Œå†³ç­–èŠ‚ç‚¹
        self.llm_manager.register_node("misinfo_intervention", {
            "role": "è™šå‡ä¿¡æ¯å¹²é¢„ä¸“å®¶",
            "reasoning_type": "strategic",
            "temperature": 0.7,
            "max_length": 512
        })
    
    def make_decision(self, current_state: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºå½“å‰çŠ¶æ€åšå‡ºå¹²é¢„å†³ç­–"""
        self.decision_count += 1
        
        # æ„å»ºå†³ç­–æç¤º
        prompt = self._construct_decision_prompt(current_state)
        
        print("=" * 80)
        print(f"Decision {self.decision_count} - Complete Prompt Content:")
        print("=" * 80)
        print(prompt)
        print("=" * 80)
        
        try:
            # ç”ŸæˆLLMå“åº”
            response = self.llm_manager.generate_for_node(
                "misinfo_intervention",
                prompt,
                temperature=0.7,
                max_new_tokens=256,
                do_sample=True
            )
            
            print(f"LLM Response Status: {response.status if hasattr(response, 'status') else 'unknown'}")
            print(f"LLM Complete Response: {response.text}")
            
            # è§£æå“åº”
            decision = self._parse_decision_response(response.text)
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£æ
            if decision is None:
                decision = self._parse_decision_fallback(response.text)
            
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å†³ç­–
            if decision is None:
                decision = {
                    "action": "FACT_CHECK",
                    "target": "general",
                    "reasoning": "Fallback decision due to parsing failure"
                }
            
            # æ›´æ–°å†å²æ•°æ®
            self._update_history(current_state, decision, response.text)
            
            return {
                "decision": decision,
                "llm_response": response.text,
                "prompt": prompt,
                "decision_count": self.decision_count
            }
            
        except Exception as e:
            print(f"âŒ Decision generation failed: {e}")
            fallback_decision = {
                "action": "FACT_CHECK",
                "target": "general", 
                "reasoning": f"Error in decision generation: {str(e)}"
            }
            
            # æ›´æ–°å†å²æ•°æ®ï¼ˆå³ä½¿å¤±è´¥ï¼‰
            self._update_history(current_state, fallback_decision, f"Error: {str(e)}")
            
            return {
                "decision": fallback_decision,
                "llm_response": f"Error: {str(e)}",
                "prompt": prompt,
                "decision_count": self.decision_count
            }
    
    def _update_history(self, state: Dict[str, Any], decision: Dict[str, Any], llm_response: str):
        """æ›´æ–°å†å²æ•°æ®"""
        # è®°å½•å†³ç­–å†å²
        decision_record = {
            "step": self.decision_count,
            "timestamp": datetime.now().isoformat(),
            "decision": decision,
            "llm_response": llm_response,
            "network_state": state.get("network_state", {}),
            "misinformation_state": state.get("misinformation_state", {}),
            "user_beliefs": state.get("user_beliefs", {})
        }
        self.decision_history.append(decision_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.decision_history) > 50:
            self.decision_history = self.decision_history[-50:]
    
    def _construct_decision_prompt(self, state: Dict[str, Any]) -> str:
        """æ„é€ å†³ç­–æç¤º"""
        network_state = state.get("network_state", {})
        misinformation_state = state.get("misinformation_state", {})
        user_beliefs = state.get("user_beliefs", {})
        intervention_stats = state.get("intervention_stats", {})
        
        # æ„å»ºç½‘ç»œçŠ¶æ€æ‘˜è¦
        network_summary = f"""
Network Status:
- Total Users: {network_state.get('total_users', 0)}
- Total Connections: {network_state.get('total_connections', 0)}
- Network Density: {network_state.get('network_density', 0):.3f}
"""
        
        # æ„å»ºè™šå‡ä¿¡æ¯çŠ¶æ€æ‘˜è¦
        misinfo_summary = f"""
Misinformation Status:
- Total Pieces: {misinformation_state.get('total_pieces', 0)}
- Total Spreads: {misinformation_state.get('total_spreads', 0)}
- Believers: {misinformation_state.get('believers_count', 0)}
- Skeptics: {misinformation_state.get('skeptics_count', 0)}
- Belief Ratio: {misinformation_state.get('belief_ratio', 0):.3f}
"""
        
        # æ„å»ºç”¨æˆ·ä¿¡å¿µåˆ†å¸ƒæ‘˜è¦
        belief_summary = f"""
User Belief Distribution:
- Believers: {user_beliefs.get('believer', 0)}
- Skeptics: {user_beliefs.get('skeptic', 0)}
- Neutral: {user_beliefs.get('neutral', 0)}
- Disbelievers: {user_beliefs.get('disbeliever', 0)}
"""
        
        # æ„å»ºå¹²é¢„å†å²æ‘˜è¦
        history_summary = ""
        if self.decision_history:
            recent_decisions = self.decision_history[-3:]  # æœ€è¿‘3ä¸ªå†³ç­–
            history_summary = "\nRecent Interventions:\n"
            for record in recent_decisions:
                decision = record["decision"]
                history_summary += f"- Step {record['step']}: {decision['action']} - {decision.get('reasoning', '')[:30]}...\n"
        
        # é‡æ„åçš„ç®€æ´æç¤º
        prompt = f"""You are a misinformation intervention expert in a social network simulation.

REQUIRED RESPONSE FORMAT:
ACTION: [FACT_CHECK|WARNING_LABEL|DOWNRANK|REMOVE|EDUCATE|PROMOTE_TRUTH] [GENERAL|HIGH_SPREAD|HIGH_BELIEF]
REASONING: [brief explanation]

Available Actions:
1. FACT_CHECK - Verify and debunk false information
2. WARNING_LABEL - Add warning labels to misleading content
3. DOWNRANK - Reduce visibility of false information
4. REMOVE - Remove false information completely
5. EDUCATE - Improve user media literacy
6. PROMOTE_TRUTH - Promote verified true information

Available Targets:
- GENERAL: General intervention across all misinformation
- HIGH_SPREAD: Target high-spread misinformation specifically
- HIGH_BELIEF: Target information with many believers specifically

{network_summary.strip()}
{misinfo_summary.strip()}
{belief_summary.strip()}
{history_summary.strip()}

Choose the best intervention strategy to combat misinformation. Respond ONLY in the required format above."""
        
        return prompt
    
    def _parse_decision_response(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æLLMå†³ç­–å“åº”"""
        response = response.strip()
        
        print(f"ğŸ” è§£æå“åº”: {response[:200]}...")
        
        # å°è¯•è§£ææ ‡å‡†æ ¼å¼
        try:
            # æŸ¥æ‰¾ACTIONè¡Œ - æ”¯æŒå¤šç§æ ¼å¼
            action_patterns = [
                # æ ‡å‡†æ ¼å¼: ACTION: FACT_CHECK GENERAL
                r'ACTION:\s*([A-Z_]+)\s+([A-Z_]+)',
                # å¸¦æ–¹æ‹¬å·æ ¼å¼: ACTION: EDUCATE [general]
                r'ACTION:\s*([A-Z_]+)\s+\[([A-Z_]+)\]',
                # å°å†™æ ¼å¼: action: fact_check general
                r'action:\s*([A-Z_]+)\s+([A-Z_]+)',
                # é¦–å­—æ¯å¤§å†™æ ¼å¼: Action: Fact_Check General
                r'Action:\s*([A-Z_]+)\s+([A-Z_]+)',
                # æ— å†’å·ç©ºæ ¼æ ¼å¼: ACTION FACT_CHECK GENERAL
                r'ACTION\s+([A-Z_]+)\s+([A-Z_]+)',
                # ç­‰å·æ ¼å¼: ACTION=FACT_CHECK GENERAL
                r'ACTION\s*=\s*([A-Z_]+)\s+([A-Z_]+)',
            ]
            
            action = None
            target = None
            
            for pattern in action_patterns:
                action_match = re.search(pattern, response, re.IGNORECASE)
                if action_match:
                    action = action_match.group(1).upper()
                    target = action_match.group(2).upper()
                    print(f"âœ… æ‰¾åˆ°ACTION: {action} {target}")
                    break
            
            if not action or not target:
                print("âŒ æœªæ‰¾åˆ°å®Œæ•´çš„ACTIONå­—æ®µ")
                return None
            
            # éªŒè¯åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ
            valid_actions = [
                "FACT_CHECK", "WARNING_LABEL", "DOWNRANK", 
                "REMOVE", "EDUCATE", "PROMOTE_TRUTH"
            ]
            
            if action not in valid_actions:
                print(f"âŒ æ— æ•ˆçš„ACTION: {action}")
                return None
            
            # éªŒè¯ç›®æ ‡æ˜¯å¦æœ‰æ•ˆ
            valid_targets = ["GENERAL", "HIGH_SPREAD", "HIGH_BELIEF"]
            if target not in valid_targets:
                print(f"âš ï¸  æ— æ•ˆçš„TARGET: {target}ï¼Œä½¿ç”¨GENERAL")
                target = "GENERAL"
            
            # æŸ¥æ‰¾REASONINGè¡Œ
            reasoning_patterns = [
                r'REASONING:\s*(.+?)(?:\n|$)',  # æ ‡å‡†æ ¼å¼
                r'reasoning:\s*(.+?)(?:\n|$)',  # å°å†™
                r'Reasoning:\s*(.+?)(?:\n|$)',  # é¦–å­—æ¯å¤§å†™
                r'REASONING\s*:\s*(.+?)(?:\n|$)',  # æ— å†’å·ç©ºæ ¼
                r'REASONING\s*=\s*(.+?)(?:\n|$)',  # ç­‰å·æ ¼å¼
            ]
            
            reasoning = "No reasoning provided"
            for pattern in reasoning_patterns:
                reasoning_match = re.search(pattern, response, re.IGNORECASE)
                if reasoning_match:
                    reasoning = reasoning_match.group(1).strip()
                    print(f"âœ… æ‰¾åˆ°REASONING: {reasoning[:50]}...")
                    break
            
            print(f"âœ… è§£ææˆåŠŸ: {action} {target} | {reasoning[:30]}...")
            
            return {
                "action": action,
                "target": target,
                "reasoning": reasoning
            }
            
        except Exception as e:
            print(f"âŒ Decision parsing failed: {e}")
            return None
    
    def _parse_decision_fallback(self, response: str) -> Optional[Dict[str, Any]]:
        """å¤‡ç”¨å†³ç­–è§£æé€»è¾‘"""
        # å°è¯•ä»å“åº”ä¸­æå–ä»»ä½•å¯èƒ½çš„åŠ¨ä½œ
        response_upper = response.upper()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æœ‰æ•ˆåŠ¨ä½œ
        valid_actions = [
            "FACT_CHECK", "WARNING_LABEL", "DOWNRANK", 
            "REMOVE", "EDUCATE", "PROMOTE_TRUTH"
        ]
        
        for action in valid_actions:
            if action in response_upper:
                return {
                    "action": action,
                    "target": "GENERAL",
                    "reasoning": f"Extracted action '{action}' from response"
                }
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåŠ¨ä½œï¼Œè¿”å›None
        return None


def create_rl_misinfo_workflow(llm_manager) -> tuple[SG_Workflow, RLTrainer, LLMDecisionMaker]:
    """åˆ›å»ºåŸºäºRLçš„LLMå†³ç­–è™šå‡ä¿¡æ¯å¹²é¢„å·¥ä½œæµ"""
    
    # åˆ›å»ºRLé…ç½®
    rl_config = RLConfig(
        algorithm=RLAlgorithm.PPO,
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        batch_size=32,
        mini_batch_size=8,
        ppo_epochs=4,
        target_kl=0.01
    )
    
    # åˆ›å»ºRLè®­ç»ƒå™¨
    rl_trainer = RLTrainer(rl_config, llm_manager)
    
    # åˆ›å»ºLLMå†³ç­–å™¨
    decision_maker = LLMDecisionMaker(llm_manager)
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow = SG_Workflow("rl_misinfo_workflow", WorkflowMode.TRADITIONAL, llm_manager)
    
    # åˆ›å»ºè™šå‡ä¿¡æ¯æ²™ç›’
    sandbox = MisinformationSandbox(
        initial_users=100,
        max_users=1000,
        initial_misinfo_count=5
    )
    
    # åˆ›å»ºè™šå‡ä¿¡æ¯ç¯å¢ƒèŠ‚ç‚¹
    def misinfo_env_func(inputs: Dict[str, Any]) -> Dict[str, Any]:
        """è™šå‡ä¿¡æ¯ç¯å¢ƒèŠ‚ç‚¹å‡½æ•°"""
        # è·å–å½“å‰çŠ¶æ€
        case = sandbox.case_generator()
        current_state = case["state"]
        
        # ä½¿ç”¨LLMåšå‡ºå†³ç­–
        decision_result = decision_maker.make_decision(current_state)
        decision = decision_result["decision"]
        
        # æ‰§è¡Œå¹²é¢„å†³ç­–
        try:
            # æ‰§è¡Œå¹²é¢„
            intervention_result = sandbox.execute_intervention(
                decision["action"], 
                decision.get("target", "general")
            )
            
            # éªŒè¯å’Œæ‰§è¡Œå†³ç­–
            score = sandbox.verify_score(
                f"{decision['action']} {decision.get('target', 'general')}",
                case
            )
            
            # è®¡ç®—å¥–åŠ±
            reward = score * 10
            
            # æ„å»ºçŠ¶æ€ç‰¹å¾
            state_features = {
                "total_users": current_state["network_state"]["total_users"],
                "belief_ratio": current_state["misinformation_state"]["belief_ratio"],
                "total_spreads": current_state["misinformation_state"]["total_spreads"],
                "intervention_count": current_state["intervention_stats"]["total_interventions"],
                "decision_type": _encode_intervention_type(decision["action"])
            }
            
            # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
            rl_trainer.add_experience(
                state=state_features,
                action=json.dumps(decision),
                reward=reward,
                done=False
            )
            
            # æ›´æ–°ç­–ç•¥
            update_result = rl_trainer.update_policy()
            
            return {
                "state": current_state,
                "decision": decision,
                "llm_response": decision_result["llm_response"],
                "intervention": intervention_result,
                "score": score,
                "reward": reward,
                "rl_update": update_result,
                "sandbox_id": sandbox.sandbox_id
            }
            
        except Exception as e:
            print(f"è™šå‡ä¿¡æ¯å¹²é¢„æ‰§è¡Œé”™è¯¯: {e}")
            return {
                "state": current_state,
                "decision": {"action": "FACT_CHECK", "reasoning": f"æ‰§è¡Œé”™è¯¯: {e}"},
                "score": 0.0,
                "reward": 0.0,
                "error": str(e)
            }
    
    # æ·»åŠ è™šå‡ä¿¡æ¯ç¯å¢ƒèŠ‚ç‚¹
    misinfo_env_node = EnhancedWorkflowNode(
        "misinfo_environment",
        NodeType.SANDBOX,
        sandbox=sandbox,
        condition=NodeCondition(),
        limits=NodeLimits(max_visits=10, resource_cost={"energy": 10, "tokens": 5})
    )
    workflow.add_node(misinfo_env_node)
    
    return workflow, rl_trainer, decision_maker


def _encode_intervention_type(action: str) -> int:
    """ç¼–ç å¹²é¢„ç±»å‹"""
    action_map = {
        "FACT_CHECK": 1,
        "WARNING_LABEL": 2,
        "DOWNRANK": 3,
        "REMOVE": 4,
        "EDUCATE": 5,
        "PROMOTE_TRUTH": 6
    }
    return action_map.get(action, 0)


def run_rl_misinfo_demo(steps: int = 5):
    """è¿è¡ŒåŸºäºRLçš„LLMå†³ç­–è™šå‡ä¿¡æ¯å¹²é¢„æ¼”ç¤º"""
    
    print("ğŸ”¥ SandGraph Misinformation Spread Demo")
    print("=" * 60)
    
    # 1. åˆ›å»ºLLMç®¡ç†å™¨
    print("\n1. Creating LLM Manager")
    llm_manager = create_shared_llm_manager(
        model_name="mistralai/Mistral-7B-Instruct-v0.2",
        backend="huggingface",
        temperature=0.7,
        max_length=512,
        device="auto",
        torch_dtype="float16"
    )
    
    # 2. åˆ›å»ºå·¥ä½œæµå’ŒRLè®­ç»ƒå™¨
    print("\n2. Creating RL Misinformation Workflow")
    workflow, rl_trainer, decision_maker = create_rl_misinfo_workflow(llm_manager)
    
    # 3. æ‰§è¡Œå¤šæ­¥è™šå‡ä¿¡æ¯å¹²é¢„
    print(f"\n3. Executing {steps} Misinformation Intervention Steps")
    
    results = []
    for step in range(steps):
        print(f"\n--- ç¬¬ {step + 1} æ­¥ ---")
        
        try:
            # ç›´æ¥æ‰§è¡Œè™šå‡ä¿¡æ¯ç¯å¢ƒèŠ‚ç‚¹
            node = workflow.nodes.get("misinfo_environment")
            if node and node.sandbox:
                # è·å–å½“å‰çŠ¶æ€
                case = node.sandbox.case_generator()
                current_state = case["state"]
                
                # ä½¿ç”¨LLMåšå‡ºå†³ç­–
                decision_result = decision_maker.make_decision(current_state)
                decision = decision_result["decision"]
                
                # æ‰§è¡Œå¹²é¢„å†³ç­–
                try:
                    # æ‰§è¡Œå¹²é¢„
                    intervention_result = node.sandbox.execute_intervention(
                        decision["action"], 
                        decision.get("target", "general")
                    )
                    
                    # éªŒè¯å’Œæ‰§è¡Œå†³ç­–
                    score = node.sandbox.verify_score(
                        f"{decision['action']} {decision.get('target', 'general')}",
                        case
                    )
                    
                    # è®¡ç®—å¥–åŠ±
                    reward = score * 10
                    
                    # æ„å»ºçŠ¶æ€ç‰¹å¾
                    state_features = {
                        "total_users": current_state["network_state"]["total_users"],
                        "belief_ratio": current_state["misinformation_state"]["belief_ratio"],
                        "total_spreads": current_state["misinformation_state"]["total_spreads"],
                        "intervention_count": current_state["intervention_stats"]["total_interventions"],
                        "decision_type": _encode_intervention_type(decision["action"])
                    }
                    
                    # æ·»åŠ åˆ°RLè®­ç»ƒå™¨
                    rl_trainer.add_experience(
                        state=state_features,
                        action=json.dumps(decision),
                        reward=reward,
                        done=False
                    )
                    
                    # æ›´æ–°ç­–ç•¥
                    update_result = rl_trainer.update_policy()
                    
                    result = {
                        "state": current_state,
                        "decision": decision,
                        "llm_response": decision_result["llm_response"],
                        "intervention": intervention_result,
                        "score": score,
                        "reward": reward,
                        "rl_update": update_result,
                        "sandbox_id": node.sandbox.sandbox_id
                    }
                    
                    print(f"LLM Decision: {decision['action']} {decision.get('target', '')}")
                    print(f"Decision Reason: {decision.get('reasoning', '')}")
                    print(f"Intervention Success: {intervention_result.get('success', False)}")
                    print(f"Intervention Score: {score:.3f}")
                    print(f"RL Reward: {reward:.3f}")
                    
                    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
                    misinfo_state = current_state["misinformation_state"]
                    print(f"Total Misinformation: {misinfo_state['total_pieces']}")
                    print(f"Total Spreads: {misinfo_state['total_spreads']}")
                    print(f"Belief Ratio: {misinfo_state['belief_ratio']:.3f}")
                    
                    results.append(result)
                    
                except Exception as e:
                    print(f"âŒ Intervention Execution Error: {e}")
                    result = {
                        "state": current_state,
                        "decision": {"action": "FACT_CHECK", "reasoning": f"Execution Error: {e}"},
                        "score": 0.0,
                        "reward": 0.0,
                        "error": str(e)
                    }
                    results.append(result)
            else:
                print("âŒ Misinformation Environment Node Not Found or Invalid")
        
        except Exception as e:
            print(f"âŒ Step {step + 1} Execution Error: {e}")
    
    # 4. è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n4. Final Results")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_reward = sum(r.get("reward", 0) for r in results)
    avg_score = sum(r.get("score", 0) for r in results) / len(results) if results else 0
    decision_count = decision_maker.decision_count
    
    print(f"Total Decisions: {decision_count}")
    print(f"Total Reward: {total_reward:.3f}")
    print(f"Average Score: {avg_score:.3f}")
    
    # æ˜¾ç¤ºRLè®­ç»ƒç»Ÿè®¡
    rl_stats = rl_trainer.get_training_stats()
    print(f"RL Training Steps: {rl_stats['training_step']}")
    print(f"RL Algorithm: {rl_stats['algorithm']}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SandGraph Misinformation Spread Demo")
    parser.add_argument("--steps", type=int, default=5, help="Number of steps to run")
    parser.add_argument("--test", action="store_true", help="Run tests instead of demo")
    
    args = parser.parse_args()
    
    if args.test:
        # è¿è¡Œæµ‹è¯•
        print("ğŸ”¥ SandGraph Misinformation Spread Demo æµ‹è¯•")
        print("=" * 80)
        
        success1 = test_misinformation_sandbox()
        success2 = test_llm_decision_maker()
        success3 = test_parsing_logic()
        
        if success1 and success2 and success3:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            print("\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
    else:
        # è¿è¡Œæ¼”ç¤º
        print("ğŸ”¥ SandGraph Misinformation Spread Demo")
        print("=" * 60)
        print(f"Steps: {args.steps}")
        
        try:
            results = run_rl_misinfo_demo(args.steps)
            print("\nâœ… Demo completed successfully!")
            
        except Exception as e:
            print(f"\nâŒ Demo failed: {e}")
            import traceback
            traceback.print_exc()


def test_misinformation_sandbox():
    """æµ‹è¯•è™šå‡ä¿¡æ¯æ²™ç›’"""
    print("ğŸ”¥ æµ‹è¯•Misinformation Sandbox")
    print("=" * 60)
    
    # åˆ›å»ºæ²™ç›’
    sandbox = MisinformationSandbox(
        initial_users=50,
        max_users=200,
        initial_misinfo_count=3
    )
    
    print(f"âœ… æ²™ç›’åˆ›å»ºæˆåŠŸ: {sandbox.sandbox_id}")
    print(f"ç”¨æˆ·æ•°é‡: {len(sandbox.users)}")
    print(f"è¿æ¥æ•°é‡: {len(sandbox.connections)}")
    print(f"è™šå‡ä¿¡æ¯æ•°é‡: {len(sandbox.information_pieces)}")
    
    # æµ‹è¯•çŠ¶æ€ç”Ÿæˆ
    case = sandbox.case_generator()
    state = case["state"]
    
    print(f"\nç½‘ç»œçŠ¶æ€:")
    print(f"- æ€»ç”¨æˆ·: {state['network_state']['total_users']}")
    print(f"- æ€»è¿æ¥: {state['network_state']['total_connections']}")
    print(f"- ç½‘ç»œå¯†åº¦: {state['network_state']['network_density']:.3f}")
    
    print(f"\nè™šå‡ä¿¡æ¯çŠ¶æ€:")
    print(f"- æ€»ä¿¡æ¯: {state['misinformation_state']['total_pieces']}")
    print(f"- æ€»ä¼ æ’­: {state['misinformation_state']['total_spreads']}")
    print(f"- ç›¸ä¿¡è€…: {state['misinformation_state']['believers_count']}")
    print(f"- æ€€ç–‘è€…: {state['misinformation_state']['skeptics_count']}")
    print(f"- ä¿¡å¿µæ¯”ä¾‹: {state['misinformation_state']['belief_ratio']:.3f}")
    
    print(f"\nç”¨æˆ·ä¿¡å¿µåˆ†å¸ƒ:")
    for belief, count in state['user_beliefs'].items():
        print(f"- {belief}: {count}")
    
    # æµ‹è¯•å¹²é¢„
    print(f"\næµ‹è¯•å¹²é¢„æ•ˆæœ:")
    intervention_result = sandbox.execute_intervention("FACT_CHECK", "general")
    print(f"å¹²é¢„ç±»å‹: {intervention_result['type']}")
    print(f"å¹²é¢„æˆåŠŸ: {intervention_result['success']}")
    print(f"å¹²é¢„å½±å“: {intervention_result['impact']}")
    
    # æµ‹è¯•è¯„åˆ†
    score = sandbox.verify_score("FACT_CHECK general", case)
    print(f"å¹²é¢„è¯„åˆ†: {score:.3f}")
    
    return True


def test_llm_decision_maker():
    """æµ‹è¯•LLMå†³ç­–å™¨"""
    print("\nğŸ”¥ æµ‹è¯•LLM Decision Maker")
    print("=" * 60)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    print("1. åˆ›å»ºLLMç®¡ç†å™¨...")
    try:
        llm_manager = create_shared_llm_manager(
            model_name="mistralai/Mistral-7B-Instruct-v0.2",
            backend="huggingface",
            temperature=0.7,
            max_length=512,
            device="auto",
            torch_dtype="float16"
        )
        print("âœ… LLMç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ LLMç®¡ç†å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºå†³ç­–å™¨
    print("2. åˆ›å»ºå†³ç­–å™¨...")
    decision_maker = LLMDecisionMaker(llm_manager)
    
    # åˆ›å»ºæµ‹è¯•çŠ¶æ€
    print("3. åˆ›å»ºæµ‹è¯•çŠ¶æ€...")
    test_state = {
        "network_state": {
            "total_users": 100,
            "total_connections": 500,
            "network_density": 0.05
        },
        "misinformation_state": {
            "total_pieces": 5,
            "total_spreads": 25,
            "believers_count": 15,
            "skeptics_count": 10,
            "belief_ratio": 0.6
        },
        "user_beliefs": {
            "believer": 30,
            "skeptic": 20,
            "neutral": 40,
            "disbeliever": 10
        },
        "intervention_stats": {
            "total_interventions": 2,
            "successful_interventions": 1,
            "recent_interventions": 1
        }
    }
    
    # æµ‹è¯•å†³ç­–ç”Ÿæˆ
    print("4. æµ‹è¯•å†³ç­–ç”Ÿæˆ...")
    try:
        decision_result = decision_maker.make_decision(test_state)
        decision = decision_result["decision"]
        
        print(f"\nâœ… å†³ç­–ç”ŸæˆæˆåŠŸ!")
        print(f"Action: {decision['action']}")
        print(f"Target: {decision['target']}")
        print(f"Reasoning: {decision['reasoning']}")
        print(f"Decision Count: {decision_result['decision_count']}")
        print(f"LLM Response Length: {len(decision_result['llm_response'])} characters")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æ˜¯fallbackå†³ç­–
        if "Fallback decision" in decision['reasoning']:
            print("âš ï¸  è­¦å‘Š: ä»ç„¶æ˜¯fallbackå†³ç­–")
            print(f"å®Œæ•´LLMå“åº”: {decision_result['llm_response']}")
            return False
        else:
            print("âœ… æˆåŠŸè§£æåˆ°æœ‰æ•ˆå†³ç­–!")
            return True
        
    except Exception as e:
        print(f"\nâŒ å†³ç­–ç”Ÿæˆå¤±è´¥: {e}")
        return False


def test_parsing_logic():
    """æµ‹è¯•è§£æé€»è¾‘"""
    print("\nğŸ”¥ æµ‹è¯•è§£æé€»è¾‘")
    print("=" * 60)
    
    # åˆ›å»ºå†³ç­–å™¨ï¼ˆä¸éœ€è¦LLMï¼‰
    decision_maker = LLMDecisionMaker(None)
    
    # æµ‹è¯•ä¸åŒçš„å“åº”æ ¼å¼
    test_cases = [
        "ACTION: FACT_CHECK GENERAL\nREASONING: High belief ratio requires fact checking",
        "ACTION: WARNING_LABEL HIGH_SPREAD\nREASONING: High spread misinformation needs warning",
        "ACTION: REMOVE HIGH_BELIEF\nREASONING: Remove false information with many believers",
        "action: educate general\nreasoning: Improve user literacy",
        "Action: Promote_Truth General\nReasoning: Promote verified information",
        "ACTION: EDUCATE [GENERAL]\nREASONING: Improve user media literacy",
        "ACTION: FACT_CHECK [HIGH_SPREAD]\nREASONING: Target high-spread misinformation",
        "ACTION WARNING_LABEL GENERAL\nREASONING: Add warning labels",
        "ACTION=REMOVE HIGH_BELIEF\nREASONING: Remove false information",
    ]
    
    test_state = {
        "network_state": {"total_users": 100},
        "misinformation_state": {"belief_ratio": 0.6},
        "user_beliefs": {"believer": 30},
        "intervention_stats": {"total_interventions": 2}
    }
    
    success_count = 0
    for i, test_response in enumerate(test_cases):
        print(f"\næµ‹è¯•æ¡ˆä¾‹ {i+1}: {test_response}")
        try:
            decision = decision_maker._parse_decision_response(test_response)
            if decision:
                print(f"âœ… è§£ææˆåŠŸ: {decision['action']} {decision['target']}")
                success_count += 1
            else:
                print("âŒ è§£æå¤±è´¥")
        except Exception as e:
            print(f"âŒ è§£æå¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š è§£ææµ‹è¯•ç»“æœ: {success_count}/{len(test_cases)} æˆåŠŸ")
    return success_count >= len(test_cases) * 0.8  # 80%æˆåŠŸç‡


if __name__ == "__main__":
    main() 