#!/usr/bin/env python3
"""
Enhanced RL Cache Demo with Areal Framework Integration
======================================================

This demo showcases the enhanced RL algorithms with Areal framework integration
for optimized caching and performance improvements.
"""

import sys
import os
import time
import json
import random
import argparse
from typing import Dict, Any, List

# Add the parent directory to the path to import sandgraph modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sandgraph.core.llm_interface import create_shared_llm_manager
from sandgraph.core.enhanced_rl_algorithms import (
    EnhancedRLConfig, 
    EnhancedRLTrainer,
    CachePolicy,
    create_enhanced_ppo_trainer,
    create_enhanced_grpo_trainer,
    create_optimized_rl_trainer
)
from sandgraph.core.rl_algorithms import RLAlgorithm
from sandgraph.core.monitoring import (
    SocialNetworkMonitor, 
    MonitoringConfig, 
    SocialNetworkMetrics, 
    create_monitor
)


def generate_sample_experience() -> Dict[str, Any]:
    """ç”Ÿæˆæ ·æœ¬ç»éªŒæ•°æ®"""
    return {
        "state": {
            "user_count": random.randint(50, 200),
            "engagement_rate": random.uniform(0.1, 0.5),
            "content_quality": random.uniform(0.3, 0.9),
            "network_density": random.uniform(0.1, 0.8),
            "active_users": random.randint(20, 100),
            "total_posts": random.randint(100, 500),
            "viral_posts": random.randint(1, 20),
            "avg_session_time": random.uniform(10, 60)
        },
        "action": random.choice([
            "CREATE_POST", "LIKE_POST", "FOLLOW", "SHARE", 
            "CREATE_COMMENT", "TREND", "DO_NOTHING"
        ]),
        "reward": random.uniform(-1.0, 5.0),
        "done": random.random() < 0.1  # 10%æ¦‚ç‡ç»“æŸ
    }


def demonstrate_basic_enhanced_rl():
    """æ¼”ç¤ºåŸºç¡€å¢å¼ºç‰ˆRLåŠŸèƒ½"""
    print("ğŸš€ Basic Enhanced RL Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºå¢å¼ºç‰ˆPPOè®­ç»ƒå™¨
    trainer = create_enhanced_ppo_trainer(
        llm_manager=llm_manager,
        learning_rate=0.001,
        enable_caching=True
    )
    
    print("âœ… Enhanced PPO trainer created")
    
    # æ·»åŠ ç»éªŒæ•°æ®
    print("\nğŸ“Š Adding experience data...")
    for i in range(50):
        exp = generate_sample_experience()
        trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        if i % 10 == 0:
            print(f"  Added {i+1} experiences")
    
    # æ›´æ–°ç­–ç•¥
    print("\nğŸ”„ Updating policy...")
    result = trainer.update_policy()
    
    print(f"âœ… Policy update result: {result['status']}")
    
    # è·å–å¢å¼ºç»Ÿè®¡ä¿¡æ¯
    stats = trainer.get_enhanced_stats()
    print(f"\nğŸ“ˆ Enhanced Stats:")
    print(f"  - Training Steps: {stats['performance_stats']['training_steps']}")
    print(f"  - Cache Hits: {stats['cache_stats']['hits']}")
    print(f"  - Cache Misses: {stats['cache_stats']['misses']}")
    print(f"  - Hit Rate: {stats['cache_stats']['hit_rate']:.3f}")
    print(f"  - Areal Available: {stats['areal_available']}")
    print(f"  - NumPy Available: {stats['numpy_available']}")
    print(f"  - PyTorch Available: {stats['torch_available']}")


def demonstrate_advanced_caching():
    """æ¼”ç¤ºé«˜çº§ç¼“å­˜åŠŸèƒ½"""
    print("\nğŸ”¥ Advanced Caching Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºé«˜çº§é…ç½®
    config = EnhancedRLConfig(
        algorithm=RLAlgorithm.PPO,
        enable_caching=True,
        cache_size=5000,
        cache_policy=CachePolicy.LFU,  # ä½¿ç”¨LFUç­–ç•¥
        enable_batching=True,
        parallel_processing=True,
        max_workers=4,
        enable_metrics=True,
        enable_persistence=True,
        persistence_interval=50
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EnhancedRLTrainer(config, llm_manager)
    
    print("âœ… Advanced RL trainer created with LFU caching")
    
    # æ¨¡æ‹Ÿå¤§é‡ç»éªŒæ•°æ®
    print("\nğŸ“Š Adding large amount of experience data...")
    for i in range(200):
        exp = generate_sample_experience()
        trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        # å®šæœŸæ›´æ–°ç­–ç•¥
        if i % 25 == 0 and i > 0:
            result = trainer.update_policy()
            stats = trainer.get_enhanced_stats()
            print(f"  Step {i}: Cache hit rate = {stats['cache_stats']['hit_rate']:.3f}")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = trainer.get_enhanced_stats()
    print(f"\nğŸ“ˆ Final Cache Performance:")
    print(f"  - Total Cache Hits: {final_stats['cache_stats']['hits']}")
    print(f"  - Total Cache Misses: {final_stats['cache_stats']['misses']}")
    print(f"  - Overall Hit Rate: {final_stats['cache_stats']['hit_rate']:.3f}")
    print(f"  - Cache Evictions: {final_stats['cache_stats']['evictions']}")
    print(f"  - Cache Size: {final_stats['cache_stats']['size']}")
    print(f"  - Memory Usage: {final_stats['cache_stats']['memory_usage']:.2f} GB")


def demonstrate_optimized_training():
    """æ¼”ç¤ºä¼˜åŒ–è®­ç»ƒåŠŸèƒ½"""
    print("\nâš¡ Optimized Training Demo")
    print("=" * 50)
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    # åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒå™¨
    trainer = create_optimized_rl_trainer(
        llm_manager=llm_manager,
        algorithm=RLAlgorithm.GRPO,  # ä½¿ç”¨GRPOç®—æ³•
        cache_size=10000,
        enable_parallel=True
    )
    
    print("âœ… Optimized GRPO trainer created")
    
    # æ€§èƒ½æµ‹è¯•
    print("\nğŸƒ Performance test with parallel processing...")
    start_time = time.time()
    
    # å¿«é€Ÿæ·»åŠ å¤§é‡ç»éªŒ
    for i in range(300):
        exp = generate_sample_experience()
        trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        # å®šæœŸæ›´æ–°
        if i % 30 == 0 and i > 0:
            update_start = time.time()
            result = trainer.update_policy()
            update_time = time.time() - update_start
            
            stats = trainer.get_enhanced_stats()
            print(f"  Step {i}: Update time = {update_time:.3f}s, "
                  f"Cache hit rate = {stats['cache_stats']['hit_rate']:.3f}")
    
    total_time = time.time() - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    final_stats = trainer.get_enhanced_stats()
    print(f"\nğŸ“Š Performance Summary:")
    print(f"  - Total Training Time: {total_time:.2f}s")
    print(f"  - Average Update Time: {final_stats['performance_stats']['total_training_time'] / final_stats['performance_stats']['training_steps']:.3f}s")
    print(f"  - Total Training Steps: {final_stats['performance_stats']['training_steps']}")
    print(f"  - Cache Performance: {final_stats['cache_stats']['hit_rate']:.3f} hit rate")
    print(f"  - Memory Efficiency: {final_stats['cache_stats']['memory_usage']:.2f} GB")


def demonstrate_monitoring_integration():
    """æ¼”ç¤ºä¸ç›‘æ§ç³»ç»Ÿçš„é›†æˆ"""
    print("\nğŸ“Š Monitoring Integration Demo")
    print("=" * 50)
    
    # åˆ›å»ºç›‘æ§é…ç½®
    monitor_config = MonitoringConfig(
        enable_wandb=False,  # è®¾ç½®ä¸ºFalseé¿å…éœ€è¦WanDBé…ç½®
        enable_tensorboard=True,
        enable_console_logging=True,
        enable_file_logging=True,
        wandb_project_name="enhanced-rl-cache-demo",
        tensorboard_log_dir="./logs/enhanced_rl_cache",
        log_file_path="./logs/enhanced_rl_cache_metrics.json",
        metrics_sampling_interval=1.0
    )
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = create_monitor(monitor_config)
    monitor.start_monitoring()
    
    # åˆ›å»ºLLMç®¡ç†å™¨å’Œè®­ç»ƒå™¨
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    trainer = create_enhanced_ppo_trainer(llm_manager, enable_caching=True)
    
    print("âœ… Monitoring and enhanced RL trainer initialized")
    
    # è¿è¡Œè®­ç»ƒå¹¶æ”¶é›†æŒ‡æ ‡
    print("\nğŸ”„ Running training with monitoring...")
    for step in range(100):
        # æ·»åŠ ç»éªŒ
        exp = generate_sample_experience()
        trainer.add_experience(
            state=exp["state"],
            action=exp["action"],
            reward=exp["reward"],
            done=exp["done"]
        )
        
        # å®šæœŸæ›´æ–°ç­–ç•¥
        if step % 20 == 0 and step > 0:
            result = trainer.update_policy()
            
            # æ”¶é›†æŒ‡æ ‡
            stats = trainer.get_enhanced_stats()
            
            # åˆ›å»ºç›‘æ§æŒ‡æ ‡
            metrics = SocialNetworkMetrics(
                total_users=exp["state"]["user_count"],
                active_users=exp["state"]["active_users"],
                engagement_rate=exp["state"]["engagement_rate"],
                content_quality_score=exp["state"]["content_quality"],
                network_density=exp["state"]["network_density"],
                viral_posts=exp["state"]["viral_posts"],
                avg_session_time=exp["state"]["avg_session_time"],
                response_time_avg=0.5,  # æ¨¡æ‹Ÿå“åº”æ—¶é—´
                error_rate=0.02,  # æ¨¡æ‹Ÿé”™è¯¯ç‡
                system_uptime=time.time()
            )
            
            # æ·»åŠ ç¼“å­˜ç›¸å…³æŒ‡æ ‡
            metrics.user_segments = {
                "cache_hits": stats['cache_stats']['hits'],
                "cache_misses": stats['cache_stats']['misses'],
                "cache_evictions": stats['cache_stats']['evictions']
            }
            
            # æ›´æ–°ç›‘æ§
            monitor.update_metrics(metrics)
            
            print(f"  Step {step}: Reward = {exp['reward']:.2f}, "
                  f"Cache hit rate = {stats['cache_stats']['hit_rate']:.3f}")
    
    # åœæ­¢ç›‘æ§
    monitor.stop_monitoring()
    
    # å¯¼å‡ºç»“æœ
    timestamp = int(time.time())
    monitor.export_metrics(f"./logs/enhanced_rl_cache_export_{timestamp}.json", "json")
    
    print(f"\nâœ… Monitoring completed. Results exported to logs/")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = trainer.get_enhanced_stats()
    print(f"\nğŸ“ˆ Final Statistics:")
    print(f"  - Training Steps: {final_stats['performance_stats']['training_steps']}")
    print(f"  - Cache Hit Rate: {final_stats['cache_stats']['hit_rate']:.3f}")
    print(f"  - Total Training Time: {final_stats['performance_stats']['total_training_time']:.2f}s")
    print(f"  - Memory Usage: {final_stats['cache_stats']['memory_usage']:.2f} GB")


def compare_performance():
    """æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½"""
    print("\nğŸ” Performance Comparison")
    print("=" * 50)
    
    llm_manager = create_shared_llm_manager("mistralai/Mistral-7B-Instruct-v0.2")
    
    configurations = [
        {
            "name": "Basic RL (No Cache)",
            "config": EnhancedRLConfig(
                algorithm=RLAlgorithm.PPO,
                enable_caching=False,
                enable_batching=False,
                parallel_processing=False
            )
        },
        {
            "name": "Enhanced RL (With Cache)",
            "config": EnhancedRLConfig(
                algorithm=RLAlgorithm.PPO,
                enable_caching=True,
                enable_batching=True,
                parallel_processing=False
            )
        },
        {
            "name": "Optimized RL (Full Features)",
            "config": EnhancedRLConfig(
                algorithm=RLAlgorithm.PPO,
                enable_caching=True,
                enable_batching=True,
                parallel_processing=True,
                max_workers=4
            )
        }
    ]
    
    results = []
    
    for config_info in configurations:
        print(f"\nğŸ§ª Testing {config_info['name']}...")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = EnhancedRLTrainer(config_info['config'], llm_manager)
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        for i in range(100):
            exp = generate_sample_experience()
            trainer.add_experience(
                state=exp["state"],
                action=exp["action"],
                reward=exp["reward"],
                done=exp["done"]
            )
            
            if i % 25 == 0 and i > 0:
                trainer.update_policy()
        
        total_time = time.time() - start_time
        
        # æ”¶é›†ç»Ÿè®¡
        stats = trainer.get_enhanced_stats()
        
        result = {
            "name": config_info['name'],
            "total_time": total_time,
            "cache_hit_rate": stats['cache_stats']['hit_rate'],
            "memory_usage": stats['cache_stats']['memory_usage'],
            "training_steps": stats['performance_stats']['training_steps']
        }
        
        results.append(result)
        
        print(f"  âœ… {config_info['name']}: {total_time:.2f}s, "
              f"Cache hit rate: {stats['cache_stats']['hit_rate']:.3f}")
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š Performance Comparison Results:")
    print(f"{'Configuration':<25} {'Time (s)':<10} {'Hit Rate':<10} {'Memory (GB)':<12}")
    print("-" * 60)
    
    for result in results:
        print(f"{result['name']:<25} {result['total_time']:<10.2f} "
              f"{result['cache_hit_rate']:<10.3f} {result['memory_usage']:<12.2f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Enhanced RL Cache Demo with Areal Framework")
    
    parser.add_argument("--demo", type=str, default="all", 
                       choices=["basic", "caching", "optimized", "monitoring", "compare", "all"],
                       help="Which demo to run")
    parser.add_argument("--steps", type=int, default=100,
                       help="Number of training steps")
    parser.add_argument("--cache-size", type=int, default=10000,
                       help="Cache size")
    parser.add_argument("--enable-parallel", action="store_true", default=True,
                       help="Enable parallel processing")
    
    args = parser.parse_args()
    
    print("ğŸš€ Enhanced RL Cache Demo with Areal Framework Integration")
    print("=" * 70)
    print(f"Demo: {args.demo}")
    print(f"Steps: {args.steps}")
    print(f"Cache Size: {args.cache_size}")
    print(f"Parallel Processing: {args.enable_parallel}")
    print("=" * 70)
    
    try:
        if args.demo == "basic" or args.demo == "all":
            demonstrate_basic_enhanced_rl()
        
        if args.demo == "caching" or args.demo == "all":
            demonstrate_advanced_caching()
        
        if args.demo == "optimized" or args.demo == "all":
            demonstrate_optimized_training()
        
        if args.demo == "monitoring" or args.demo == "all":
            demonstrate_monitoring_integration()
        
        if args.demo == "compare" or args.demo == "all":
            compare_performance()
        
        print(f"\nğŸ‰ Enhanced RL Cache Demo completed successfully!")
        print("ğŸ“ Check the logs/ directory for detailed results and metrics.")
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 