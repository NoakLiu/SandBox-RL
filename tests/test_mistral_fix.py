#!/usr/bin/env python3
"""
æµ‹è¯•Mistralæ¨¡å‹ä¿®å¤
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sandbox_rl.core.llm_interface import create_mistral_manager


def test_mistral():
    """æµ‹è¯•Mistralæ¨¡å‹"""
    print("ğŸ”¥ æµ‹è¯•Mistral-7B-Instruct-v0.2")
    print("=" * 60)
    
    # åˆ›å»ºMistralç®¡ç†å™¨
    print("1. åˆ›å»ºMistralç®¡ç†å™¨...")
    try:
        llm_manager = create_mistral_manager(
            model_name="mistralai/Mistral-7B-Instruct-v0.2",
            device="auto"
        )
        print("âœ… Mistralç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ Mistralç®¡ç†å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹
    print("2. æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹...")
    try:
        llm_manager.register_node("test_node", {
            "role": "æµ‹è¯•åŠ©æ‰‹",
            "reasoning_type": "logical",
            "temperature": 0.7,
            "max_length": 512
        })
        print("âœ… æµ‹è¯•èŠ‚ç‚¹æ³¨å†ŒæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æµ‹è¯•èŠ‚ç‚¹æ³¨å†Œå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ç®€å•æç¤º
    print("3. æµ‹è¯•ç®€å•æç¤º...")
    simple_prompt = "What is 2+2?"
    try:
        response = llm_manager.generate_for_node(
            "test_node",
            simple_prompt,
            temperature=0.7,
            max_new_tokens=50,
            do_sample=True
        )
        
        print(f"âœ… ç®€å•æç¤ºæµ‹è¯•æˆåŠŸ")
        print(f"Response: {response.text}")
        print(f"Confidence: {response.confidence}")
        print(f"Metadata: {response.metadata}")
        
    except Exception as e:
        print(f"âŒ ç®€å•æç¤ºæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ç¤¾äº¤ç½‘ç»œæç¤º
    print("4. æµ‹è¯•ç¤¾äº¤ç½‘ç»œæç¤º...")
    social_prompt = """You are a social network strategy expert. Based on the current network state, what action would you take to improve user engagement?

Current Network State:
- Active Users: 30
- Average Engagement: 0.08%
- Content Quality Score: 0.63

Please respond with:
ACTION: [specific action]
TARGET: [target if applicable]
REASONING: [explanation]"""
    
    try:
        response = llm_manager.generate_for_node(
            "test_node",
            social_prompt,
            temperature=0.7,
            max_new_tokens=200,
            do_sample=True
        )
        
        print(f"âœ… ç¤¾äº¤ç½‘ç»œæç¤ºæµ‹è¯•æˆåŠŸ")
        print(f"Response: {response.text}")
        print(f"Confidence: {response.confidence}")
        print(f"Response Length: {len(response.text)} characters")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦æ±‚çš„æ ¼å¼
        has_action = "ACTION:" in response.text.upper()
        has_target = "TARGET:" in response.text.upper()
        has_reasoning = "REASONING:" in response.text.upper()
        
        print(f"åŒ…å«ACTION: {has_action}")
        print(f"åŒ…å«TARGET: {has_target}")
        print(f"åŒ…å«REASONING: {has_reasoning}")
        
    except Exception as e:
        print(f"âŒ ç¤¾äº¤ç½‘ç»œæç¤ºæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    return True


if __name__ == "__main__":
    success = test_mistral()
    if success:
        print("\nâœ… Mistralä¿®å¤éªŒè¯æˆåŠŸ!")
    else:
        print("\nï¿½ï¿½ Mistralä¿®å¤éªŒè¯å¤±è´¥!") 